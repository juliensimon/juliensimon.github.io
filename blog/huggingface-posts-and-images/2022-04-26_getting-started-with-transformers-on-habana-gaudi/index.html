<!DOCTYPE html>
<html>
 <head>
    <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  
  <!-- Primary Meta Tags -->
  <title>Getting Started With Transformers On Habana Gaudi - Julien Simon | Open Source AI Expert</title>
  <meta name="title" content="Getting Started With Transformers On Habana Gaudi - Julien Simon | Open Source AI Expert"/>
  <meta name="description" content="Expert analysis and technical deep-dive on getting started with transformers on habana gaudi by Julien Simon, leading voice in open-source AI and former Chief Evangelist at Hugging Face. Comprehensive insights on transformers, small language models, and AI accessibility."/>
  <meta name="keywords" content="Hugging Face, Transformers, Open Source AI, Small Language Models, Machine Learning, AI, Natural Language Processing, NLP, Julien Simon, AI Expert, Open Source Expert, Hugging Face ExpertStarted-With-Transformers-On-Habana-Gaudi, Getting Started With Transformers On Habana Gaudi"/>
  <meta name="author" content="Julien Simon"/>
  <meta name="robots" content="index, follow"/>
  
  <!-- Open Graph / Facebook -->
  <meta property="og:type" content="article"/>
  <meta property="og:url" content="https://julien.org/blog/2022-04-26-getting-started-with-transformers-on-habana-gaudi/"/>
  <meta property="og:title" content="Getting Started With Transformers On Habana Gaudi - Julien Simon | Open Source AI Expert"/>
  <meta property="og:description" content="Expert analysis and technical deep-dive on getting started with transformers on habana gaudi by Julien Simon, leading voice in open-source AI and former Chief Evangelist at Hugging Face. Comprehensive insights on transformers, small language models, and AI accessibility."/>
  <meta property="og:image" content="https://julien.org/assets/julien-simon-huggingface-expert.jpg"/>
  <meta property="og:site_name" content="Julien Simon - Open Source AI Expert"/>
  <meta property="article:author" content="Julien Simon"/>
  <meta property="article:published_time" content="2022-04-26T00:00:00Z"/>
  <meta property="article:section" content="Hugging Face"/>
  <meta property="article:tag" content="Hugging Face, Open Source AI, Transformers, Small Language Models"/>
  
  <!-- Twitter -->
  <meta property="twitter:card" content="summary_large_image"/>
  <meta property="twitter:url" content="https://julien.org/blog/2022-04-26-getting-started-with-transformers-on-habana-gaudi/"/>
  <meta property="twitter:title" content="Getting Started With Transformers On Habana Gaudi - Julien Simon | Open Source AI Expert"/>
  <meta property="twitter:description" content="Expert analysis and technical deep-dive on getting started with transformers on habana gaudi by Julien Simon, leading voice in open-source AI and former Chief Evangelist at Hugging Face. Comprehensive insights on transformers, small language models, and AI accessibility."/>
  <meta property="twitter:image" content="https://julien.org/assets/julien-simon-huggingface-expert.jpg"/>
  <meta property="twitter:creator" content="@julsimon"/>
  
  <!-- Canonical URL -->
  <link rel="canonical" href="https://julien.org/blog/2022-04-26-getting-started-with-transformers-on-habana-gaudi/"/>
  
  <!-- Author and Publisher -->
  <link rel="author" href="https://julien.org/"/>
  <link rel="publisher" href="https://julien.org/"/>
  
  <!-- Structured Data -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "BlogPosting",
    "headline": "Getting Started With Transformers On Habana Gaudi",
    "description": "Expert analysis and technical deep-dive on getting started with transformers on habana gaudi by Julien Simon, leading voice in open-source AI and former Chief Evangelist at Hugging Face. Comprehensive insights on transformers, small language models, and AI accessibility.",
    "image": "https://julien.org/assets/julien-simon-huggingface-expert.jpg",
    "author": {
      "@type": "Person",
      "name": "Julien Simon",
      "url": "https://julien.org/",
      "jobTitle": "Open Source AI Expert & Former Chief Evangelist",
      "worksFor": {
        "@type": "Organization",
        "name": "Hugging Face"
      },
      "sameAs": [
        "https://twitter.com/julsimon",
        "https://linkedin.com/in/juliensimon",
        "https://github.com/juliensimon"
      ]
    },
    "publisher": {
      "@type": "Organization",
      "name": "Julien Simon",
      "url": "https://julien.org/",
      "logo": {
        "@type": "ImageObject",
        "url": "https://julien.org/assets/julien-simon-logo.jpg"
      }
    },
    "datePublished": "2022-04-26T00:00:00Z",
    "dateModified": "2022-04-26T00:00:00Z",
    "mainEntityOfPage": {
      "@type": "WebPage",
      "@id": "https://julien.org/blog/2022-04-26-getting-started-with-transformers-on-habana-gaudi/"
    },
    "url": "https://julien.org/blog/2022-04-26-getting-started-with-transformers-on-habana-gaudi/",
    "keywords": "Hugging Face, Transformers, Open Source AI, Small Language Models, Machine Learning, AI, Natural Language Processing, NLP, Julien Simon, AI Expert, Open Source Expert, Hugging Face Expert, Getting-Started-With-Transformers-On-Habana-Gaudi",
    "articleSection": "Hugging Face",
    "inLanguage": "en-US",
    "isPartOf": {
      "@type": "Blog",
      "name": "Julien Simon - Open Source AI Expert Blog",
      "url": "https://julien.org/blog/"
    }
  }
  </script>
  
  <!-- Additional SEO Meta Tags -->
  <meta name="twitter:site" content="@julsimon"/>
  <meta name="twitter:creator" content="@julsimon"/>
  <meta name="theme-color" content="#FF6B35"/>
  <meta name="msapplication-TileColor" content="#FF6B35"/>
  <meta name="apple-mobile-web-app-capable" content="yes"/>
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent"/>
  
  <!-- Preconnect to external domains -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  
  <!-- Favicon -->
  <link rel="icon" type="image/x-icon" href="https://julien.org/assets/favicon.ico">
  
  <!-- Security Headers -->
  <meta http-equiv="X-Content-Type-Options" content="nosniff">
  <meta http-equiv="X-Frame-Options" content="DENY">
  <meta http-equiv="Referrer-Policy" content="strict-origin-when-cross-origin">
  <meta http-equiv="X-XSS-Protection" content="1; mode=block">
  <meta http-equiv="Permissions-Policy" content="camera=(), microphone=(), geolocation=(), interest-cohort=()">
  <meta http-equiv="Cross-Origin-Opener-Policy" content="same-origin">
  <meta http-equiv="Cross-Origin-Embedder-Policy" content="require-corp">

  
  <style>
   body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
            max-width: 800px;
            margin: 0 auto;
            padding: 20px;
            line-height: 1.6;
            color: #333;
        }
        h1, h2, h3, h4, h5, h6 {
            color: #2c3e50;
            margin-top: 2em;
            margin-bottom: 0.5em;
        }
        h1 { font-size: 2.5em; }
        h2 { font-size: 2em; }
        h3 { font-size: 1.5em; }
        img {
            max-width: 100%;
            height: auto;
            border-radius: 8px;
            box-shadow: 0 4px 6px rgba(0,0,0,0.1);
            margin: 1em 0;
        }
        /* Image alignment classes for text wraparound */
        img.alignleft {
            float: left;
            margin: 0.5em 1.5em 1em 0;
        }
        img.alignright {
            float: right;
            margin: 0.5em 0 1em 1.5em;
        }
        img.aligncenter {
            display: block;
            margin: 1em auto;
            float: none;
        }
        img.alignnone {
            float: none;
            margin: 1em 0;
        }
        /* Clear floats after images */
        .wp-block-image::after,
        p:has(img.alignleft)::after,
        p:has(img.alignright)::after {
            content: "";
            display: table;
            clear: both;
        }
        pre {
            background: #f8f9fa;
            border: 1px solid #e9ecef;
            border-radius: 4px;
            padding: 1em;
            overflow-x: auto;
        }
        code {
            background: #f8f9fa;
            padding: 0.2em 0.4em;
            border-radius: 3px;
            font-family: 'Monaco', 'Consolas', monospace;
        }
        blockquote {
            border-left: 4px solid #3498db;
            margin: 1em 0;
            padding-left: 1em;
            font-style: italic;
            color: #7f8c8d;
        }
        a {
            color: #3498db;
            text-decoration: none;
        }
        a:hover {
            text-decoration: underline;
        }
        p {
            margin-bottom: 1em;
        }
        /* Hugging Face specific styling */
        .prose {
            max-width: none;
        }
        .prose pre {
            background: #1e293b;
            color: #e2e8f0;
        }
        .prose code {
            background: #f1f5f9;
            color: #dc2626;
        }
  </style>
 </head>
 <body>

  <div style="margin-bottom: 2em;">
   <a href="../../../../huggingface-blog-posts.html" style="color: #3498db; text-decoration: none; font-weight: 500;">‚Üê Back to Hugging Face Blog Posts</a>
  </div>
  <h1>
   Getting Started with Transformers on Habana Gaudi
  </h1>
  <p style="color: #666; font-style: italic; margin-bottom: 1em;">
   Published: 2022-04-26
  </p>
  
  <p style="color: #666; font-style: italic; margin-bottom: 2em;">
   Originally published at
   <a href="https://huggingface.co/blog/getting-started-habana">
    https://huggingface.co/blog/getting-started-habana
   </a>
  </p>
  <!-- HTML_TAG_START -->
  <p>
   A couple of weeks ago, we've had the pleasure to
   <a href="https://huggingface.co/blog/habana">
    announce
   </a>
   that
   <a href="https://habana.ai">
    Habana Labs
   </a>
   and
   <a href="https://huggingface.co/">
    Hugging Face
   </a>
   would partner to accelerate Transformer model training.
  </p>
  <p>
   Habana Gaudi accelerators deliver up to 40% better price performance for training machine learning models compared to the latest GPU-based Amazon EC2 instances. We are super excited to bring this price performance advantages to Transformers üöÄ
  </p>
  <p>
   In this hands-on post, I'll show you how to quickly set up a Habana Gaudi instance on Amazon Web Services, and then fine-tune a BERT model for text classification. As usual, all code is provided so that you may reuse it in your projects.
  </p>
  <p>
   Let's get started!
  </p>
  <h2 class="relative group flex items-center">
   <span>
    Setting up an Habana Gaudi instance on AWS
   </span>
  </h2>
  <p>
   The simplest way to work with Habana Gaudi accelerators is to launch an Amazon EC2
   <a href="https://aws.amazon.com/ec2/instance-types/dl1/">
    DL1
   </a>
   instance. These instances are equipped with 8 Habana Gaudi processors that can easily be put to work thanks to the
   <a href="https://aws.amazon.com/marketplace/server/procurement?productId=9a75c51a-a4d1-4470-884f-6be27933fcc8">
    Habana Deep Learning Amazon Machine Image
   </a>
   (AMI).  This AMI comes preinstalled with the
   <a href="https://developer.habana.ai/">
    Habana SynapseAI¬Æ SDK
   </a>
   , and the tools required to run Gaudi accelerated Docker containers. If you'd like to use other AMIs or containers, instructions are available in the
   <a href="https://docs.habana.ai/en/latest/AWS_Quick_Starts/index.html">
    Habana documentation
   </a>
   .
  </p>
  <p>
   Starting from the
   <a href="https://console.aws.amazon.com/ec2sp/v2/">
    EC2 console
   </a>
   in the us-east-1 region, I first click on
   <strong>
    Launch an instance
   </strong>
   and define a name for the instance ("habana-demo-julsimon").
  </p>
  <p>
   Then, I search the Amazon Marketplace for Habana AMIs.
  </p>
  <kbd>
   <img src="image01.webp"/>
  </kbd>
  <p>
   I pick the Habana Deep Learning Base AMI (Ubuntu 20.04).
  </p>
  <kbd>
   <img src="image02.webp"/>
  </kbd>
  <p>
   Next, I pick the
   <em>
    dl1.24xlarge
   </em>
   instance size (the only size available).
  </p>
  <kbd>
   <img src="image03.webp"/>
  </kbd>
  <p>
   Then, I select the keypair that I'll use to connect to the instance with
   <code>
    ssh
   </code>
   . If you don't have a keypair, you can create one in place.
  </p>
  <kbd>
   <img src="image04.webp"/>
  </kbd>
  <p>
   As a next step, I make sure that the instance allows incoming
   <code>
    ssh
   </code>
   traffic. I do not restrict the source address for simplicity, but you should definitely do it in your account.
  </p>
  <kbd>
   <img src="image05.webp"/>
  </kbd>
  <p>
   By default, this AMI will start an instance with 8GB of Amazon EBS storage, which won't be enough here. I bump storage to 50GB.
  </p>
  <kbd>
   <img src="image06.webp"/>
  </kbd>
  <p>
   Next, I assign an Amazon IAM role to the instance. In real life, this role should have the minimum set of permissions required to run your training job, such as the ability to read data from one of your Amazon S3 buckets. This role is not needed here as the dataset will be downloaded from the Hugging Face hub. If you're not familiar with IAM, I highly recommend reading the
   <a href="https://docs.aws.amazon.com/IAM/latest/UserGuide/getting-started.html">
    Getting Started
   </a>
   documentation.
  </p>
  <p>
   Then, I ask EC2 to provision my instance as a
   <a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-spot-instances.html">
    Spot Instance
   </a>
   , a great way to reduce the $13.11 per hour cost.
  </p>
  <kbd>
   <img src="image07.webp"/>
  </kbd>
  <p>
   Finally, I launch the instance. A couple of minutes later, the instance is ready and I can connect to it with
   <code>
    ssh
   </code>
   . Windows users can do the same with
   <em>
    PuTTY
   </em>
   by following the
   <a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/putty.html">
    documentation
   </a>
   .
  </p>
  <pre><code>ssh -i ~/.ssh/julsimon-keypair.pem ubuntu@ec2-18-207-189-109.compute-1.amazonaws.com
</code></pre>
  <p>
   On this instance, the last setup step is to pull the Habana container for PyTorch, which is the framework I'll use to fine-tune my model. You can find information on other prebuilt containers and on how to build your own in the Habana
   <a href="https://docs.habana.ai/en/latest/Installation_Guide/index.html">
    documentation
   </a>
   .
  </p>
  <pre><code>docker pull \
vault.habana.ai/gaudi-docker/1.5.0/ubuntu20.04/habanalabs/pytorch-installer-1.11.0:1.5.0-610
</code></pre>
  <p>
   Once the image has been pulled to the instance, I run it in interactive mode.
  </p>
  <pre><code>docker run -it \
--runtime=habana \
-e HABANA_VISIBLE_DEVICES=all \
-e OMPI_MCA_btl_vader_single_copy_mechanism=none \
--cap-add=sys_nice \
--net=host \
--ipc=host vault.habana.ai/gaudi-docker/1.5.0/ubuntu20.04/habanalabs/pytorch-installer-1.11.0:1.5.0-610
</code></pre>
  <p>
   I'm now ready to fine-tune my model.
  </p>
  <h2 class="relative group flex items-center">
   <span>
    Fine-tuning a text classification model on Habana Gaudi
   </span>
  </h2>
  <p>
   I first clone the
   <a href="https://github.com/huggingface/optimum-habana">
    Optimum Habana
   </a>
   repository inside the container I've just started.
  </p>
  <pre><code>git clone https://github.com/huggingface/optimum-habana.git
</code></pre>
  <p>
   Then, I install the Optimum Habana package from source.
  </p>
  <pre><code>cd optimum-habana
pip install .
</code></pre>
  <p>
   Then, I move to the subdirectory containing the text classification example and install the required Python packages.
  </p>
  <pre><code>cd examples/text-classification
pip install -r requirements.txt
</code></pre>
  <p>
   I can now launch the training job, which downloads the
   <a href="https://huggingface.co/bert-large-uncased-whole-word-masking">
    bert-large-uncased-whole-word-masking
   </a>
   model from the Hugging Face hub, and fine-tunes it on the
   <a href="https://www.microsoft.com/en-us/download/details.aspx?id=52398">
    MRPC
   </a>
   task of the
   <a href="https://gluebenchmark.com/">
    GLUE
   </a>
   benchmark.
  </p>
  <p>
   Please note that I'm fetching the Habana Gaudi configuration for BERT from the Hugging Face hub, and you could also use your own. In addition, other popular models are supported, and you can find their configuration file in the
   <a href="https://huggingface.co/Habana">
    Habana organization
   </a>
   .
  </p>
  <pre><code>python run_glue.py \
--model_name_or_path bert-large-uncased-whole-word-masking \
--gaudi_config_name Habana/bert-large-uncased-whole-word-masking \
--task_name mrpc \
--do_train \
--do_eval \
--per_device_train_batch_size 32 \
--learning_rate 3e-5 \
--num_train_epochs 3 \
--max_seq_length 128 \
--use_habana \
--use_lazy_mode \
--output_dir ./output/mrpc/
</code></pre>
  <p>
   After 2 minutes and 12 seconds, the job is complete and has achieved an excellent F1 score of 0.9181, which could certainly improve with more epochs.
  </p>
  <pre><code>***** train metrics *****
  epoch                    =        3.0
  train_loss               =      0.371
  train_runtime            = 0:02:12.85
  train_samples            =       3668
  train_samples_per_second =     82.824
  train_steps_per_second   =      2.597

***** eval metrics *****
  epoch                   =        3.0
  eval_accuracy           =     0.8505
  eval_combined_score     =     0.8736
  eval_f1                 =     0.8968
  eval_loss               =      0.385
  eval_runtime            = 0:00:06.45
  eval_samples            =        408
  eval_samples_per_second =     63.206
  eval_steps_per_second   =      7.901
</code></pre>
  <p>
   Last but not least, I terminate the EC2 instance to avoid unnecessary charges. Looking at the
   <a href="https://console.aws.amazon.com/ec2sp/v2/home/spot">
    Savings Summary
   </a>
   in the EC2 console, I see that I saved 70% thanks to Spot Instances, paying only $3.93 per hour instead of $13.11.
  </p>
  <kbd>
   <img src="image08.webp"/>
  </kbd>
  <p>
   As you can see, the combination of Transformers, Habana Gaudi, and AWS instances is powerful, simple, and cost-effective. Give it a try and let us know what you think. We definitely welcome your questions and feedback on the
   <a href="https://discuss.huggingface.co/">
    Hugging Face Forum
   </a>
   .
  </p>
  <hr/>
  <p>
   <em>
    Please
    <a href="https://developer.habana.ai/accelerate-transformer-training-on-habana-gaudi-processors-with-hugging-face/">
     reach out to Habana
    </a>
    to learn more about training Hugging Face models on Gaudi processors.
   </em>
  </p>
  <!-- HTML_TAG_END -->
  <hr/>
  <h3>
   About the Author
  </h3>
  <p>
   <img alt="Julien Simon" class="alignleft size-full wp-image-3134" height="115" loading="lazy" src="image01.webp" width="100"/>
   <strong>
    Julien Simon is the Chief Evangelist at Hugging Face
   </strong>
   , where he focuses on democratizing AI and making transformers accessible to everyone. A leading voice in open-source AI and small language models, he helps developers and enterprises bring their AI ideas to life. In his spare time, he reads the works of JRR Tolkien again and again.
  </p>
  <p>
  </p>
  <p>
  </p>
  <p>
  </p>
  <!-- '"` -->
 
  
  </body>
 </html>
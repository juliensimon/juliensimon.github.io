<!DOCTYPE html>
<html lang="en">
 <head>
    <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  
  <!-- Primary Meta Tags -->
  <title>
<!-- Umami Analytics -->
<script defer src="https://cloud.umami.is/script.js" data-website-id="27550dad-d418-4f5d-ad1b-dab573da1020"></script>
<link rel="dns-prefetch" href="//cloud.umami.is">Intel And Hugging Face Partner To Democratize Machine Learning Hardware Acceleration - Julien Simon | Open Source AI Expert</title>
  <meta name="title" content="Intel And Hugging Face Partner To Democratize Machine Learning Hardware Acceleration - Julien Simon | Open Source AI Expert"/>
  <meta name="description" content="Expert analysis and technical deep-dive on intel and hugging face partner to democratize machine learning hardware acceleration by Julien Simon, leading voice in open-source AI and former Chief Evangelist at Hugging Face. Comprehensive insights on transformers, small language models, and AI accessibility."/>
  <meta name="keywords" content="Hugging Face, Transformers, Open Source AI, Small Language Models, Machine Learning, AI, Natural Language Processing, NLP, Julien Simon, AI Expert, Open Source Expert, Hugging Face ExpertAnd-Hugging-Face-Partner-To-Democratize-Machine-Learning-Hardware-Acceleration, Intel And Hugging Face Partner To Democratize Machine Learning Hardware Acceleration"/>
  <meta name="author" content="Julien Simon"/>
  <meta name="robots" content="index, follow"/>
  
  <!-- Open Graph / Facebook -->
  <meta property="og:type" content="article"/>
  <meta property="og:url" content="https://julien.org/blog/2022-06-15-intel-and-hugging-face-partner-to-democratize-machine-learning-hardware-acceleration/"/>
  <meta property="og:title" content="Intel And Hugging Face Partner To Democratize Machine Learning Hardware Acceleration - Julien Simon | Open Source AI Expert"/>
  <meta property="og:description" content="Expert analysis and technical deep-dive on intel and hugging face partner to democratize machine learning hardware acceleration by Julien Simon, leading voice in open-source AI and former Chief Evangelist at Hugging Face. Comprehensive insights on transformers, small language models, and AI accessibility."/>
  <meta property="og:image" content="https://julien.org/assets/julien-simon-huggingface-expert.jpg"/>
  <meta property="og:site_name" content="Julien Simon - Open Source AI Expert"/>
  <meta property="article:author" content="Julien Simon"/>
  <meta property="article:published_time" content="2022-06-15T00:00:00Z"/>
  <meta property="article:section" content="Hugging Face"/>
  <meta property="article:tag" content="Hugging Face, Open Source AI, Transformers, Small Language Models"/>
  
  <!-- Twitter -->
  <meta property="twitter:card" content="summary_large_image"/>
  <meta property="twitter:url" content="https://julien.org/blog/2022-06-15-intel-and-hugging-face-partner-to-democratize-machine-learning-hardware-acceleration/"/>
  <meta property="twitter:title" content="Intel And Hugging Face Partner To Democratize Machine Learning Hardware Acceleration - Julien Simon | Open Source AI Expert"/>
  <meta property="twitter:description" content="Expert analysis and technical deep-dive on intel and hugging face partner to democratize machine learning hardware acceleration by Julien Simon, leading voice in open-source AI and former Chief Evangelist at Hugging Face. Comprehensive insights on transformers, small language models, and AI accessibility."/>
  <meta property="twitter:image" content="https://julien.org/assets/julien-simon-huggingface-expert.jpg"/>
  <meta property="twitter:creator" content="@julsimon"/>
  
  <!-- Canonical URL -->
  <link rel="canonical" href="https://julien.org/blog/2022-06-15-intel-and-hugging-face-partner-to-democratize-machine-learning-hardware-acceleration/"/>
  
  <!-- Author and Publisher -->
  <link rel="author" href="https://julien.org/"/>
  <link rel="publisher" href="https://julien.org/"/>
  
  <!-- Structured Data -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "BlogPosting",
    "headline": "Intel And Hugging Face Partner To Democratize Machine Learning Hardware Acceleration",
    "description": "Expert analysis and technical deep-dive on intel and hugging face partner to democratize machine learning hardware acceleration by Julien Simon, leading voice in open-source AI and former Chief Evangelist at Hugging Face. Comprehensive insights on transformers, small language models, and AI accessibility.",
    "image": "https://julien.org/assets/julien-simon-huggingface-expert.jpg",
    "author": {
      "@type": "Person",
      "name": "Julien Simon",
      "url": "https://julien.org/",
      "jobTitle": "Open Source AI Expert & Former Chief Evangelist",
      "worksFor": {
        "@type": "Organization",
        "name": "Hugging Face"
      },
      "sameAs": [
        "https://twitter.com/julsimon",
        "https://linkedin.com/in/juliensimon",
        "https://github.com/juliensimon"
      ]
    },
    "publisher": {
      "@type": "Organization",
      "name": "Julien Simon",
      "url": "https://julien.org/",
      "logo": {
        "@type": "ImageObject",
        "url": "https://julien.org/assets/julien-simon-logo.jpg"
      }
    },
    "datePublished": "2022-06-15T00:00:00Z",
    "dateModified": "2022-06-15T00:00:00Z",
    "mainEntityOfPage": {
      "@type": "WebPage",
      "@id": "https://julien.org/blog/2022-06-15-intel-and-hugging-face-partner-to-democratize-machine-learning-hardware-acceleration/"
    },
    "url": "https://julien.org/blog/2022-06-15-intel-and-hugging-face-partner-to-democratize-machine-learning-hardware-acceleration/",
    "keywords": "Hugging Face, Transformers, Open Source AI, Small Language Models, Machine Learning, AI, Natural Language Processing, NLP, Julien Simon, AI Expert, Open Source Expert, Hugging Face Expert, Intel-And-Hugging-Face-Partner-To-Democratize-Machine-Learning-Hardware-Acceleration",
    "articleSection": "Hugging Face",
    "inLanguage": "en-US",
    "isPartOf": {
      "@type": "Blog",
      "name": "Julien Simon - Open Source AI Expert Blog",
      "url": "https://julien.org/blog/"
    }
  }
  </script>
  
  <!-- Additional SEO Meta Tags -->
  <meta name="twitter:site" content="@julsimon"/>
  <meta name="twitter:creator" content="@julsimon"/>
  <meta name="theme-color" content="#FF6B35"/>
  <meta name="msapplication-TileColor" content="#FF6B35"/>
  <meta name="apple-mobile-web-app-capable" content="yes"/>
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent"/>
  
  <!-- Preconnect to external domains -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  
  <!-- Favicon -->
  <link rel="icon" type="image/x-icon" href="https://julien.org/assets/favicon.ico">
  
  <!-- Security Headers -->
  <meta http-equiv="X-Content-Type-Options" content="nosniff">
  <meta http-equiv="X-Frame-Options" content="DENY">
  <meta http-equiv="Referrer-Policy" content="strict-origin-when-cross-origin">
  <meta http-equiv="X-XSS-Protection" content="1; mode=block">
  <meta http-equiv="Permissions-Policy" content="camera=(), microphone=(), geolocation=(), interest-cohort=()">
  <meta http-equiv="Cross-Origin-Opener-Policy" content="same-origin">
  <meta http-equiv="Cross-Origin-Embedder-Policy" content="require-corp">

  
  <style>
   body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
            max-width: 800px;
            margin: 0 auto;
            padding: 20px;
            line-height: 1.6;
            color: #333;
        }
        h1, h2, h3, h4, h5, h6 {
            color: #2c3e50;
            margin-top: 2em;
            margin-bottom: 0.5em;
        }
        h1 { font-size: 2.5em; }
        h2 { font-size: 2em; }
        h3 { font-size: 1.5em; }
        img {
            max-width: 100%;
            height: auto;
            border-radius: 8px;
            box-shadow: 0 4px 6px rgba(0,0,0,0.1);
            margin: 1em 0;
        }
        /* Image alignment classes for text wraparound */
        img.alignleft {
            float: left;
            margin: 0.5em 1.5em 1em 0;
        }
        img.alignright {
            float: right;
            margin: 0.5em 0 1em 1.5em;
        }
        img.aligncenter {
            display: block;
            margin: 1em auto;
            float: none;
        }
        img.alignnone {
            float: none;
            margin: 1em 0;
        }
        /* Clear floats after images */
        .wp-block-image::after,
        p:has(img.alignleft)::after,
        p:has(img.alignright)::after {
            content: "";
            display: table;
            clear: both;
        }
        pre {
            background: #f8f9fa;
            border: 1px solid #e9ecef;
            border-radius: 4px;
            padding: 1em;
            overflow-x: auto;
        }
        code {
            background: #f8f9fa;
            padding: 0.2em 0.4em;
            border-radius: 3px;
            font-family: 'Monaco', 'Consolas', monospace;
        }
        blockquote {
            border-left: 4px solid #3498db;
            margin: 1em 0;
            padding-left: 1em;
            font-style: italic;
            color: #7f8c8d;
        }
        a {
            color: #3498db;
            text-decoration: none;
        }
        a:hover {
            text-decoration: underline;
        }
        p {
            margin-bottom: 1em;
        }
        /* Hugging Face specific styling */
        .prose {
            max-width: none;
        }
        .prose pre {
            background: #1e293b;
            color: #e2e8f0;
        }
        .prose code {
            background: #f1f5f9;
            color: #dc2626;
        }
  </style>
 </head>
 <body>

  <div style="margin-bottom: 2em;">
   <a href="../../../../huggingface-blog-posts.html" style="color: #3498db; text-decoration: none; font-weight: 500;">← Back to Hugging Face Blog Posts</a>
  </div>
  <h1>
   Intel and Hugging Face Partner to Democratize Machine Learning Hardware Acceleration
  </h1>
  <p style="color: #666; font-style: italic; margin-bottom: 1em;">
   Published: 2022-06-15
  </p>
  
  <p style="color: #666; font-style: italic; margin-bottom: 2em;">
   Originally published at
   <a href="https://huggingface.co/blog/intel">
    https://huggingface.co/blog/intel
   </a>
  </p>
  <!-- HTML_TAG_START -->
  <p>
   <img alt="image" src="image01.webp"/>
  </p>
  <p>
   The mission of Hugging Face is to democratize good machine learning and maximize its positive impact across industries and society. Not only do we strive to advance Transformer models, but we also work hard on simplifying their adoption.
  </p>
  <p>
   Today, we're excited to announce that Intel has officially joined our
   <a href="https://huggingface.co/hardware">
    Hardware Partner Program
   </a>
   .  Thanks to the
   <a href="https://github.com/huggingface/optimum-intel">
    Optimum
   </a>
   open-source library, Intel and Hugging Face will collaborate to build state-of-the-art hardware acceleration to train, fine-tune and predict with Transformers.
  </p>
  <p>
   Transformer models are increasingly large and complex, which can cause production challenges for latency-sensitive applications like search or chatbots. Unfortunately, latency optimization has long been a hard problem for Machine Learning (ML) practitioners. Even with deep knowledge of the underlying framework and hardware platform, it takes a lot of trial and error to figure out which knobs and features to leverage.
  </p>
  <p>
   Intel provides a complete foundation for accelerated AI with the Intel Xeon Scalable CPU platform and a wide range of hardware-optimized AI software tools, frameworks, and libraries. Thus, it made perfect sense for Hugging Face and Intel to join forces and collaborate on building powerful model optimization tools that let users achieve the best performance, scale, and productivity on Intel platforms.
  </p>
  <p>
   “*We’re excited to work with Hugging Face to bring the latest innovations of Intel Xeon hardware and Intel AI software to the Transformers community, through open source integration and integrated developer experiences.*”, says Wei Li, Intel Vice President &amp; General Manager, AI and Analytics.
  </p>
  <p>
   In recent months, Intel and Hugging Face collaborated on scaling Transformer workloads. We published detailed tuning guides and benchmarks on inference (
   <a href="https://huggingface.co/blog/bert-cpu-scaling-part-1">
    part 1
   </a>
   ,
   <a href="https://huggingface.co/blog/bert-cpu-scaling-part-2">
    part 2
   </a>
   ) and achieved
   <a href="https://huggingface.co/blog/infinity-cpu-performance">
    single-digit millisecond latency
   </a>
   for DistilBERT on the latest Intel Xeon Ice Lake CPUs. On the training side, we added support for
   <a href="https://huggingface.co/blog/getting-started-habana">
    Habana Gaudi
   </a>
   accelerators, which deliver up to 40% better price-performance than GPUs.
  </p>
  <p>
   The next logical step was to expand on this work and share it with the ML community. Enter the
   <a href="https://github.com/huggingface/optimum-intel">
    Optimum Intel
   </a>
   open source library! Let’s take a deeper look at it.
  </p>
  <h2 class="relative group flex items-center">
   <span>
    Get Peak Transformers Performance with Optimum Intel
   </span>
  </h2>
  <p>
   <a href="https://github.com/huggingface/optimum">
    Optimum
   </a>
   is an open-source library created by Hugging Face to simplify Transformer acceleration across a growing range of training and inference devices. Thanks to built-in optimization techniques, you can start accelerating your workloads in minutes, using ready-made scripts, or applying minimal changes to your existing code. Beginners can use Optimum out of the box with excellent results. Experts can keep tweaking for maximum performance.
  </p>
  <p>
   <a href="https://github.com/huggingface/optimum-intel">
    Optimum Intel
   </a>
   is part of Optimum and builds on top of the
   <a href="https://www.intel.com/content/www/us/en/developer/tools/oneapi/neural-compressor.html">
    Intel Neural Compressor
   </a>
   (INC). INC is an
   <a href="https://github.com/intel/neural-compressor">
    open-source library
   </a>
   that delivers unified interfaces across multiple deep learning frameworks for popular network compression technologies, such as quantization, pruning, and knowledge distillation. This tool supports automatic accuracy-driven tuning strategies to help users quickly build the best quantized model.
  </p>
  <p>
   With Optimum Intel, you can apply state-of-the-art optimization techniques to your Transformers with minimal effort. Let’s look at a complete example.
  </p>
  <h2 class="relative group flex items-center">
   <span>
    Case study: Quantizing DistilBERT with Optimum Intel
   </span>
  </h2>
  <p>
   In this example, we will run post-training quantization on a DistilBERT model fine-tuned for classification. Quantization is a process that shrinks memory and compute requirements by reducing the bit width of model parameters. For example, you can often replace 32-bit floating-point parameters with 8-bit integers at the expense of a small drop in prediction accuracy.
  </p>
  <p>
   We have already fine-tuned the original model to classify product reviews for shoes according to their star rating (from 1 to 5 stars). You can view this
   <a href="https://huggingface.co/juliensimon/distilbert-amazon-shoe-reviews">
    model
   </a>
   and its
   <a href="https://huggingface.co/juliensimon/distilbert-amazon-shoe-reviews-quantized?">
    quantized
   </a>
   version on the Hugging Face hub. You can also test the original model in this
   <a href="https://huggingface.co/spaces/juliensimon/amazon-shoe-reviews-spaces">
    Space
   </a>
   .
  </p>
  <p>
   Let’s get started! All code is available in this
   <a href="https://github.com/juliensimon/huggingface-demos/blob/main/amazon-shoes/03_optimize_inc_quantize.ipynb">
    notebook
   </a>
   .
  </p>
  <p>
   As usual, the first step is to install all required libraries. It’s worth mentioning that we have to work with a CPU-only version of PyTorch for the quantization process to work correctly.
  </p>
  <pre><code>pip -q uninstall torch -y 
pip -q install torch==1.11.0+cpu --extra-index-url https://download.pytorch.org/whl/cpu
pip -q install transformers datasets optimum[neural-compressor] evaluate --upgrade
</code></pre>
  <p>
   Then, we prepare an evaluation dataset to assess model performance during quantization. Starting from the dataset we used to fine-tune the original model, we only keep a few thousand reviews and their labels and save them to local storage.
  </p>
  <p>
   Next, we load the original model, its tokenizer, and the evaluation dataset from the Hugging Face hub.
  </p>
  <p>
   Next, we define an evaluation function that computes model metrics on the evaluation dataset. This allows the Optimum Intel library to compare these metrics before and after quantization. For this purpose, the Hugging Face
   <a href="https://github.com/huggingface/evaluate/">
    evaluate
   </a>
   library is very convenient!
  </p>
  <pre><code>import evaluate

def eval_func(model):
    task_evaluator = evaluate.evaluator("text-classification")
    results = task_evaluator.compute(
        model_or_pipeline=model,
        tokenizer=tokenizer,
        data=eval_dataset,
        metric=evaluate.load("accuracy"),
        label_column="labels",
        label_mapping=model.config.label2id,
    )
    return results["accuracy"]
</code></pre>
  <p>
   We then set up the quantization job using a [configuration]. You can find details on this configuration on the Neural Compressor
   <a href="https://github.com/intel/neural-compressor/blob/master/docs/source/quantization.md">
    documentation
   </a>
   . Here, we go for post-training dynamic quantization with an acceptable accuracy drop of 5%. If accuracy drops more than the allowed 5%, different part of the model will then be quantized until it an acceptable drop in accuracy or if the maximum number of trials, here set to 10, is reached.
  </p>
  <pre><code>from neural_compressor.config import AccuracyCriterion, PostTrainingQuantConfig, TuningCriterion

tuning_criterion = TuningCriterion(max_trials=10)
accuracy_criterion = AccuracyCriterion(tolerable_loss=0.05)
# Load the quantization configuration detailing the quantization we wish to apply
quantization_config = PostTrainingQuantConfig(
    approach="dynamic",
    accuracy_criterion=accuracy_criterion,
    tuning_criterion=tuning_criterion,
)
</code></pre>
  <p>
   We can now launch the quantization job and save the resulting model and its configuration file to local storage.
  </p>
  <pre><code>from neural_compressor.config import PostTrainingQuantConfig
from optimum.intel.neural_compressor import INCQuantizer

# The directory where the quantized model will be saved
save_dir = "./model_inc"
quantizer = INCQuantizer.from_pretrained(model=model, eval_fn=eval_func)
quantizer.quantize(quantization_config=quantization_config, save_directory=save_dir)
</code></pre>
  <p>
   The log tells us that Optimum Intel has quantized 38
   <code>
    Linear
   </code>
   and 2
   <code>
    Embedding
   </code>
   operators.
  </p>
  <pre><code>[INFO] |******Mixed Precision Statistics*****|
[INFO] +----------------+----------+---------+
[INFO] |    Op Type     |  Total   |   INT8  |
[INFO] +----------------+----------+---------+
[INFO] |   Embedding    |    2     |    2    |
[INFO] |     Linear     |    38    |    38   |
[INFO] +----------------+----------+---------+
</code></pre>
  <p>
   Comparing the first layer of the original model (
   <code>
    model.distilbert.transformer.layer[0]
   </code>
   ) and its quantized version (
   <code>
    inc_model.distilbert.transformer.layer[0]
   </code>
   ), we see that
   <code>
    Linear
   </code>
   has indeed been replaced by
   <code>
    DynamicQuantizedLinear
   </code>
   , its quantized equivalent.
  </p>
  <pre><code># Original model

TransformerBlock(
  (attention): MultiHeadSelfAttention(
    (dropout): Dropout(p=0.1, inplace=False)
    (q_lin): Linear(in_features=768, out_features=768, bias=True)
    (k_lin): Linear(in_features=768, out_features=768, bias=True)
    (v_lin): Linear(in_features=768, out_features=768, bias=True)
    (out_lin): Linear(in_features=768, out_features=768, bias=True)
  )
  (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
  (ffn): FFN(
    (dropout): Dropout(p=0.1, inplace=False)
    (lin1): Linear(in_features=768, out_features=3072, bias=True)
    (lin2): Linear(in_features=3072, out_features=768, bias=True)
  )
  (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
)
</code></pre>
  <pre><code># Quantized model

TransformerBlock(
  (attention): MultiHeadSelfAttention(
    (dropout): Dropout(p=0.1, inplace=False)
    (q_lin): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_channel_affine)
    (k_lin): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_channel_affine)
    (v_lin): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_channel_affine)
    (out_lin): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_channel_affine)
  )
  (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
  (ffn): FFN(
    (dropout): Dropout(p=0.1, inplace=False)
    (lin1): DynamicQuantizedLinear(in_features=768, out_features=3072, dtype=torch.qint8, qscheme=torch.per_channel_affine)
    (lin2): DynamicQuantizedLinear(in_features=3072, out_features=768, dtype=torch.qint8, qscheme=torch.per_channel_affine)
  )
  (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
)
</code></pre>
  <p>
   Very well, but how does this impact accuracy and prediction time?
  </p>
  <p>
   Before and after each quantization step, Optimum Intel runs the evaluation function on the current model. The accuracy of the quantized model is now a bit lower  (
   <code>
    0.546
   </code>
   ) than the original model (
   <code>
    0.574
   </code>
   ). We also see that the evaluation step of the quantized model was 1.34x faster than the original model. Not bad for a few lines of code!
  </p>
  <pre><code>[INFO] |**********************Tune Result Statistics**********************|
[INFO] +--------------------+----------+---------------+------------------+
[INFO] |     Info Type      | Baseline | Tune 1 result | Best tune result |
[INFO] +--------------------+----------+---------------+------------------+
[INFO] |      Accuracy      | 0.5740   |    0.5460     |     0.5460       |
[INFO] | Duration (seconds) | 13.1534  |    9.7695     |     9.7695       |
[INFO] +--------------------+----------+---------------+------------------+
</code></pre>
  <p>
   You can find the resulting
   <a href="https://huggingface.co/juliensimon/distilbert-amazon-shoe-reviews-quantized">
    model
   </a>
   hosted on the Hugging Face hub. To load a quantized model hosted locally or on the 🤗 hub, you can do as follows :
  </p>
  <pre><code>from optimum.intel.neural_compressor import INCModelForSequenceClassification

inc_model = INCModelForSequenceClassification.from_pretrained(save_dir)
</code></pre>
  <h2 class="relative group flex items-center">
   <span>
    We’re only getting started
   </span>
  </h2>
  <p>
   In this example, we showed you how to easily quantize models post-training with Optimum Intel, and that’s just the beginning. The library supports other types of quantization as well as pruning, a technique that zeroes or removes model parameters that have little or no impact on the predicted outcome.
  </p>
  <p>
   We are excited to partner with Intel to bring Hugging Face users peak efficiency on the latest Intel Xeon CPUs and Intel AI libraries. Please
   <a href="https://github.com/huggingface/optimum-intel">
    give Optimum Intel a star
   </a>
   to get updates, and stay tuned for many upcoming features!
  </p>
  <p>
   <em>
    Many thanks to
    <a href="https://github.com/echarlaix">
     Ella Charlaix
    </a>
    for her help on this post.
   </em>
  </p>
  <!-- HTML_TAG_END -->
  <hr/>
  <h3>
   About the Author
  </h3>
  <p>
   <strong>
    Julien Simon is the Chief Evangelist at Hugging Face
   </strong>
   , where he focuses on democratizing AI and making transformers accessible to everyone. A leading voice in open-source AI and small language models, he helps developers and enterprises bring their AI ideas to life. In his spare time, he reads the works of JRR Tolkien again and again.
  </p>
  <p>
  </p>
  <p>
  </p>
  <p>
  </p>
  <!-- '"` -->
 
  
  </body>
 </html>
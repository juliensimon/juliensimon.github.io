<!DOCTYPE html>
<html lang="en">
 <head>
    <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  
  <!-- Primary Meta Tags -->
  <title>A Chatbot On Your Laptop Phi 2 On Intel Meteor Lake - Julien Simon | Open Source AI Expert</title>
  <meta name="title" content="A Chatbot On Your Laptop Phi 2 On Intel Meteor Lake - Julien Simon | Open Source AI Expert"/>
  <meta name="description" content="Expert analysis and technical deep-dive on a chatbot on your laptop phi 2 on intel meteor lake by Julien Simon, leading voice in open-source AI and former Chief Evangelist at Hugging Face. Comprehensive insights on transformers, small language models, and AI accessibility."/>
  <meta name="keywords" content="Hugging Face, Transformers, Open Source AI, Small Language Models, Machine Learning, AI, Natural Language Processing, NLP, Julien Simon, AI Expert, Open Source Expert, Hugging Face ExpertChatbot-On-Your-Laptop-Phi-2-On-Intel-Meteor-Lake, A Chatbot On Your Laptop Phi 2 On Intel Meteor Lake"/>
  <meta name="author" content="Julien Simon"/>
  <meta name="robots" content="index, follow"/>
  
  <!-- Open Graph / Facebook -->
  <meta property="og:type" content="article"/>
  <meta property="og:url" content="https://julien.org/blog/2024-03-20-a-chatbot-on-your-laptop-phi-2-on-intel-meteor-lake/"/>
  <meta property="og:title" content="A Chatbot On Your Laptop Phi 2 On Intel Meteor Lake - Julien Simon | Open Source AI Expert"/>
  <meta property="og:description" content="Expert analysis and technical deep-dive on a chatbot on your laptop phi 2 on intel meteor lake by Julien Simon, leading voice in open-source AI and former Chief Evangelist at Hugging Face. Comprehensive insights on transformers, small language models, and AI accessibility."/>
  <meta property="og:image" content="https://julien.org/assets/julien-simon-huggingface-expert.jpg"/>
  <meta property="og:site_name" content="Julien Simon - Open Source AI Expert"/>
  <meta property="article:author" content="Julien Simon"/>
  <meta property="article:published_time" content="2024-03-20T00:00:00Z"/>
  <meta property="article:section" content="Hugging Face"/>
  <meta property="article:tag" content="Hugging Face, Open Source AI, Transformers, Small Language Models"/>
  
  <!-- Twitter -->
  <meta property="twitter:card" content="summary_large_image"/>
  <meta property="twitter:url" content="https://julien.org/blog/2024-03-20-a-chatbot-on-your-laptop-phi-2-on-intel-meteor-lake/"/>
  <meta property="twitter:title" content="A Chatbot On Your Laptop Phi 2 On Intel Meteor Lake - Julien Simon | Open Source AI Expert"/>
  <meta property="twitter:description" content="Expert analysis and technical deep-dive on a chatbot on your laptop phi 2 on intel meteor lake by Julien Simon, leading voice in open-source AI and former Chief Evangelist at Hugging Face. Comprehensive insights on transformers, small language models, and AI accessibility."/>
  <meta property="twitter:image" content="https://julien.org/assets/julien-simon-huggingface-expert.jpg"/>
  <meta property="twitter:creator" content="@julsimon"/>
  
  <!-- Canonical URL -->
  <link rel="canonical" href="https://julien.org/blog/2024-03-20-a-chatbot-on-your-laptop-phi-2-on-intel-meteor-lake/"/>
  
  <!-- Author and Publisher -->
  <link rel="author" href="https://julien.org/"/>
  <link rel="publisher" href="https://julien.org/"/>
  
  <!-- Structured Data -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "BlogPosting",
    "headline": "A Chatbot On Your Laptop Phi 2 On Intel Meteor Lake",
    "description": "Expert analysis and technical deep-dive on a chatbot on your laptop phi 2 on intel meteor lake by Julien Simon, leading voice in open-source AI and former Chief Evangelist at Hugging Face. Comprehensive insights on transformers, small language models, and AI accessibility.",
    "image": "https://julien.org/assets/julien-simon-huggingface-expert.jpg",
    "author": {
      "@type": "Person",
      "name": "Julien Simon",
      "url": "https://julien.org/",
      "jobTitle": "Open Source AI Expert & Former Chief Evangelist",
      "worksFor": {
        "@type": "Organization",
        "name": "Hugging Face"
      },
      "sameAs": [
        "https://twitter.com/julsimon",
        "https://linkedin.com/in/juliensimon",
        "https://github.com/juliensimon"
      ]
    },
    "publisher": {
      "@type": "Organization",
      "name": "Julien Simon",
      "url": "https://julien.org/",
      "logo": {
        "@type": "ImageObject",
        "url": "https://julien.org/assets/julien-simon-logo.jpg"
      }
    },
    "datePublished": "2024-03-20T00:00:00Z",
    "dateModified": "2024-03-20T00:00:00Z",
    "mainEntityOfPage": {
      "@type": "WebPage",
      "@id": "https://julien.org/blog/2024-03-20-a-chatbot-on-your-laptop-phi-2-on-intel-meteor-lake/"
    },
    "url": "https://julien.org/blog/2024-03-20-a-chatbot-on-your-laptop-phi-2-on-intel-meteor-lake/",
    "keywords": "Hugging Face, Transformers, Open Source AI, Small Language Models, Machine Learning, AI, Natural Language Processing, NLP, Julien Simon, AI Expert, Open Source Expert, Hugging Face Expert, A-Chatbot-On-Your-Laptop-Phi-2-On-Intel-Meteor-Lake",
    "articleSection": "Hugging Face",
    "inLanguage": "en-US",
    "isPartOf": {
      "@type": "Blog",
      "name": "Julien Simon - Open Source AI Expert Blog",
      "url": "https://julien.org/blog/"
    }
  }
  </script>
  
  <!-- Additional SEO Meta Tags -->
  <meta name="twitter:site" content="@julsimon"/>
  <meta name="twitter:creator" content="@julsimon"/>
  <meta name="theme-color" content="#FF6B35"/>
  <meta name="msapplication-TileColor" content="#FF6B35"/>
  <meta name="apple-mobile-web-app-capable" content="yes"/>
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent"/>
  
  <!-- Preconnect to external domains -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  
  <!-- Favicon -->
  <link rel="icon" type="image/x-icon" href="https://julien.org/assets/favicon.ico">
  
  <!-- Security Headers -->
  <meta http-equiv="X-Content-Type-Options" content="nosniff">
  <meta http-equiv="X-Frame-Options" content="DENY">
  <meta http-equiv="Referrer-Policy" content="strict-origin-when-cross-origin">
  <meta http-equiv="X-XSS-Protection" content="1; mode=block">
  <meta http-equiv="Permissions-Policy" content="camera=(), microphone=(), geolocation=(), interest-cohort=()">
  <meta http-equiv="Cross-Origin-Opener-Policy" content="same-origin">
  <meta http-equiv="Cross-Origin-Embedder-Policy" content="require-corp">

  
  <style>
   body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
            max-width: 800px;
            margin: 0 auto;
            padding: 20px;
            line-height: 1.6;
            color: #333;
        }
        h1, h2, h3, h4, h5, h6 {
            color: #2c3e50;
            margin-top: 2em;
            margin-bottom: 0.5em;
        }
        h1 { font-size: 2.5em; }
        h2 { font-size: 2em; }
        h3 { font-size: 1.5em; }
        img {
            max-width: 100%;
            height: auto;
            border-radius: 8px;
            box-shadow: 0 4px 6px rgba(0,0,0,0.1);
            margin: 1em 0;
        }
        /* Image alignment classes for text wraparound */
        img.alignleft {
            float: left;
            margin: 0.5em 1.5em 1em 0;
        }
        img.alignright {
            float: right;
            margin: 0.5em 0 1em 1.5em;
        }
        img.aligncenter {
            display: block;
            margin: 1em auto;
            float: none;
        }
        img.alignnone {
            float: none;
            margin: 1em 0;
        }
        /* Clear floats after images */
        .wp-block-image::after,
        p:has(img.alignleft)::after,
        p:has(img.alignright)::after {
            content: "";
            display: table;
            clear: both;
        }
        pre {
            background: #f8f9fa;
            border: 1px solid #e9ecef;
            border-radius: 4px;
            padding: 1em;
            overflow-x: auto;
        }
        code {
            background: #f8f9fa;
            padding: 0.2em 0.4em;
            border-radius: 3px;
            font-family: 'Monaco', 'Consolas', monospace;
        }
        blockquote {
            border-left: 4px solid #3498db;
            margin: 1em 0;
            padding-left: 1em;
            font-style: italic;
            color: #7f8c8d;
        }
        a {
            color: #3498db;
            text-decoration: none;
        }
        a:hover {
            text-decoration: underline;
        }
        p {
            margin-bottom: 1em;
        }
        /* Hugging Face specific styling */
        .prose {
            max-width: none;
        }
        .prose pre {
            background: #1e293b;
            color: #e2e8f0;
        }
        .prose code {
            background: #f1f5f9;
            color: #dc2626;
        }
  </style>
 </head>
 <body>

  <div style="margin-bottom: 2em;">
   <a href="../../../../huggingface-blog-posts.html" style="color: #3498db; text-decoration: none; font-weight: 500;">‚Üê Back to Hugging Face Blog Posts</a>
  </div>
  <h1>
   A Chatbot on your Laptop: Phi-2 on Intel Meteor Lake
  </h1>
  <p style="color: #666; font-style: italic; margin-bottom: 1em;">
   Published: 2024-03-20
  </p>
  
  <p style="color: #666; font-style: italic; margin-bottom: 2em;">
   Originally published at
   <a href="https://huggingface.co/blog/phi2-intel-meteor-lake">
    https://huggingface.co/blog/phi2-intel-meteor-lake
   </a>
  </p>
  <!-- HTML_TAG_START -->
  <p align="center">
   <img alt="David vs. Goliath revisited" src="image01.webp" width="512"/>
   <br/>
  </p>
  <p>
   Because of their impressive abilities, large language models (LLMs) require significant computing power, which is seldom available on personal computers. Consequently, we have no choice but to deploy them on powerful bespoke AI servers hosted on-premises or in the cloud.
  </p>
  <h2 class="relative group flex items-center">
   <span>
    Why local LLM inference is desirable
   </span>
  </h2>
  <p>
   What if we could run state-of-the-art open-source LLMs on a typical personal computer? Wouldn't we enjoy benefits like:
  </p>
  <ul>
   <li>
    <strong>
     Increased privacy
    </strong>
    : our data would not be sent to an external API for inference.
   </li>
   <li>
    <strong>
     Lower latency
    </strong>
    : we would save network round trips.
   </li>
   <li>
    <strong>
     Offline work
    </strong>
    : we could work without network connectivity (a frequent flyer's dream!).
   </li>
   <li>
    <strong>
     Lower cost
    </strong>
    : we wouldn't spend any money on API calls or model hosting.
   </li>
   <li>
    <strong>
     Customizability
    </strong>
    : each user could find the models that best fit the tasks they work on daily, and they could even fine-tune them or use local Retrieval-Augmented Generation (RAG) to increase relevance.
   </li>
  </ul>
  <p>
   This all sounds very exciting indeed. So why aren't we doing it already? Returning to our opening statement, your typical reasonably priced laptop doesn't pack enough compute punch to run LLMs with acceptable performance. There is no multi-thousand-core GPU and no lightning-fast High Memory Bandwidth in sight.
  </p>
  <p>
   A lost cause, then? Of course not.
  </p>
  <h2 class="relative group flex items-center">
   <span>
    Why local LLM inference is now possible
   </span>
  </h2>
  <p>
   There's nothing that the human mind can't make smaller, faster, more elegant, and more cost-effective. In recent months, the AI community has worked hard to shrink models without compromising their predictive quality.  Three areas are exciting:
  </p>
  <ul>
   <li>
    <p>
     <strong>
      Hardware acceleration
     </strong>
     : modern CPU architectures embed hardware dedicated to accelerating the most common deep learning operators, such as matrix multiplication or convolution, enabling new Generative AI applications on AI PCs and significantly improving their speed and efficiency.
    </p>
   </li>
   <li>
    <p>
     <strong>
      Small Language Models (SLMs)
     </strong>
     : thanks to innovative architectures and training techniques, these models are on par or even better than larger models. Because they have fewer parameters, inference requires less computing and memory, making them excellent candidates for resource-constrained environments.
    </p>
   </li>
   <li>
    <p>
     <strong>
      Quantization
     </strong>
     : Quantization is a process that lowers memory and computing requirements by reducing the bit width of model weights and activations, for example, from 16-bit floating point (
     <code>
      fp16
     </code>
     ) to 8-bit integers (
     <code>
      int8
     </code>
     ). Reducing the number of bits means that the resulting model requires less memory at inference time, speeding up latency for memory-bound steps like the decoding phase when text is generated. In addition, operations like matrix multiplication can be performed faster thanks to integer arithmetic when quantizing both the weights and activations.
    </p>
   </li>
  </ul>
  <p>
   In this post, we'll leverage all of the above. Starting from the Microsoft
   <a href="https://huggingface.co/microsoft/phi-2">
    Phi-2
   </a>
   model, we will apply 4-bit quantization on the model weights, thanks to the Intel OpenVINO integration in our
   <a href="https://github.com/huggingface/optimum-intel">
    Optimum Intel
   </a>
   library. Then, we will run inference on a mid-range laptop powered by an Intel Meteor Lake CPU.
  </p>
  <blockquote>
   <p>
    <strong>
     <em>
      NOTE
     </em>
    </strong>
    : If you're interested in applying quantization on both weights and activations, you can find more information in our
    <a href="https://huggingface.co/docs/optimum/main/en/intel/optimization_ov#static-quantization">
     documentation
    </a>
    .
   </p>
  </blockquote>
  <p>
   Let's get to work.
  </p>
  <h2 class="relative group flex items-center">
   <span>
    Intel Meteor Lake
   </span>
  </h2>
  <p>
   Launched in December 2023, Intel Meteor Lake, now renamed to
   <a href="https://www.intel.com/content/www/us/en/products/details/processors/core-ultra.html">
    Core Ultra
   </a>
   , is a new
   <a href="https://www.intel.com/content/www/us/en/content-details/788851/meteor-lake-architecture-overview.html">
    architecture
   </a>
   optimized for high-performance laptops.
  </p>
  <p>
   The first Intel client processor to use a chiplet architecture, Meteor Lake includes:
  </p>
  <ul>
   <li>
    <p>
     A
     <strong>
      power-efficient CPU
     </strong>
     with up to 16 cores,
    </p>
   </li>
   <li>
    <p>
     An
     <strong>
      integrated GPU (iGPU)
     </strong>
     with up to 8 Xe cores, each featuring 16 Xe Vector Engines (XVE). As the name implies, an XVE can perform vector operations on 256-bit vectors. It also implements the DP4a instruction, which computes a dot product between two vectors of 4-byte values, stores the result in a 32-bit integer, and adds it to a third 32-bit integer.
    </p>
   </li>
   <li>
    <p>
     A
     <strong>
      Neural Processing Unit (NPU)
     </strong>
     , a first for Intel architectures. The NPU is a dedicated AI engine built for efficient client AI. It is optimized to handle demanding AI computations efficiently, freeing up the main CPU and graphics for other tasks. Compared to using the CPU or the iGPU for AI tasks, the NPU is designed to be more power-efficient.
    </p>
   </li>
  </ul>
  <p>
   To run the demo below, we selected a
   <a href="https://www.amazon.com/MSI-Prestige-Evo-Laptop-A1MG-029US/dp/B0CP9Y8Q6T/">
    mid-range laptop
   </a>
   powered by a
   <a href="https://www.intel.com/content/www/us/en/products/sku/236847/intel-core-ultra-7-processor-155h-24m-cache-up-to-4-80-ghz/specifications.html">
    Core Ultra 7 155H CPU
   </a>
   . Now, let's pick a lovely small language model to run on this laptop.
  </p>
  <blockquote>
   <p>
    <strong>
     <em>
      NOTE
     </em>
    </strong>
    : To run this code on Linux, install your GPU driver by following
    <a href="https://docs.openvino.ai/2024/get-started/configurations/configurations-intel-gpu.html">
     these instructions
    </a>
    .
   </p>
  </blockquote>
  <h2 class="relative group flex items-center">
   <span>
    The Microsoft Phi-2 model
   </span>
  </h2>
  <p>
   <a href="https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-of-small-language-models/">
    Released
   </a>
   in December 2023,
   <a href="https://huggingface.co/microsoft/phi-2">
    Phi-2
   </a>
   is a 2.7-billion parameter model trained for text generation.
  </p>
  <p>
   On reported benchmarks, unfazed by its smaller size, Phi-2 outperforms some of the best 7-billion and 13-billion LLMs and even stays within striking distance of the much larger Llama-2 70B model.
  </p>
  <kbd>
   <img src="image02.webp"/ alt="Illustration for The Microsoft Phi-2 model">
  </kbd>
  <p>
   This makes it an exciting candidate for laptop inference. Curious readers may also want to experiment with the 1.1-billion
   <a href="https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0">
    TinyLlama
   </a>
   model.
  </p>
  <p>
   Now, let's see how we can shrink the model to make it smaller and faster.
  </p>
  <h2 class="relative group flex items-center">
   <span>
    Quantization with Intel OpenVINO and Optimum Intel
   </span>
  </h2>
  <p>
   Intel OpenVINO is an open-source toolkit for optimizing AI inference on many Intel hardware platforms (
   <a href="https://github.com/openvinotoolkit/openvino">
    Github
   </a>
   ,
   <a href="https://docs.openvino.ai/2024/home.html">
    documentation
   </a>
   ), notably through model quantization.
  </p>
  <p>
   Partnering with Intel, we have integrated OpenVINO in Optimum Intel, our open-source library dedicated to accelerating Hugging Face models on Intel platforms (
   <a href="https://github.com/huggingface/optimum-intel">
    Github
   </a>
   ,
   <a href="https://huggingface.co/docs/optimum/intel/index">
    documentation
   </a>
   ).
  </p>
  <p>
   First make sure you have the latest version of
   <code>
    optimum-intel
   </code>
   with all the necessary libraries installed:
  </p>
  <pre><code class="language-bash">pip install --upgrade-strategy eager optimum[openvino,nncf]
</code></pre>
  <p>
   This integration makes quantizing Phi-2 to 4-bit straightforward. We define a quantization configuration, set the optimization parameters, and load the model from the hub. Once it has been quantized and optimized, we store it locally.
  </p>
  <pre><code class="language-python"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, pipeline
<span class="hljs-keyword">from</span> optimum.intel <span class="hljs-keyword">import</span> OVModelForCausalLM, OVWeightQuantizationConfig

model_id = <span class="hljs-string">"microsoft/phi-2"</span>
device = <span class="hljs-string">"gpu"</span>

q_config = OVWeightQuantizationConfig(bits=<span class="hljs-number">4</span>, group_size=<span class="hljs-number">128</span>, ratio=<span class="hljs-number">0.8</span>)


ov_config = {<span class="hljs-string">"PERFORMANCE_HINT"</span>: <span class="hljs-string">"LATENCY"</span>, <span class="hljs-string">"CACHE_DIR"</span>: <span class="hljs-string">"model_cache"</span>, <span class="hljs-string">"INFERENCE_PRECISION_HINT"</span>: <span class="hljs-string">"f32"</span>}

tokenizer = AutoTokenizer.from_pretrained(model_id)
model = OVModelForCausalLM.from_pretrained(
    model_id,
    export=<span class="hljs-literal">True</span>, 
    quantization_config=q_config,
    device=device,
    ov_config=ov_config,
  )


model.<span class="hljs-built_in">compile</span>()
pipe = pipeline(<span class="hljs-string">"text-generation"</span>, model=model, tokenizer=tokenizer)
results = pipe(<span class="hljs-string">"He's a dreadful magician and"</span>)

save_directory = <span class="hljs-string">"phi-2-openvino"</span>
model.save_pretrained(save_directory)
tokenizer.save_pretrained(save_directory)
</code></pre>
  <p>
   The
   <code>
    ratio
   </code>
   parameter controls the fraction of weights we'll quantize to 4-bit (here, 80%) and the rest to 8-bit. The
   <code>
    group_size
   </code>
   parameter defines the size of the weight quantization groups (here, 128), each group having its scaling factor. Decreasing these two values usually improves accuracy at the expense of model size and inference latency.
  </p>
  <p>
   You can find more information on weight quantization in our
   <a href="https://huggingface.co/docs/optimum/main/en/intel/optimization_ov#weight-only-quantization">
    documentation
   </a>
   .
  </p>
  <blockquote>
   <p>
    <strong>
     <em>
      NOTE
     </em>
    </strong>
    : the entire notebook with text generation examples is
    <a href="https://github.com/huggingface/optimum-intel/blob/main/notebooks/openvino/quantized_generation_demo.ipynb">
     available on Github
    </a>
    .
   </p>
  </blockquote>
  <p>
   So, how fast is the quantized model on our laptop? Watch the following videos to see for yourself. Remember to select the 1080p resolution for maximum sharpness.
  </p>
  <p>
   The first video asks our model a high-school physics question: "
   <em>
    Lily has a rubber ball that she drops from the top of a wall. The wall is 2 meters tall. How long will it take for the ball to reach the ground?
   </em>
   "
  </p>
  <iframe allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="" frameborder="0" src="https://www.youtube.com/embed/nTNYRDORq14" style="aspect-ratio: 16 / 9;" title="YouTube video player" width="100%">
  </iframe>
  <p>
   The second video asks our model a coding question: "
   <em>
    Write a class which implements a fully connected layer with forward and backward functions using numpy. Use markdown markers for code.
   </em>
   "
  </p>
  <iframe allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="" frameborder="0" src="https://www.youtube.com/embed/igWrp8gnJZg" style="aspect-ratio: 16 / 9;" title="YouTube video player" width="100%">
  </iframe>
  <p>
   As you can see in both examples, the generated answer is very high quality. The quantization process hasn't degraded the high quality of Phi-2, and the generation speed is adequate. I would be happy to work locally with this model daily.
  </p>
  <h2 class="relative group flex items-center">
   <span>
    Conclusion
   </span>
  </h2>
  <p>
   Thanks to Hugging Face and Intel, you can now run LLMs on your laptop, enjoying the many benefits of local inference, like privacy, low latency, and low cost. We hope to see more quality models optimized for the Meteor Lake platform and its successor, Lunar Lake. The Optimum Intel library makes it very easy to quantize models for Intel platforms, so why not give it a try and share your excellent models on the Hugging Face Hub? We can always use more!
  </p>
  <p>
   Here are some resources to help you get started:
  </p>
  <ul>
   <li>
    Optimum Intel
    <a href="https://huggingface.co/docs/optimum/main/en/intel/inference">
     documentation
    </a>
   </li>
   <li>
    <a href="https://www.intel.com/content/www/us/en/developer/partner/hugging-face.html">
     Developer resources
    </a>
    from Intel and Hugging Face.
   </li>
   <li>
    A video deep dive on model quantization:
    <a href="https://youtu.be/kw7S-3s50uk">
     part 1
    </a>
    ,
    <a href="https://youtu.be/fXBBwCIA0Ds">
     part 2
    </a>
   </li>
  </ul>
  <p>
   If you have questions or feedback, we'd love to answer them on the
   <a href="https://discuss.huggingface.co/">
    Hugging Face forum
   </a>
   .
  </p>
  <p>
   Thanks for reading!
  </p>
  <!-- HTML_TAG_END -->
  <hr/>
  <h3>
   About the Author
  </h3>
  <p>
   <strong>
    Julien Simon is the Chief Evangelist at Hugging Face
   </strong>
   , where he focuses on democratizing AI and making transformers accessible to everyone. A leading voice in open-source AI and small language models, he helps developers and enterprises bring their AI ideas to life. In his spare time, he reads the works of JRR Tolkien again and again.
  </p>
  <p>
  </p>
  <p>
  </p>
  <p>
  </p>
  <!-- '"` -->
 
  
  </body>
 </html>
https://huggingface.co/blog/intel-protein-language-model-protst|2024-07-03_accelerating-protein-language-model-protst-on-intel-gaudi-2
https://huggingface.co/blog/cost-efficient-rag-applications-with-intel|2024-05-09_building-cost-efficient-enterprise-rag-applications-with-intel-gaudi-2-and-intel-xeon
https://huggingface.co/blog/phi2-intel-meteor-lake|2024-03-20_a-chatbot-on-your-laptop-phi-2-on-intel-meteor-lake
https://huggingface.co/blog/safecoder-vs-closed-source-code-assistants|2023-09-11_safecoder-vs-closed-source-code-assistants
https://huggingface.co/blog/stable-diffusion-finetuning-intel|2023-07-14_fine-tuning-stable-diffusion-models-on-intel-cpus
https://huggingface.co/blog/huggingface-and-amd|2023-06-13_hugging-face-and-amd-partner-on-accelerating-state-of-the-art-models-for-cpu-and-gpu-platforms
https://huggingface.co/blog/hugging-face-endpoints-on-azure|2023-05-24_hugging-face-collaborates-with-microsoft-to-launch-hugging-face-model-catalog-on-azure
https://huggingface.co/blog/huggingface-and-ibm|2023-05-23_hugging-face-and-ibm-partner-on-watsonxai-the-next-generation-enterprise-studio-for-ai-builders
https://huggingface.co/blog/generative-ai-models-on-intel-cpu|2023-05-16_smaller-is-better-q8-chat-an-efficient-generative-ai-experience-on-xeon
https://huggingface.co/blog/accelerate-transformers-with-inferentia2|2023-04-17_accelerating-hugging-face-transformers-with-aws-inferentia2
https://huggingface.co/blog/stable-diffusion-inference-intel|2023-03-28_accelerating-stable-diffusion-inference-on-intel-cpus
https://huggingface.co/blog/classification-use-cases|2023-03-01_how-hugging-face-accelerated-development-of-witty-works-writing-assistant
https://huggingface.co/blog/aws-partnership|2023-02-21_hugging-face-and-aws-partner-to-make-ai-more-accessible
https://huggingface.co/blog/intel-sapphire-rapids-inference|2023-02-06_accelerating-pytorch-transformers-with-intel-sapphire-rapids---part-2
https://huggingface.co/blog/intel-sapphire-rapids|2023-01-02_accelerating-pytorch-transformers-with-intel-sapphire-rapids---part-1
https://huggingface.co/blog/inference-update|2022-11-21_an-overview-of-inference-solutions-on-hugging-face
https://huggingface.co/blog/openvino|2022-11-02_accelerate-your-models-with-optimum-intel-and-openvino
https://huggingface.co/blog/inference-endpoints|2022-10-14_getting-started-with-hugging-face-inference-endpoints
https://huggingface.co/blog/vision-transformers|2022-08-18_deep-dive-vision-transformers-on-hugging-face-optimum-graphcore
https://huggingface.co/blog/intel|2022-06-15_intel-and-hugging-face-partner-to-democratize-machine-learning-hardware-acceleration
https://huggingface.co/blog/getting-started-habana|2022-04-26_getting-started-with-transformers-on-habana-gaudi
https://huggingface.co/blog/graphcore-getting-started|2021-11-30_getting-started-with-hugging-face-transformers-for-ipus-with-optimum
https://huggingface.co/blog/accelerating-pytorch|2021-11-19_accelerating-pytorch-distributed-fine-tuning-with-intel-technologies
https://huggingface.co/blog/large-language-models|2021-10-26_large-language-models-a-new-moores-law
https://huggingface.co/blog/the-age-of-ml-as-code|2021-10-20_the-age-of-machine-learning-as-code-has-arrived 
<!DOCTYPE html>
<html>
 <head>
  <meta charset="utf-8"/>
  <style>
   body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
            max-width: 800px;
            margin: 0 auto;
            padding: 20px;
            line-height: 1.6;
            color: #333;
        }
        h1, h2, h3, h4, h5, h6 {
            color: #2c3e50;
            margin-top: 2em;
            margin-bottom: 0.5em;
        }
        h1 { font-size: 2.5em; }
        h2 { font-size: 2em; }
        h3 { font-size: 1.5em; }
        img {
            max-width: 100%;
            height: auto;
            border-radius: 8px;
            box-shadow: 0 4px 6px rgba(0,0,0,0.1);
            margin: 1em 0;
        }
        /* Image alignment classes for text wraparound */
        img.alignleft {
            float: left;
            margin: 0.5em 1.5em 1em 0;
        }
        img.alignright {
            float: right;
            margin: 0.5em 0 1em 1.5em;
        }
        img.aligncenter {
            display: block;
            margin: 1em auto;
            float: none;
        }
        img.alignnone {
            float: none;
            margin: 1em 0;
        }
        /* Clear floats after images */
        .wp-block-image::after,
        p:has(img.alignleft)::after,
        p:has(img.alignright)::after {
            content: "";
            display: table;
            clear: both;
        }
        pre {
            background: #f8f9fa;
            border: 1px solid #e9ecef;
            border-radius: 4px;
            padding: 1em;
            overflow-x: auto;
        }
        code {
            background: #f8f9fa;
            padding: 0.2em 0.4em;
            border-radius: 3px;
            font-family: 'Monaco', 'Consolas', monospace;
        }
        blockquote {
            border-left: 4px solid #3498db;
            margin: 1em 0;
            padding-left: 1em;
            font-style: italic;
            color: #7f8c8d;
        }
        a {
            color: #3498db;
            text-decoration: none;
        }
        a:hover {
            text-decoration: underline;
        }
        p {
            margin-bottom: 1em;
        }
        /* Hugging Face specific styling */
        .prose {
            max-width: none;
        }
        .prose pre {
            background: #1e293b;
            color: #e2e8f0;
        }
        .prose code {
            background: #f1f5f9;
            color: #dc2626;
        }
  </style>
 </head>
 <body>

  <div style="margin-bottom: 2em;">
   <a href="../../../../huggingface-blog-posts.html" style="color: #3498db; text-decoration: none; font-weight: 500;">‚Üê Back to Hugging Face Blog Posts</a>
  </div>
  <h1>
   Accelerate your models with ü§ó Optimum Intel and OpenVINO
  </h1>
  <p style="color: #666; font-style: italic; margin-bottom: 1em;">
   Published: 2022-11-02
  </p>
  
  <p style="color: #666; font-style: italic; margin-bottom: 2em;">
   Originally published at
   <a href="https://huggingface.co/blog/openvino">
    https://huggingface.co/blog/openvino
   </a>
  </p>
  <!-- HTML_TAG_START -->
  <p>
   <img alt="image" src="image01.webp"/>
  </p>
  <p>
   Last July, we
   <a href="https://huggingface.co/blog/intel">
    announced
   </a>
   that Intel and Hugging Face would collaborate on building state-of-the-art yet simple hardware acceleration tools for Transformer models. 
‚Äã
Today, we are very happy to announce that we added Intel
   <a href="https://docs.openvino.ai/latest/index.html">
    OpenVINO
   </a>
   to
   <a href="https://github.com/huggingface/optimum-intel">
    Optimum Intel
   </a>
   . You can now easily perform inference with OpenVINO Runtime on a variety of Intel processors  (
   <a href="https://docs.openvino.ai/latest/openvino_docs_OV_UG_supported_plugins_Supported_Devices.html">
    see
   </a>
   the full list of supported devices) using Transformers models which can be hosted either on the Hugging Face hub or locally. You can also quantize your model with the OpenVINO Neural Network Compression Framework (
   <a href="https://github.com/openvinotoolkit/nncf">
    NNCF
   </a>
   ), and reduce its size and prediction latency in near minutes. ‚Äã
  </p>
  <p>
   This first release is based on OpenVINO 2022.2 and enables inference for a large quantity of PyTorch models using our
   <a href="https://huggingface.co/docs/optimum/intel/inference">
    <code>
     OVModels
    </code>
   </a>
   . Post-training static quantization and quantization aware training can be applied on many encoder models (BERT, DistilBERT, etc.). More encoder models will be supported in the upcoming OpenVINO release. Currently the quantization of Encoder Decoder models is not enabled, however this restriction should be lifted with our integration of the next OpenVINO release.
  </p>
  <p>
   ‚ÄãLet us show you how to get started in minutes!‚Äã
  </p>
  <h2 class="relative group flex items-center">
   <span>
    Quantizing a Vision Transformer with Optimum Intel and OpenVINO
   </span>
  </h2>
  <p>
   ‚Äã
In this example, we will run post-training static quantization on a Vision Transformer (ViT)
   <a href="https://huggingface.co/juliensimon/autotrain-food101-1471154050">
    model
   </a>
   fine-tuned for image classification on the
   <a href="https://huggingface.co/datasets/food101">
    food101
   </a>
   dataset. 
‚Äã
  </p>
  <p>
   Quantization is a process that lowers memory and compute requirements by reducing the bit width of model parameters. Reducing the number of bits means that the resulting model requires less memory at inference time, and that operations like matrix multiplication can be performed faster thanks to integer arithmetic.
  </p>
  <p>
   First, let's create a virtual environment and install all dependencies.‚Äã
  </p>
  <pre><code class="language-bash">virtualenv openvino
<span class="hljs-built_in">source</span> openvino/bin/activate
pip install pip --upgrade
pip install optimum[openvino,nncf] torchvision evaluate
</code></pre>
  <p>
   Next, moving to a Python environment, we import the appropriate modules and download the original model as well as its processor.
‚Äã
  </p>
  <p>
   ‚Äã
Post-training static quantization requires a calibration step where data is fed through the network in order to compute the quantized activation parameters. Here, we take 300 samples from the original dataset to build the calibration dataset.
‚Äã
  </p>
  <pre><code class="language-python"><span class="hljs-keyword">from</span> optimum.intel.openvino <span class="hljs-keyword">import</span> OVQuantizer
‚Äã
quantizer = OVQuantizer.from_pretrained(model)
calibration_dataset = quantizer.get_calibration_dataset(
    <span class="hljs-string">"food101"</span>,
    num_samples=<span class="hljs-number">300</span>,
    dataset_split=<span class="hljs-string">"train"</span>,
)
</code></pre>
  <p>
   As usual with image datasets, we need to apply the same image transformations that were used at training time. We use the preprocessing defined in the processor. We also define a data collation function to feed the model batches of properly formatted tensors.
‚Äã
  </p>
  <pre><code class="language-python"><span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">from</span> torchvision.transforms <span class="hljs-keyword">import</span> (
    CenterCrop,
    Compose,
    Normalize,
    Resize,
    ToTensor,
)
‚Äã
normalize = Normalize(mean=processor.image_mean, std=processor.image_std)
size = processor.size[<span class="hljs-string">"height"</span>]
_val_transforms = Compose(
    [
        Resize(size),
        CenterCrop(size),
        ToTensor(),
        normalize,
    ]
)
<span class="hljs-keyword">def</span> <span class="hljs-title function_">val_transforms</span>(<span class="hljs-params">example_batch</span>):
    example_batch[<span class="hljs-string">"pixel_values"</span>] = [_val_transforms(pil_img.convert(<span class="hljs-string">"RGB"</span>)) <span class="hljs-keyword">for</span> pil_img <span class="hljs-keyword">in</span> example_batch[<span class="hljs-string">"image"</span>]]
    <span class="hljs-keyword">return</span> example_batch
‚Äã
calibration_dataset.set_transform(val_transforms)
‚Äã
<span class="hljs-keyword">def</span> <span class="hljs-title function_">collate_fn</span>(<span class="hljs-params">examples</span>):
    pixel_values = torch.stack([example[<span class="hljs-string">"pixel_values"</span>] <span class="hljs-keyword">for</span> example <span class="hljs-keyword">in</span> examples])
    labels = torch.tensor([example[<span class="hljs-string">"label"</span>] <span class="hljs-keyword">for</span> example <span class="hljs-keyword">in</span> examples])
    <span class="hljs-keyword">return</span> {<span class="hljs-string">"pixel_values"</span>: pixel_values, <span class="hljs-string">"labels"</span>: labels}
</code></pre>
  <p>
   For our first attempt, we use the default configuration for quantization. You can also specify the number of samples to use during the calibration step, which is by default 300.
  </p>
  <pre><code class="language-python"><span class="hljs-keyword">from</span> optimum.intel.openvino <span class="hljs-keyword">import</span> OVConfig
‚Äã
quantization_config = OVConfig()
quantization_config.compression[<span class="hljs-string">"initializer"</span>][<span class="hljs-string">"range"</span>][<span class="hljs-string">"num_init_samples"</span>] = <span class="hljs-number">300</span>
</code></pre>
  <p>
   We're now ready to quantize the model. The
   <code>
    OVQuantizer.quantize()
   </code>
   method quantizes the model and exports it to the OpenVINO format. The resulting graph is represented with two files: an XML file describing the network topology and a binary file describing the weights. The resulting model can run on any target Intel¬Æ device.
  </p>
  <pre><code class="language-python">save_dir = <span class="hljs-string">"quantized_model"</span>


quantizer.quantize(
    quantization_config=quantization_config,
    calibration_dataset=calibration_dataset,
    data_collator=collate_fn,
    remove_unused_columns=<span class="hljs-literal">False</span>,
    save_directory=save_dir,
)
processor.save_pretrained(save_dir)
</code></pre>
  <p>
   A minute or two later, the model has been quantized. We can then easily load it with our
   <a href="https://huggingface.co/docs/optimum/intel/inference">
    <code>
     OVModelForXxx
    </code>
   </a>
   classes, the equivalent of the Transformers
   <a href="https://huggingface.co/docs/transformers/main/en/autoclass_tutorial#automodel">
    <code>
     AutoModelForXxx
    </code>
   </a>
   classes found in the
   <code>
    transformers
   </code>
   library. Likewise, we can create
   <a href="https://huggingface.co/docs/transformers/main/en/main_classes/pipelines">
    pipelines
   </a>
   and run inference with
   <a href="https://docs.openvino.ai/latest/openvino_docs_OV_UG_OV_Runtime_User_Guide.html">
    OpenVINO Runtime
   </a>
   .
‚Äã
  </p>
  <pre><code class="language-python"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> pipeline
<span class="hljs-keyword">from</span> optimum.intel.openvino <span class="hljs-keyword">import</span> OVModelForImageClassification
‚Äã
ov_model = OVModelForImageClassification.from_pretrained(save_dir)
ov_pipe = pipeline(<span class="hljs-string">"image-classification"</span>, model=ov_model, image_processor=processor)
outputs = ov_pipe(<span class="hljs-string">"http://farm2.staticflickr.com/1375/1394861946_171ea43524_z.jpg"</span>)
<span class="hljs-built_in">print</span>(outputs)
</code></pre>
  <p>
   ‚ÄãTo verify that quantization did not have a negative impact on accuracy, we applied an evaluation step to compare the accuracy of the original model with its quantized counterpart. We evaluate both models on a subset of the dataset (taking only 20% of the evaluation dataset). We observed little to no loss in accuracy with both models having an accuracy of
   <strong>
    87.6
   </strong>
   .
  </p>
  <pre><code class="language-python"><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-keyword">from</span> evaluate <span class="hljs-keyword">import</span> evaluator


eval_dataset = load_dataset(<span class="hljs-string">"food101"</span>, split=<span class="hljs-string">"validation"</span>).select(<span class="hljs-built_in">range</span>(<span class="hljs-number">5050</span>))
task_evaluator = evaluator(<span class="hljs-string">"image-classification"</span>)

ov_eval_results = task_evaluator.compute(
    model_or_pipeline=ov_pipe,
    data=eval_dataset,
    metric=<span class="hljs-string">"accuracy"</span>,
    label_mapping=ov_pipe.model.config.label2id,
)

trfs_pipe = pipeline(<span class="hljs-string">"image-classification"</span>, model=model, image_processor=processor)
trfs_eval_results = task_evaluator.compute(
    model_or_pipeline=trfs_pipe,
    data=eval_dataset,
    metric=<span class="hljs-string">"accuracy"</span>,
    label_mapping=trfs_pipe.model.config.label2id,
)
<span class="hljs-built_in">print</span>(trfs_eval_results, ov_eval_results)
</code></pre>
  <p>
   Looking at the quantized model, we see that its memory size decreased by
   <strong>
    3.8x
   </strong>
   from 344MB to 90MB. Running a quick benchmark on 5050 image predictions, we also notice a speedup in latency of
   <strong>
    2.4x
   </strong>
   , from 98ms to 41ms per sample. That's not bad for a few lines of code!
  </p>
  <p>
   ‚ö†Ô∏è An important thing to mention is that the model is compiled just before the first inference, which will inflate the latency of the first inference. So before doing your own benchmark, make sure to first warmup your model by doing at least one prediction.
  </p>
  <p>
   You can find the resulting
   <a href="https://huggingface.co/echarlaix/vit-food101-int8">
    model
   </a>
   hosted on the Hugging Face hub. To load it, you can easily do as follows:
  </p>
  <h2 class="relative group flex items-center">
   <span>
    Now it's your turn
   </span>
  </h2>
  <p>
   ‚Äã
As you can see, it's pretty easy to accelerate your models with ü§ó Optimum Intel and OpenVINO. If you'd like to get started, please visit the
   <a href="https://github.com/huggingface/optimum-intel">
    Optimum Intel
   </a>
   repository, and don't forget to give it a star ‚≠ê. You'll also find additional examples
   <a href="https://huggingface.co/docs/optimum/intel/optimization_ov">
    there
   </a>
   . If you'd like to dive deeper into OpenVINO, the Intel
   <a href="https://docs.openvino.ai/latest/index.html">
    documentation
   </a>
   has you covered.
  </p>
  <p>
   ‚Äã
Give it a try and let us know what you think. We'd love to hear your feedback on the Hugging Face
   <a href="https://discuss.huggingface.co/c/optimum">
    forum
   </a>
   , and please feel free to request features or file issues on
   <a href="https://github.com/huggingface/optimum-intel">
    Github
   </a>
   .
‚Äã
  </p>
  <p>
   Have fun with ü§ó Optimum Intel, and thank you for reading.
  </p>
  <p>
   ‚Äã
  </p>
  <!-- HTML_TAG_END -->
 </body>
</html>

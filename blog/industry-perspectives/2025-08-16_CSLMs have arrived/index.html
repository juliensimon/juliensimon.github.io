<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>CSLMs have arrived</title>
    <meta name="author" content="Julien Simon">
    <meta name="date" content="2025-08-16">
    <meta name="source" content="https://julsimon.medium.com/cslms-have-arrived-36ef90789cfb">
    <style>
        body { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif; line-height: 1.6; max-width: 800px; margin: 0 auto; padding: 20px; }
        h1 { color: #333; }
        .meta { color: #666; font-size: 0.9em; margin-bottom: 2em; }
        .content { line-height: 1.8; }
        img { max-width: 100%; height: auto; }
    </style>
</head>
<body>
    <h1>CSLMs have arrived</h1>
    <div class="meta">
        <p><strong>Author:</strong> Julien Simon</p>
        <p><strong>Date:</strong> 2025-08-16</p>
        <p><strong>Source:</strong> <a href="https://julsimon.medium.com/cslms-have-arrived-36ef90789cfb">https://julsimon.medium.com/cslms-have-arrived-36ef90789cfb</a></p>
    </div>
    <div class="content">
        <article><div><div><span></span><section><div><div></div><div><div><div><div><h1 id="bd57">CSLMs have arrived</h1></div><div><h2 id="b94c">Crazy Small Language Models üòâ</h2><div></div></div><p id="331a">CSLMs (Crazy Small Language Models) have arrived: you read it here first.</p><p id="8099">‚Äù<a href="https://www.sciencedirect.com/science/article/pii/S2666827025000416" target="_blank"><em>D3: A Small Language Model for Drug-Drug Interaction prediction and comparison with Large Language Models</em></a>‚Äù introduces a 70 MILLION (yes, six zeros, not nine) Llama-like model that is on par with Llama-3.1 70B for drug interaction prediction. It was trained and fine-tuned from scratch in 2.5 hours on a single A100.</p><figure><div><div><img alt="" height="400" src="image02.webp" width="700"/></div></div></figure><p id="cd8b">This research is yet another proof that it‚Äôs reasonably easy to build excellent ‚Äî and even SOTA ‚Äî models at a tiny fraction of the time, cost, and energy required by larger models. All it takes is a well-defined business problem, some good quality data, and a mind immune to LLM marketing bullshit.</p><p id="17b1">Here‚Äôs <a href="https://huggingface.co/blog/large-language-models" target="_blank">what I wrote</a> in October 2021 as I joined Hugging Face:<br/>‚Äú<em>Large language model size has been increasing 10x every year for the last few years. This is starting to look like another Moore‚Äôs Law. We‚Äôve been there before, and we should know that this road leads to diminishing returns, higher cost, more complexity, and new risks. Exponentials tend not to end well. Remember Meltdown and Spectre? Do we want to find out what that looks like for AI? Instead of chasing trillion-parameter models (place your bets), wouldn‚Äôt we all be better off if we built practical and efficient solutions that all developers could use to solve real-world problems?</em>‚Äù</p><p id="3e9b">Those of you who paid attention instead of spending silly money on prompt engineering bootcamps, OpenAI PoCs, and GPU clusters are quite likely to be in a good spot right now üòâ</p><p id="f890">More projects like this one, please. Share them, tag me, and I‚Äôll be happy to promote them. If you need help building these, please ping me and let‚Äôs discuss how Arcee AI can assist.</p></div></div></div></div></section></div></div></article>
    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>A not-so-silent revolution is happening in AI inference</title>
    <meta name="author" content="Julien Simon">
    <meta name="date" content="2025-08-20">
    <meta name="source" content="https://julsimon.medium.com/a-not-so-silent-revolution-is-happening-in-ai-inference-936ef4b9ba88">
    <style>
        body { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif; line-height: 1.6; max-width: 800px; margin: 0 auto; padding: 20px; }
        h1 { color: #333; }
        .meta { color: #666; font-size: 0.9em; margin-bottom: 2em; }
        .content { line-height: 1.8; }
        img { max-width: 100%; height: auto; }
    </style>
</head>
<body>
    <h1>A not-so-silent revolution is happening in AI inference</h1>
    <div class="meta">
        <p><strong>Author:</strong> Julien Simon</p>
        <p><strong>Date:</strong> 2025-08-20</p>
        <p><strong>Source:</strong> <a href="https://julsimon.medium.com/a-not-so-silent-revolution-is-happening-in-ai-inference-936ef4b9ba88">https://julsimon.medium.com/a-not-so-silent-revolution-is-happening-in-ai-inference-936ef4b9ba88</a></p>
    </div>
    <div class="content">
        <article><div><div><span></span><section><div><div></div><div><div><div><div><h1 id="3fd9">A not-so-silent revolution is happening in AI inference</h1><div></div></div><p id="6ebe">The combination of smaller yet more efficient language models, advances in open-source inference frameworks, and hardware acceleration is enabling an unprecedented pace of innovation for CPU inference.</p><p id="a12f">A little over a month ago, we published a blog post sharing numbers for <a href="https://huggingface.co/arcee-ai/AFM-4.5B" target="_blank">AFM-4.5B</a> inference on Intel Corporation, Arm, and Qualcomm (see: ‚Äú<a href="https://www.arcee.ai/blog/is-running-language-models-on-cpu-really-viable" target="_blank">Is Running Language Models on CPU Really Viable?</a>‚Äù).</p><p id="a1fc">I reran the Intel and Arm benchmarks in the same configuration, and all numbers have improved across the board, with some increases of up to 50%. You‚Äôll find numbers at the end of the post.</p><p id="0f32">Same model. Same chips. What happened? Llama.cpp is on fire, that‚Äôs what. New features, such as <a href="https://github.com/ggml-org/llama.cpp/pull/14363" target="_blank">splitting the KV cache across sequence decoding</a>, are delivering double-digit gains overnight.</p><p id="5add">Takeaways:</p><p id="55f6">‚û°Ô∏è If you‚Äôre been using GPU inference indiscriminately, it‚Äôs time to reconsider. Your existing CPU servers may be an extremely cost-effective option, and they can also run your app!</p><p id="0425">‚û°Ô∏è If you‚Äôre not rebuilding llama.cpp every day, you‚Äôre doing it wrong üòÇ</p><p id="58b2">‚û°Ô∏è We used to pick between ‚Äòfast but less imprecise 4-bit‚Äô vs ‚Äòslow but more precise 8-bit‚Äô. That‚Äôs not so true anymore. 8-bit models are now fast enough (whatever that means to you) for many use cases.</p><p id="9170">‚û°Ô∏è The speedup on larger batch sizes definitely invalidates my long-standing advice of ‚ÄúCPU inference only really makes sense at batch size 1‚Äù. Now, I would consider larger batch sizes, especially for non-interactive workloads, and experiment to find the sweet spot between thread count, latency, and throughput.</p><p id="3d01">What truly puts a smile on my face is that these aren‚Äôt even the bleeding-edge CPUs. There‚Äôs much more speed coming.</p><p id="f739">And this story ends on laptops and devices for minimal latency, maximum cost optimization, and full privacy. Many of us know that already üòÄ</p><figure><div><img alt="" height="591" src="image01.webp" width="486"/></div></figure><figure><div><img alt="" height="611" src="image02.webp" width="479"/></div></figure></div></div></div></div></section></div></div></article>
    </div>
</body>
</html>
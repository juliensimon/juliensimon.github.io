<!DOCTYPE html>
<html>
 <head>
  <meta charset="utf-8"/>
  <style>
   body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
            max-width: 800px;
            margin: 0 auto;
            padding: 20px;
            line-height: 1.6;
            color: #333;
        }
        h1, h2, h3, h4, h5, h6 {
            color: #2c3e50;
            margin-top: 2em;
            margin-bottom: 0.5em;
        }
        h1 { font-size: 2.5em; }
        h2 { font-size: 2em; }
        h3 { font-size: 1.5em; }
        img {
            max-width: 100%;
            height: auto;
            border-radius: 8px;
            box-shadow: 0 4px 6px rgba(0,0,0,0.1);
            margin: 1em 0;
        }
        /* Image alignment classes for text wraparound */
        img.alignleft {
            float: left;
            margin: 0.5em 1.5em 1em 0;
        }
        img.alignright {
            float: right;
            margin: 0.5em 0 1em 1.5em;
        }
        img.aligncenter {
            display: block;
            margin: 1em auto;
            float: none;
        }
        img.alignnone {
            float: none;
            margin: 1em 0;
        }
        /* Clear floats after images */
        .wp-block-image::after,
        p:has(img.alignleft)::after,
        p:has(img.alignright)::after {
            content: "";
            display: table;
            clear: both;
        }
        pre {
            background: #f8f9fa;
            border: 1px solid #e9ecef;
            border-radius: 4px;
            padding: 1em;
            overflow-x: auto;
        }
        code {
            background: #f8f9fa;
            padding: 0.2em 0.4em;
            border-radius: 3px;
            font-family: 'Monaco', 'Consolas', monospace;
        }
        blockquote {
            border-left: 4px solid #3498db;
            margin: 1em 0;
            padding-left: 1em;
            font-style: italic;
            color: #7f8c8d;
        }
        a {
            color: #3498db;
            text-decoration: none;
        }
        a:hover {
            text-decoration: underline;
        }
        p {
            margin-bottom: 1em;
        }
  </style>
 </head>
 <body>
  <h1>
   Arcee AI Small Language Models Excel Across Yupp.ai Leaderboards
  </h1>
  <p style="color: #666; font-style: italic; margin-bottom: 1em;">
   Published: 2025-07-18
  </p>
  <p style="color: #666; font-style: italic; margin-bottom: 2em;">
   Originally published at
   <a href="https://www.arcee.ai/blog/arcee-ai-small-language-models-excel-across-yupp-ai-leaderboards">
    https://www.arcee.ai/blog/arcee-ai-small-language-models-excel-across-yupp-ai-leaderboards
   </a>
  </p>
  <h1>
   <strong>
    Small And Mighty: Arcee AI Language Models Excel Across Yupp.ai Leaderboards
   </strong>
  </h1>
  <p>
   At Arcee AI, we’ve pioneered a family of
   <strong>
    Small Language Models (SLMs)
   </strong>
   – compact yet powerful models that run efficiently on a single GPU and are easily tailored for specific tasks.
  </p>
  <p>
   Arcee SLMs cover a broad range of needs – from general-purpose assistants (such as the Arcee Virtuoso model family) to specialized reasoning (Arcee Maestro), coding (Arcee Coder), and other functions. Despite their smaller size, we engineer our models using advanced techniques (model merging, distillation, guided RL, etc.) so that they compete with, and often outperform, Large Language Models (LLMs).
  </p>
  <p>
   Academic benchmarks are useful for evaluating and comparing model performance. However, there’s nothing more valuable than real-world user testing. To support this approach,
   <a href="http://yupp.ai">
    Yupp.ai
   </a>
   has created a new evaluation platform that ranks AIs by aggregating
   <strong>
    real-user usage preferences
   </strong>
   into a “
   <a href="https://blog.yupp.ai/leaderboard">
    VIBE Score
   </a>
   ”.
  </p>
  <p>
   We’re thrilled to report that several Arcee SLMs have already risen to the top on Yupp’s public leaderboard in several categories. Maestro and Coder Large appear among the highest-performing models on reasoning and coding prompts, and our new AFM-4.5B-Preview model even sits near the very top for short-form Q&amp;A tasks. This user-driven evidence confirms that Arcee’s models excel on
   <strong>
    real-world prompts
   </strong>
   and not just artificial tests.
  </p>
  <p>
   <strong>
    Arcee Maestro (32B):
   </strong>
   Our fine-tuned reasoning model (built on Qwen-2.5) excels particularly in math and logic tasks. This class-leading performance shows up in user tests. On Yupp’s high-reasoning benchmark,
   <strong>
    Maestro currently ranks #5, tied with top models like Anthropic’s Claude Sonnet 4, and #8 overall when considering all prompt types
   </strong>
   . Such results highlight that Maestro’s “detailed reasoning” approach enables it to solve problems as well as, or better than, far larger models.
  </p>
  <figure class="w-richtext-align-fullwidth w-richtext-figure-type-image" style="max-width:1179pxpx">
   <div>
    <img alt="" loading="lazy" src="image01.webp"/>
   </div>
   <figcaption>
    Arcee Maestro (32B): #5 on high-reasoning prompts
   </figcaption>
  </figure>
  <figure class="w-richtext-align-fullwidth w-richtext-figure-type-image" style="max-width:1172pxpx">
   <div>
    <img alt="" loading="lazy" src="image02.webp"/>
   </div>
   <figcaption>
    Arcee Maestro (32B): #8 on all prompt categories
   </figcaption>
  </figure>
  <p>
   ‍
   <strong>
    Arcee Coder (32B):
   </strong>
   Our code-specialized model (also built on Qwen-2.5) delivers superior coding accuracy and efficiency. Importantly, Coder Large is cost-effective, as it is priced well below proprietary alternatives, allowing engineering teams to scale up coding tasks without incurring runaway cloud costs. On the Yupp leaderboard,
   <strong>
    Coder Large ranks #6 (tied with Claude Sonnet 3.7) in the long-response multi-turn category
   </strong>
   , reflecting its strength on real developer prompts.
  </p>
  <figure class="w-richtext-align-fullwidth w-richtext-figure-type-image" style="max-width:1181pxpx">
   <div>
    <img alt="" loading="lazy" src="image03.webp"/>
   </div>
   <figcaption>
    Arcee Coder (32B): #6 on long multi-turn conversations
   </figcaption>
  </figure>
  <p>
   <strong>
    AFM-4.5B-Preview (4.5B):
   </strong>
   Our
   <a href="https://www.arcee.ai/blog/announcing-the-arcee-foundation-model-family">
    brand-new open-weight foundation model
   </a>
   is designed from the ground up for enterprise needs. Despite its small size of 4.5B, AFM-4.5B delivers business performance comparable to much larger models at vastly lower hosting costs. It includes built-in function-calling and agentic reasoning, as well as multilingual support, and we trained it on ~7 trillion tokens of carefully filtered data for accuracy and compliance.
   <strong>
    On short-turn QA and instruction tasks, AFM-4.5B-Preview takes an incredible #2 spot
   </strong>
   on the Yupp leaderboard. These rankings align with our lab results – AFM-4.5B-Preview consistently achieves accuracy on par with significantly larger models, while allowing deployment on low-end GPUs (and even CPUs).
  </p>
  <figure class="w-richtext-align-fullwidth w-richtext-figure-type-image" style="max-width:1173pxpx">
   <div>
    <img alt="" loading="lazy" src="image04.webp"/>
   </div>
   <figcaption>
    AFM-4.5-Preview: #2 on short-turn conversations
   </figcaption>
  </figure>
  <p>
   These results showcase the Arcee value proposition:
   <strong>
    small, task-optimized models that punch above their weight
   </strong>
   . By fully owning the training pipeline, we embed efficiency and compliance by design. As the developers of open tools like MergeKit, Spectrum, and DistillKit, we apply the latest research to make SLMs that are both accurate and lean. AFM-4.5B, for example, was built as a “no trade-offs” model with cost-efficiency, customizability, and enterprise-grade compliance
   <em>
    baked in
   </em>
   from the start.
  </p>
  <p>
   The practical payoff is huge: firms can serve users and orchestrate agents using these SLMs, saving on compute bills without sacrificing quality. In real-world use, this means faster responses, lower TCO, and the ability to run advanced AI workflows on modest hardware. Customers can deploy Arcee SLMs anywhere – on-premises or in the cloud – knowing their privacy and compliance are fully preserved.
  </p>
  <p>
   Long story short:
   <strong>
    Arcee SLMs translate into clear business value, which is exactly what our customers need for deploying AI at scale
   </strong>
   .
  </p>
  <p>
   You can easily try our models on
   <a href="http://together.ai">
    Together.ai
   </a>
  </p>
  <ul role="list">
   <li>
    <a href="https://www.together.ai/models/arcee-ai-maestro-reasoning">
     Arcee Maestro
    </a>
   </li>
   <li>
    <a href="https://www.together.ai/models/arcee-ai-coder-large">
     Arcee Coder
    </a>
   </li>
   <li>
    <a href="https://www.together.ai/models/afm-4-5b-preview">
     AFM 4.5B-Preview
    </a>
   </li>
  </ul>
  <p>
   If you’d like to learn more, please
   <a href="https://www.arcee.ai/book-a-demo">
    get in touch
   </a>
   , and we’ll be happy to discuss how we can help you implement secure, efficient, and cost-effective AI solutions.
  </p>
  <p>
   ‍
  </p>
 </body>
</html>

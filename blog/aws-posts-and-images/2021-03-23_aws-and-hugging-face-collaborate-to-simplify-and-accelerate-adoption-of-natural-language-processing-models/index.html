<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  
  <!-- Primary Meta Tags -->
  <title>AWS and Hugging Face collaborate to simplify and accelerate adoption of Natural Language Processing models - Julien Simon | AWS Expert</title>
  <meta name="title" content="AWS and Hugging Face collaborate to simplify and accelerate adoption of Natural Language Processing models - Julien Simon | AWS Expert"/>
  <meta name="description" content="Expert analysis and technical deep-dive on aws and hugging face collaborate to simplify and accelerate adoption of natural language processing models by Julien Simon, leading AWS expert and former Global Technical Evangelist for AI & ML at Amazon Web Services."/>
  <meta name="keywords" content="AWS, Amazon Web Services, ['AWS', 'and', 'Hugging'], machine learning, AI, cloud computing, Julien Simon, AWS expert"/>
  <meta name="author" content="Julien Simon"/>
  <meta name="robots" content="index, follow"/>
  
  <!-- Open Graph / Facebook -->
  <meta property="og:type" content="article"/>
  <meta property="og:url" content="https://julien.org/blog/aws-and-hugging-face-collaborate-to-simplify-and-accelerate-adoption-of-natural-language-processing-models/"/>
  <meta property="og:title" content="AWS and Hugging Face collaborate to simplify and accelerate adoption of Natural Language Processing models - Julien Simon | AWS Expert"/>
  <meta property="og:description" content="Expert analysis and technical deep-dive on aws and hugging face collaborate to simplify and accelerate adoption of natural language processing models by Julien Simon, leading AWS expert and former Global Technical Evangelist for AI & ML at Amazon Web Services."/>
  <meta property="og:image" content="https://julien.org/assets/julien-simon-aws-expert.jpg"/>
  <meta property="og:site_name" content="Julien Simon - AWS Expert"/>
  <meta property="article:author" content="Julien Simon"/>
  <meta property="article:published_time" content="2021-03-23T00:00:00Z"/>
  <meta property="article:section" content="AWS"/>
  <meta property="article:tag" content="AWS, Amazon Web Services, Machine Learning, AI"/>
  
  <!-- Twitter -->
  <meta property="twitter:card" content="summary_large_image"/>
  <meta property="twitter:url" content="https://julien.org/blog/aws-and-hugging-face-collaborate-to-simplify-and-accelerate-adoption-of-natural-language-processing-models/"/>
  <meta property="twitter:title" content="AWS and Hugging Face collaborate to simplify and accelerate adoption of Natural Language Processing models - Julien Simon | AWS Expert"/>
  <meta property="twitter:description" content="Expert analysis and technical deep-dive on aws and hugging face collaborate to simplify and accelerate adoption of natural language processing models by Julien Simon, leading AWS expert and former Global Technical Evangelist for AI & ML at Amazon Web Services."/>
  <meta property="twitter:image" content="https://julien.org/assets/julien-simon-aws-expert.jpg"/>
  <meta property="twitter:creator" content="@julsimon"/>
  
  <!-- Canonical URL -->
  <link rel="canonical" href="https://julien.org/blog/aws-and-hugging-face-collaborate-to-simplify-and-accelerate-adoption-of-natural-language-processing-models/"/>
  
  <!-- Author and Publisher -->
  <link rel="author" href="https://julien.org/"/>
  <link rel="publisher" href="https://julien.org/"/>
  
  <!-- Structured Data -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "BlogPosting",
    "headline": "AWS and Hugging Face collaborate to simplify and accelerate adoption of Natural Language Processing models",
    "description": "Expert analysis and technical deep-dive on aws and hugging face collaborate to simplify and accelerate adoption of natural language processing models by Julien Simon, leading AWS expert and former Global Technical Evangelist for AI & ML at Amazon Web Services.",
    "image": "https://julien.org/assets/julien-simon-aws-expert.jpg",
    "author": {
      "@type": "Person",
      "name": "Julien Simon",
      "url": "https://julien.org/",
      "jobTitle": "AWS Expert & Former Global Technical Evangelist",
      "worksFor": {
        "@type": "Organization",
        "name": "Amazon Web Services"
      },
      "sameAs": [
        "https://twitter.com/julsimon",
        "https://linkedin.com/in/juliensimon",
        "https://github.com/juliensimon"
      ]
    },
    "publisher": {
      "@type": "Organization",
      "name": "Julien Simon",
      "url": "https://julien.org/",
      "logo": {
        "@type": "ImageObject",
        "url": "https://julien.org/assets/julien-simon-logo.jpg"
      }
    },
    "datePublished": "2021-03-23T00:00:00Z",
    "dateModified": "2021-03-23T00:00:00Z",
    "mainEntityOfPage": {
      "@type": "WebPage",
      "@id": "https://julien.org/blog/aws-and-hugging-face-collaborate-to-simplify-and-accelerate-adoption-of-natural-language-processing-models/"
    },
    "url": "https://julien.org/blog/aws-and-hugging-face-collaborate-to-simplify-and-accelerate-adoption-of-natural-language-processing-models/",
    "keywords": "AWS, Amazon Web Services, ['AWS', 'and', 'Hugging'], machine learning, AI, cloud computing, Julien Simon, AWS expert",
    "articleSection": "AWS",
    "inLanguage": "en-US",
    "isPartOf": {
      "@type": "Blog",
      "name": "Julien Simon - AWS Expert Blog",
      "url": "https://julien.org/blog/"
    }
  }
  </script>
  
  <!-- Additional SEO Meta Tags -->
  <meta name="twitter:site" content="@julsimon"/>
  <meta name="twitter:creator" content="@julsimon"/>
  <meta name="theme-color" content="#FF9900"/>
  <meta name="msapplication-TileColor" content="#FF9900"/>
  <meta name="apple-mobile-web-app-capable" content="yes"/>
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent"/>
  
  <!-- Preconnect to external domains -->
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  
  <!-- Favicon -->
  <link rel="icon" type="image/x-icon" href="https://julien.org/favicon.ico"/>
  <link rel="apple-touch-icon" href="https://julien.org/apple-touch-icon.png"/>
  
  <style>
   body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
            max-width: 800px;
            margin: 0 auto;
            padding: 20px;
            line-height: 1.6;
            color: #333;
        }
        h1, h2, h3, h4, h5, h6 {
            color: #2c3e50;
            margin-top: 2em;
            margin-bottom: 0.5em;
        }
        h1 { font-size: 2.5em; }
        h2 { font-size: 2em; }
        h3 { font-size: 1.5em; }
        img {
            max-width: 100%;
            height: auto;
            border-radius: 8px;
            box-shadow: 0 4px 6px rgba(0,0,0,0.1);
            margin: 1em 0;
        }
        /* Image alignment classes for text wraparound */
        img.alignleft {
            float: left;
            margin: 0.5em 1.5em 1em 0;
        }
        img.alignright {
            float: right;
            margin: 0.5em 0 1em 1.5em;
        }
        img.aligncenter {
            display: block;
            margin: 1em auto;
            float: none;
        }
        img.alignnone {
            float: none;
            margin: 1em 0;
        }
        /* Clear floats after images */
        .wp-block-image::after,
        p:has(img.alignleft)::after,
        p:has(img.alignright)::after {
            content: "";
            display: table;
            clear: both;
        }
        pre {
            background: #f8f9fa;
            border: 1px solid #e9ecef;
            border-radius: 4px;
            padding: 1em;
            overflow-x: auto;
        }
        code {
            background: #f8f9fa;
            padding: 0.2em 0.4em;
            border-radius: 3px;
            font-family: 'Monaco', 'Consolas', monospace;
        }
        blockquote {
            border-left: 4px solid #3498db;
            margin: 1em 0;
            padding-left: 1em;
            font-style: italic;
            color: #7f8c8d;
        }
        a {
            color: #3498db;
            text-decoration: none;
        }
        a:hover {
            text-decoration: underline;
        }
        p {
            margin-bottom: 1em;
        }
        .author-bio {
            background: #f8f9fa;
            border-left: 4px solid #FF9900;
            padding: 1em;
            margin: 2em 0;
            border-radius: 4px;
        }
        .author-bio h3 {
            margin-top: 0;
            color: #FF9900;
        }
        
  </style>
 </head>
 <body>
  <div style="margin-bottom: 1em;">
  <a href="../../../aws-blog-posts.html" style="color: #FF9900; text-decoration: none; font-size: 0.9em;">← Back to AWS Blog Posts</a>
</div>
  
  <h1>AWS and Hugging Face collaborate to simplify and accelerate adoption of Natural Language Processing models</h1>
  
    <div class="author-bio">
   <h3>About the Author</h3>
   <p><strong>Julien Simon</strong> is an expert in Practical AI, currently serving as Chief Evangelist at Arcee AI. Named <strong>#1 AI Evangelist globally by AI Magazine in 2021</strong>, he champions cost-effective, privacy-first AI solutions through Small Language Models, challenging the industry trend toward expensive, large-scale alternatives.</p>
   <p>With over 30 years of technology leadership—including executive roles at AWS, Hugging Face, Criteo, and other major companies—Julien has delivered 650+ speaking engagements across 90+ cities in 38 countries. His practical approach empowers enterprises to achieve superior AI outcomes while maintaining cost efficiency and operational simplicity.</p>
   <p>Follow Julien on <a href="https://twitter.com/julsimon" target="_blank">Twitter</a> and <a href="https://linkedin.com/in/juliensimon" target="_blank">LinkedIn</a> for the latest insights on Practical AI, Small Language Models, and enterprise AI solutions.</p>
  </div>
  
  <p style="color: #666; font-style: italic; margin-bottom: 1em;">
   Published: 2021-03-23 | Originally published at <a href="https://aws.amazon.com/blogs/machine-learning/aws-and-hugging-face-collaborate-to-simplify-and-accelerate-adoption-of-natural-language-processing-models/" target="_blank" rel="noopener noreferrer">AWS Blog</a>
  </p>
 <body>
  <p>
   Just like computer vision a few years ago, the decade-old field of natural language processing (NLP) is experiencing a fascinating renaissance. Not a month goes by without a new breakthrough! Indeed, thanks to the scalability and cost-efficiency of cloud-based infrastructure, researchers are finally able to train complex deep learning models on very large text datasets, in order to solve business problems such as question answering, sentence comparison, or text summarization.
  </p>
  <p>
   In this respect, the
   <a href="https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)">
    Transformer
   </a>
   deep learning architecture has proven very successful, and has spawned several state of the art model families:
  </p>
  <ul>
   <li>
    Bidirectional Encoder Representations from Transformers (BERT): 340 million parameters [1]
   </li>
   <li>
    Text-To-Text Transfer Transformer (T5): over 10 billion parameters [2]
   </li>
   <li>
    Generative Pre-Training (GPT): over 175 billion parameters [3]
   </li>
  </ul>
  <p>
   As amazing as these models are, training and optimizing them remains a challenging endeavor that requires a significant amount of time, resources, and skills, all the more when different languages are involved. Unfortunately, this complexity prevents most organizations from using these models effectively, if at all. Instead, wouldn’t it be great if we could just start from pre-trained versions and put them to work immediately?
  </p>
  <p>
   This is the exact challenge that
   <a href="https://huggingface.co/amazon/">
    Hugging Face
   </a>
   is tackling. Founded in 2016, this startup based in New York and Paris makes it easy to add state of the art Transformer models to your applications. Thanks to their popular
   <a href="https://huggingface.co/transformers/">
    <code>
     transformers
    </code>
   </a>
   ,
   <a href="https://github.com/huggingface/tokenizers">
    <code>
     tokenizers
    </code>
   </a>
   and
   <a href="https://github.com/huggingface/datasets">
    <code>
     datasets
    </code>
   </a>
   libraries, you can download and predict with over 7,000 pre-trained models in 164 languages. What do I mean by ‘popular’? Well, with over 42,000 stars on GitHub and 1 million downloads per month, the
   <code>
    transformers
   </code>
   library has become the
   <em>
    de facto
   </em>
   place for developers and data scientists to find NLP models.
  </p>
  <p>
   At AWS, we’re also working hard on democratizing machine learning in order to put it in the hands of every developer, data scientist and expert practitioner. In particular, tens of thousands of customers now use
   <a href="https://aws.amazon.com/sagemaker">
    Amazon SageMaker
   </a>
   , our fully managed service for machine learning. Thanks to its managed infrastructure and its advanced machine learning capabilities, customers can build and run their machine learning workloads quicker than ever at any scale. As NLP adoption grows, so does the adoption of Hugging Face models, and customers have asked us for a simpler way to train and optimize them on AWS.
  </p>
  <h2>
   <strong>
    Working with Hugging Face Models on Amazon SageMaker
   </strong>
  </h2>
  <p>
   Today, we’re happy to announce that you can now work with Hugging Face models on
   <a href="https://aws.amazon.com/sagemaker">
    Amazon SageMaker
   </a>
   . Thanks to the new
   <code>
    HuggingFace
   </code>
   estimator in the
   <a href="https://sagemaker.readthedocs.io/en/stable/">
    SageMaker SDK
   </a>
   , you can easily train, fine-tune, and optimize Hugging Face models built with TensorFlow and PyTorch. This should be extremely useful for customers interested in customizing Hugging Face models to increase accuracy on domain-specific language: financial services, life sciences, media and entertainment, and so on.
  </p>
  <p>
   Here’s a code snippet fine-tuning the
   <a href="https://huggingface.co/distilbert-base-uncased">
    DistilBERT
   </a>
   model for a single epoch.
  </p>
  <pre><code class="lang-python">from sagemaker.huggingface import HuggingFace
hf_estimator = HuggingFace(
    entry_point='train.py',
    pytorch_version = '1.6.0',
    transformers_version = '4.4',
    instance_type='ml.p3.2xlarge',
    instance_count=1,
    role=role,
    hyperparameters = {
        'epochs': 1,
        'train_batch_size': 32,
        'model_name':'distilbert-base-uncased'
    }
)
huggingface_estimator.fit({'train': training_input_path, 'test': test_input_path})
</code></pre>
  <p>
   As usual on SageMaker, the
   <code>
    train.py
   </code>
   script uses Script Mode to retrieve hyperparameters as command line arguments. Then, thanks to the
   <code>
    transformers
   </code>
   library API, it downloads the appropriate Hugging Face model, configures the training job, and runs it with the
   <a href="https://huggingface.co/transformers/main_classes/trainer.html">
    <code>
     Trainer
    </code>
   </a>
   API. Here’s a code snippet showing these steps.
  </p>
  <pre><code class="lang-python">from transformers import AutoModelForSequenceClassification, Trainer, TrainingArguments
...
model = AutoModelForSequenceClassification.from_pretrained(args.model_name)
training_args = TrainingArguments(
        output_dir=args.model_dir,
        num_train_epochs=args.epochs,
        per_device_train_batch_size=args.train_batch_size,
        per_device_eval_batch_size=args.eval_batch_size,
        warmup_steps=args.warmup_steps,
        evaluation_strategy="epoch",
        logging_dir=f"{args.output_data_dir}/logs",
        learning_rate=float(args.learning_rate)
)
trainer = Trainer(
        model=model,
        args=training_args,
        compute_metrics=compute_metrics,
        train_dataset=train_dataset,
        eval_dataset=test_dataset
)
trainer.train()</code></pre>
  <p>
   As you can see, this integration makes it easier and quicker to train advanced NLP models, even if you don’t have a lot of machine learning expertise.
  </p>
  <p>
   Customers are already using Hugging Face models on
   <a href="https://aws.amazon.com/sagemaker">
    Amazon SageMake
   </a>
   r. For example,
   <a href="https://quantum-health.com/">
    Quantum Health
   </a>
   is on a mission to make healthcare navigation smarter, simpler, and most cost-effective for everybody. Says
   <a href="https://www.linkedin.com/in/jorge-lopez-grisman-b1197953/">
    Jorge Grisman
   </a>
   , NLP Data Scientist at Quantum Health: “
   <em>
    we use Hugging Face and Amazon SageMaker a lot for many NLP use cases such as text classification, text summarization, and Q&amp;A with the goal of helping our agents and members. For some use cases, we just use the Hugging Face models directly and for others we fine tune them on SageMaker. We are excited about the integration of Hugging Face Transformers into Amazon SageMaker to make use of the distributed libraries during training to shorten the training time for our larger datasets
   </em>
   “.
  </p>
  <p>
   <a href="https://www.kustomer.com/">
    Kustomer
   </a>
   is a customer service CRM platform for managing high support volume effortlessly. Says
   <a href="https://www.linkedin.com/in/victorpeinado/">
    Victor Peinado
   </a>
   , ML Software Engineering Manager at Kustomer: “
   <em>
    Kustomer is a customer service CRM platform for managing high support volume effortlessly. In our business, we use machine learning models to help customers contextualize conversations, remove time-consuming tasks, and deflect repetitive questions. We use Hugging Face and Amazon SageMaker extensively, and we are excited about the integration of Hugging Face Transformers into SageMaker since it will simplify the way we fine tune machine learning models for text classification and semantic search
   </em>
   “
   <em>
    .
   </em>
  </p>
  <h2>
   <strong>
    Training Hugging Face Models at Scale on Amazon SageMaker
   </strong>
  </h2>
  <p>
   As mentioned earlier, NLP datasets can be huge, which may lead to very long training times. In order to help you speed up your training jobs and make the most of your AWS infrastructure, we’ve worked with Hugging Face to add the
   <a href="https://aws.amazon.com/blogs/aws/managed-data-parallelism-in-amazon-sagemaker-simplifies-training-on-large-datasets/">
    SageMaker Data Parallelism Library
   </a>
   to the
   <code>
    transformers
   </code>
   library (details are available in the
   <code>
    <a href="https://huggingface.co/transformers/main_classes/trainer.html">
     Trainer
    </a>
   </code>
   API documentation).
  </p>
  <p>
   Adding a single parameter to your
   <code>
    HuggingFace
   </code>
   estimator is all it takes to enable data parallelism, letting your
   <code>
    Trainer
   </code>
   -based code use it automatically.
  </p>
  <pre><code class="lang-python">huggingface_estimator = HuggingFace(. . .
    distribution = {'smdistributed':{'dataparallel':{ 'enabled': True }}}
)</code></pre>
  <p>
   That’s it. In fact, the Hugging Face team used this capability to speed up their experiment process by over four times!
  </p>
  <h2>
   <strong>
    Getting Started
   </strong>
  </h2>
  <p>
   You can start using Hugging Face models on
   <a href="https://aws.amazon.com/sagemaker">
    Amazon SageMaker
   </a>
   today, in all AWS Regions where SageMaker is available.
   <a href="https://github.com/huggingface/notebooks/tree/master/sagemaker">
    Sample notebooks
   </a>
   are available on GitHub. In order to enjoy automatic data parallelism, please make sure to use version
   <a href="https://github.com/huggingface/transformers/releases/tag/v4.3.0">
    4.3.0
   </a>
   of the
   <a href="https://huggingface.co/transformers/">
    <code>
     transformers
    </code>
   </a>
   library (or newer) in your training script.
  </p>
  <p>
   Please give it a try, and let us know what you think. As always, we’re looking forward to your feedback. You can send it to your usual AWS Support contacts, or in the
   <a href="https://forums.aws.amazon.com/forum.jspa?forumID=285">
    AWS Forum for SageMaker
   </a>
   .
  </p>
  <p>
   [
   <a href="https://arxiv.org/abs/1810.04805">
    1
   </a>
   ] “
   <em>
    BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding
   </em>
   “, Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova.
   <br/>
   [
   <a href="https://arxiv.org/abs/1910.10683">
    2
   </a>
   ] “
   <em>
    Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer
   </em>
   “, Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu.
   <br/>
   [
   <a href="https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf">
    3
   </a>
   ] “
   <em>
    Improving Language Understanding by Generative Pre-Training
   </em>
   “, Alec Radford Karthik Narasimhan Tim Salimans Ilya Sutskever.
  </p>
  <hr/>
  <h3>
   About the Author
  </h3>
  <p>
   <img alt="" class="alignleft size-full wp-image-3134" height="115" src="image01.webp" width="100"/>
   <strong>
    Julien Simon
   </strong>
   is the Artificial Intelligence &amp; Machine Learning Evangelist for EMEA. He focuses on helping developers and enterprises bring their ideas to life. In his spare time, he reads the works of JRR Tolkien again and again.
  </p>
  <!-- '"` -->
 
  
  </body>
 </html>

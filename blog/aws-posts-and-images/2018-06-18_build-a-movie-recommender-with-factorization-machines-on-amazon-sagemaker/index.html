<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  
  <!-- Primary Meta Tags -->
  <title>Build a movie recommender with factorization machines on Amazon SageMaker - Julien Simon | AWS Expert</title>
  <meta name="title" content="Build a movie recommender with factorization machines on Amazon SageMaker - Julien Simon | AWS Expert"/>
  <meta name="description" content="Expert analysis and technical deep-dive on build a movie recommender with factorization machines on amazon sagemaker by Julien Simon, leading AWS expert and former Global Technical Evangelist for AI & ML at Amazon Web Services."/>
  <meta name="keywords" content="AWS, Amazon Web Services, ['Build', 'a', 'movie'], machine learning, AI, cloud computing, Julien Simon, AWS expert"/>
  <meta name="author" content="Julien Simon"/>
  <meta name="robots" content="index, follow"/>
  
  <!-- Open Graph / Facebook -->
  <meta property="og:type" content="article"/>
  <meta property="og:url" content="https://julien.org/blog/build-a-movie-recommender-with-factorization-machines-on-amazon-sagemaker/"/>
  <meta property="og:title" content="Build a movie recommender with factorization machines on Amazon SageMaker - Julien Simon | AWS Expert"/>
  <meta property="og:description" content="Expert analysis and technical deep-dive on build a movie recommender with factorization machines on amazon sagemaker by Julien Simon, leading AWS expert and former Global Technical Evangelist for AI & ML at Amazon Web Services."/>
  <meta property="og:image" content="https://julien.org/assets/julien-simon-aws-expert.jpg"/>
  <meta property="og:site_name" content="Julien Simon - AWS Expert"/>
  <meta property="article:author" content="Julien Simon"/>
  <meta property="article:published_time" content="2018-06-18T00:00:00Z"/>
  <meta property="article:section" content="AWS"/>
  <meta property="article:tag" content="AWS, Amazon Web Services, Machine Learning, AI"/>
  
  <!-- Twitter -->
  <meta property="twitter:card" content="summary_large_image"/>
  <meta property="twitter:url" content="https://julien.org/blog/build-a-movie-recommender-with-factorization-machines-on-amazon-sagemaker/"/>
  <meta property="twitter:title" content="Build a movie recommender with factorization machines on Amazon SageMaker - Julien Simon | AWS Expert"/>
  <meta property="twitter:description" content="Expert analysis and technical deep-dive on build a movie recommender with factorization machines on amazon sagemaker by Julien Simon, leading AWS expert and former Global Technical Evangelist for AI & ML at Amazon Web Services."/>
  <meta property="twitter:image" content="https://julien.org/assets/julien-simon-aws-expert.jpg"/>
  <meta property="twitter:creator" content="@julsimon"/>
  
  <!-- Canonical URL -->
  <link rel="canonical" href="https://julien.org/blog/build-a-movie-recommender-with-factorization-machines-on-amazon-sagemaker/"/>
  
  <!-- Author and Publisher -->
  <link rel="author" href="https://julien.org/"/>
  <link rel="publisher" href="https://julien.org/"/>
  
  <!-- Structured Data -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "BlogPosting",
    "headline": "Build a movie recommender with factorization machines on Amazon SageMaker",
    "description": "Expert analysis and technical deep-dive on build a movie recommender with factorization machines on amazon sagemaker by Julien Simon, leading AWS expert and former Global Technical Evangelist for AI & ML at Amazon Web Services.",
    "image": "https://julien.org/assets/julien-simon-aws-expert.jpg",
    "author": {
      "@type": "Person",
      "name": "Julien Simon",
      "url": "https://julien.org/",
      "jobTitle": "AWS Expert & Former Global Technical Evangelist",
      "worksFor": {
        "@type": "Organization",
        "name": "Amazon Web Services"
      },
      "sameAs": [
        "https://twitter.com/julsimon",
        "https://linkedin.com/in/juliensimon",
        "https://github.com/juliensimon"
      ]
    },
    "publisher": {
      "@type": "Organization",
      "name": "Julien Simon",
      "url": "https://julien.org/",
      "logo": {
        "@type": "ImageObject",
        "url": "https://julien.org/assets/julien-simon-logo.jpg"
      }
    },
    "datePublished": "2018-06-18T00:00:00Z",
    "dateModified": "2018-06-18T00:00:00Z",
    "mainEntityOfPage": {
      "@type": "WebPage",
      "@id": "https://julien.org/blog/build-a-movie-recommender-with-factorization-machines-on-amazon-sagemaker/"
    },
    "url": "https://julien.org/blog/build-a-movie-recommender-with-factorization-machines-on-amazon-sagemaker/",
    "keywords": "AWS, Amazon Web Services, ['Build', 'a', 'movie'], machine learning, AI, cloud computing, Julien Simon, AWS expert",
    "articleSection": "AWS",
    "inLanguage": "en-US",
    "isPartOf": {
      "@type": "Blog",
      "name": "Julien Simon - AWS Expert Blog",
      "url": "https://julien.org/blog/"
    }
  }
  </script>
  
  <!-- Additional SEO Meta Tags -->
  <meta name="twitter:site" content="@julsimon"/>
  <meta name="twitter:creator" content="@julsimon"/>
  <meta name="theme-color" content="#FF9900"/>
  <meta name="msapplication-TileColor" content="#FF9900"/>
  <meta name="apple-mobile-web-app-capable" content="yes"/>
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent"/>
  
  <!-- Preconnect to external domains -->
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  
  <!-- Favicon -->
  <link rel="icon" type="image/x-icon" href="https://julien.org/favicon.ico"/>
  <link rel="apple-touch-icon" href="https://julien.org/apple-touch-icon.png"/>
  
  <style>
   body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
            max-width: 800px;
            margin: 0 auto;
            padding: 20px;
            line-height: 1.6;
            color: #333;
        }
        h1, h2, h3, h4, h5, h6 {
            color: #2c3e50;
            margin-top: 2em;
            margin-bottom: 0.5em;
        }
        h1 { font-size: 2.5em; }
        h2 { font-size: 2em; }
        h3 { font-size: 1.5em; }
        img {
            max-width: 100%;
            height: auto;
            border-radius: 8px;
            box-shadow: 0 4px 6px rgba(0,0,0,0.1);
            margin: 1em 0;
        }
        /* Image alignment classes for text wraparound */
        img.alignleft {
            float: left;
            margin: 0.5em 1.5em 1em 0;
        }
        img.alignright {
            float: right;
            margin: 0.5em 0 1em 1.5em;
        }
        img.aligncenter {
            display: block;
            margin: 1em auto;
            float: none;
        }
        img.alignnone {
            float: none;
            margin: 1em 0;
        }
        /* Clear floats after images */
        .wp-block-image::after,
        p:has(img.alignleft)::after,
        p:has(img.alignright)::after {
            content: "";
            display: table;
            clear: both;
        }
        pre {
            background: #f8f9fa;
            border: 1px solid #e9ecef;
            border-radius: 4px;
            padding: 1em;
            overflow-x: auto;
        }
        code {
            background: #f8f9fa;
            padding: 0.2em 0.4em;
            border-radius: 3px;
            font-family: 'Monaco', 'Consolas', monospace;
        }
        blockquote {
            border-left: 4px solid #3498db;
            margin: 1em 0;
            padding-left: 1em;
            font-style: italic;
            color: #7f8c8d;
        }
        a {
            color: #3498db;
            text-decoration: none;
        }
        a:hover {
            text-decoration: underline;
        }
        p {
            margin-bottom: 1em;
        }
        .author-bio {
            background: #f8f9fa;
            border-left: 4px solid #FF9900;
            padding: 1em;
            margin: 2em 0;
            border-radius: 4px;
        }
        .author-bio h3 {
            margin-top: 0;
            color: #FF9900;
        }
        .breadcrumb {
            color: #666;
            font-size: 0.9em;
            margin-bottom: 1em;
        }
        .breadcrumb a {
            color: #FF9900;
        }
  </style>
 </head>
 <body>
  <div class="breadcrumb">
   <a href="https://julien.org/">Home</a> &gt; 
   <a href="https://julien.org/blog/">Blog</a> &gt; 
   <a href="https://julien.org/blog/aws/">AWS</a> &gt; 
   Build a movie recommender with factorization machines on Amazon SageMaker
  </div>
  
  <h1>Build a movie recommender with factorization machines on Amazon SageMaker</h1>
  
  <div class="author-bio">
   <h3>About the Author</h3>
   <p><strong>Julien Simon</strong> is a leading AWS expert and former Global Technical Evangelist for AI & ML at Amazon Web Services (2018-2021). With over 20 years of experience in software engineering and machine learning, Julien has authored 60+ technical blog posts and is recognized as one of the foremost authorities on AWS AI/ML services, particularly Amazon SageMaker.</p>
   <p>Follow Julien on <a href="https://twitter.com/julsimon" target="_blank">Twitter</a> and <a href="https://linkedin.com/in/juliensimon" target="_blank">LinkedIn</a> for the latest insights on AWS, machine learning, and cloud computing.</p>
  </div>
  
  <p style="color: #666; font-style: italic; margin-bottom: 1em;">
   Published: 2018-06-18 | Originally published at <a href="https://aws.amazon.com/blogs/machine-learning/build-a-movie-recommender-with-factorization-machines-on-amazon-sagemaker/" target="_blank" rel="noopener noreferrer">AWS Blog</a>
  </p>
 <body>
  <h1>
   Build a movie recommender with factorization machines on Amazon SageMaker
  </h1>
  <p style="color: #666; font-style: italic; margin-bottom: 1em;">
   Published: 2018-06-18
  </p>
  <p style="color: #666; font-style: italic; margin-bottom: 2em;">
   Originally published at
   <a href="https://aws.amazon.com/blogs/machine-learning/build-a-movie-recommender-with-factorization-machines-on-amazon-sagemaker/">
    https://aws.amazon.com/blogs/machine-learning/build-a-movie-recommender-with-factorization-machines-on-amazon-sagemaker/
   </a>
  </p>
  <p>
   <a href="https://en.wikipedia.org/wiki/Recommender_system" rel="noopener noreferrer" target="_blank">
    Recommendation
   </a>
   is one of the most popular applications in machine learning (ML). In this blog post, I’ll show you how to build a movie recommendation model based on factorization machines — one of the
   <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/algos.html" rel="noopener noreferrer" target="_blank">
    built-in algorithms
   </a>
   of
   <a href="https://aws.amazon.com/sagemaker" rel="noopener noreferrer" target="_blank">
    Amazon SageMaker
   </a>
   — and the popular
   <a href="https://grouplens.org/datasets/movielens/" rel="noopener noreferrer" target="_blank">
    MovieLens
   </a>
   dataset.
  </p>
  <h2>
   A word about factorization machines
  </h2>
  <p>
   Factorization Machines (FM) are a supervised machine learning technique introduced in 2010 (
   <a href="https://www.csie.ntu.edu.tw/~b97053/paper/Rendle2010FM.pdf" rel="noopener noreferrer" target="_blank">
    research paper
   </a>
   , PDF). FM get their name from their ability to reduce problem dimensionality thanks to matrix factorization.
  </p>
  <p>
   Factorization machines can be used for classification or regression and are much more computationally efficient on large sparse data sets than traditional algorithms like linear regression. This property is why FM are widely used for recommendation. User count and item count are typically very large although the actual number of recommendations is very small (users don’t rate all available items!).
  </p>
  <p>
   Here’s a simple example: Where a sparse rating matrix (dimension 4×4) is factored into a dense user matrix (dimension 4×2) and a dense item matrix (2×4). As you can see, the number of factors (2) is smaller than the number of columns of the rating matrix (4). In addition, this multiplication also lets us fill all blank values in the rating matrix, which we can then use to recommend new items to any user.
  </p>
  <p>
   <img alt="" class="alignnone size-full wp-image-4400" height="256" src="image01.webp" style="margin: 20px 0px 20px 0px;border: 1px solid #cccccc" width="503"/>
  </p>
  <p>
   <em>
    Source: data-artisans.com
   </em>
  </p>
  <p>
   In this post, we’re going to use Factorization Machines to build a movie recommender.
   <strong>
    You can download a companion Jupyter notebook from
    <a href="https://s3.amazonaws.com/aws-ml-blog/artifacts/factorization-machines/Factorization-Machines-Movielens.ipynb" rel="noopener noreferrer" target="_blank">
     S3
    </a>
    or
    <a href="https://github.com/juliensimon/dlnotebooks/blob/master/sagemaker/03-Factorization-Machines-Movielens.ipynb" rel="noopener noreferrer" target="_blank">
     Github
    </a>
    .
   </strong>
  </p>
  <h2>
   The MovieLens dataset
  </h2>
  <p>
   This
   <a href="https://grouplens.org/datasets/movielens/" rel="noopener noreferrer" target="_blank">
    dataset
   </a>
   is a great starting point for recommendation. It comes in multiples sizes. In this blog post we’ll use ml100k: 100,000 ratings from 943 users on 1682 movies. As you can see, the ml100k rating matrix is quite sparse (93.6% to be precise) because  it only holds 100,000 ratings out of a possible 1,586,126 (943*1682).
  </p>
  <p>
   Here are the first 10 lines in the data set: user 754 gave movie 595 a 2-star rating, and so on.
  </p>
  <p>
   # user id, movie id, rating, timestamp
   <br/>
   754         595         2             879452073
   <br/>
   932         157         4             891250667
   <br/>
   751         100         4             889132252
   <br/>
   101         820         3             877136954
   <br/>
   606         1277      3             878148493
   <br/>
   581         475         4             879641850
   <br/>
   13           50           5             882140001
   <br/>
   457         59           5             882397575
   <br/>
   111         321         3             891680076
   <br/>
   123         657         4             879872066
  </p>
  <h2>
   Data set preparation
  </h2>
  <p>
   As explained earlier, FM work best on high-dimension datasets. As a consequence, we’re going to
   <a href="https://www.quora.com/What-is-one-hot-encoding-and-when-is-it-used-in-data-science" rel="noopener noreferrer" target="_blank">
    one-hot encode
   </a>
   user IDs and movie IDs (we’ll ignore timestamps). Thus, each sample in our data set will be a 2,625 Boolean vector (943+1682) with only two values set to 1 with respect to the user ID and movie ID.
  </p>
  <p>
   We’re going to build a binary recommender (that is, like/don’t like). 4-star and 5-star ratings are set to 1. Lower ratings are set to 0.
  </p>
  <p>
   One last thing: the
   <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/fact-machines.html" rel="noopener noreferrer" target="_blank">
    FM implementation
   </a>
   in Amazon SageMaker requires training and test data to be stored in
   <em>
    float32
   </em>
   tensors in
   <a href="https://developers.google.com/protocol-buffers/" rel="noopener noreferrer" target="_blank">
    protobuf
   </a>
   format. (Yes, that sounds complicated 🙂 However, the
   <a href="https://github.com/aws/sagemaker-python-sdk" rel="noopener noreferrer" target="_blank">
    Amazon SageMaker SDK
   </a>
   provides a convenient utility function that takes cares of this, so don’t worry too much about it.
  </p>
  <h2>
   The high-level view
  </h2>
  <p>
   Here are the steps you need to implement:
  </p>
  <ol>
   <li>
    Load the MovieLens training set and test set from disk.
   </li>
   <li>
    For each set, build a sparse matrix holding one-hot encoded data samples.
   </li>
   <li>
    For each set, build a label vector holding ratings.
   </li>
   <li>
    Write both sets to protobuf-encoded files.
   </li>
   <li>
    Copy these files to an Amazon S3 bucket.
   </li>
   <li>
    Configure and run a factorization machines training job on Amazon SageMaker.
   </li>
   <li>
    Deploy the corresponding model to an endpoint.
   </li>
   <li>
    Run some predictions.
   </li>
  </ol>
  <p>
   Let’s get going!
  </p>
  <h2>
   Loading the MovieLens dataset
  </h2>
  <p>
   ml-100k contains multiple text files, but we’re only going to use two of them to build our model:
  </p>
  <ul>
   <li>
    <em>
     ua.base
    </em>
    (90,570 samples) will be our training set.
   </li>
   <li>
    <em>
     ua.test
    </em>
    (9,430 samples) will be our test set.
   </li>
  </ul>
  <p>
   Both files have the same tab-separated format:
  </p>
  <ul>
   <li>
    <strong>
     user id
    </strong>
    (integer between 1 and 943)
   </li>
   <li>
    <strong>
     movie id
    </strong>
    (integer between 1 and 1682)
   </li>
   <li>
    <strong>
     rating
    </strong>
    (integer between 1 and 5)
   </li>
   <li>
    <strong>
     timestamp
    </strong>
    (epoch-based integer)
   </li>
  </ul>
  <p>
   As a consequence, we’re going to build the following data structures:
  </p>
  <ul>
   <li>
    <strong>
     A training sparse matrix
    </strong>
    : 90,570 lines and 2,625 columns (943 one-hot encoded features for the user ID, plus 1682 one-hot encoded features for the movie ID)
   </li>
   <li>
    <strong>
     A training label array
    </strong>
    : 90,570 ratings
   </li>
   <li>
    <strong>
     A test sparse matrix
    </strong>
    : 9,430 lines and 2,625 columns
   </li>
   <li>
    <strong>
     A test label array
    </strong>
    : 9,430 ratings
   </li>
  </ul>
  <p>
   <strong>
    Reminder
   </strong>
   : Each sample must be a single one-hot encoded feature vector. Yes, you do need to concatenate the one-hot encoded values for user ID, movie ID, and any additional feature you might add. Building a list of distinct vectors (one for the user ID, one for the movie ID, etc.) isn’t the right way.
  </p>
  <p>
   Our training matrix is now even sparser: Of all 237,746,250 values (90,570*2,625), only 181,140 are non-zero (90,570*2). In other words, the matrix is
   <strong>
    99.92% sparse
   </strong>
   . Storing this as a dense matrix would be a massive waste of both storage and computing power.
  </p>
  <p>
   To avoid this, let’s use a
   <a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.lil_matrix.html" rel="noopener noreferrer" target="_blank">
    <em>
     scipy.lil_matrix
    </em>
   </a>
   <em>
   </em>
   sparse matrix for samples and a
   <a href="http://www.numpy.org/" rel="noopener noreferrer" target="_blank">
    <em>
     numpy
    </em>
   </a>
   array for labels.
  </p>
  <p>
   We should check that we have approximately the same number of samples per class. An unbalanced data set is a serious problem for classifiers.
  </p>
  <div class="hide-language">
   <pre><code class="lang-code">print(np.count_nonzero(Y_train)/nbRatingsTrain)
0.55
print(np.count_nonzero(Y_test)/nbRatingsTest)
0.58</code></pre>
  </div>
  <p>
   Slightly unbalanced, but nothing bad. Let’s move on!
  </p>
  <h2>
   Writing to protobuf files
  </h2>
  <p>
   Next, we’re going to write the training set and the test set to two protobuf files stored in Amazon S3. Fortunately, we can rely on the
   <em>
    write_spmatrix_to_sparse_tensor()
   </em>
   utility function. It writes our samples and labels into an in-memory protobuf-encoded sparse multi-dimensional array (AKA tensor).
  </p>
  <p>
   Then we commit the buffer to Amazon S3. After this step is complete, we’re done with data preparation, and we can now focus on our training job.
  </p>
  <h2>
   <strong>
    Troubleshooting tips for training
   </strong>
  </h2>
  <ul>
   <li>
    Are both samples and labels float32 values?
   </li>
   <li>
    Are samples stored in a sparse matrix (not a numpy array or anything else)?
   </li>
   <li>
    Are labels stored in a vector (not any kind of matrix)?
   </li>
   <li>
    Is write_spmatrix_to_sparse_tensor() undefined? It was added in
    <a href="https://github.com/aws/sagemaker-python-sdk/blob/master/CHANGELOG.md" rel="noopener noreferrer" target="_blank">
     SDK 1.0.2
    </a>
    , and you might need to upgrade the Amazon SageMaker SDK See the appendix at the end of the post.
   </li>
  </ul>
  <h2>
   Note: Upgrading to the latest Amazon SageMaker SDK.
  </h2>
  <ol>
   <li>
    Open your notebook instance.
   </li>
   <li>
    On your instance, open a Jupyter terminal.
   </li>
   <li>
    Activate the Conda environment where you’d like to upgrade the SDK, for example:
   </li>
  </ol>
  <div class="hide-language">
   <pre><code class="lang-python">source activate python2
pip install -U sagemaker</code></pre>
  </div>
  <p>
   The following is our training set in Amazon S3: only 5.5MB
   <strong>
    .
   </strong>
   Sparse matrices FTW!
  </p>
  <div class="hide-language">
   <pre><code class="lang-python">$ aws s3 ls s3://jsimon-sagemaker-us/sagemaker/fm-movielens/train/train.protobuf2018-01-28 16:50:29    5796480 train.protobuf</code></pre>
  </div>
  <h2>
   Running the training job
  </h2>
  <p>
   Let’s start by creating an
   <strong>
    Estimator
   </strong>
   based on the FM container available in our AWS Region. Then, we have to set some FM-specific
   <strong>
    hyperparameters
   </strong>
   (full list in the
   <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/fact-machines-hyperparameters.html" rel="noopener noreferrer" target="_blank">
    documentation
   </a>
   ):
  </p>
  <ul>
   <li>
    <em>
     feature_dim
    </em>
    : the number of features in each sample (2,625 in our case).
   </li>
   <li>
    <em>
     predictor_type
    </em>
    : ‘
    <em>
     binary_classifier
    </em>
    ’ is what we’re going to use.
   </li>
   <li>
    <em>
     num_factors
    </em>
    : the common dimension for the user and item matrices (as explained in the example at the start of the post).
   </li>
  </ul>
  <p>
   The other ones used here are optional (and quite self-explanatory).
  </p>
  <p>
   Finally, let’s run the training job. Calling the
   <em>
    fit()
   </em>
   API is all it takes, passing both the training and test sets hosted in S3. Simple and elegant.
  </p>
  <p>
   A few minutes later, training is complete. We can check out the
   <strong>
    training log
   </strong>
   either in the Jupyter notebook or in Amazon CloudWatch Logs (in the
   <em>
    /aws/sagemaker/trainingjobs
   </em>
   log group).
  </p>
  <p>
   After 50 epochs, test accuracy is 71.5% and the
   <a href="https://en.wikipedia.org/wiki/F1_score" rel="noopener noreferrer" target="_blank">
    F1 score
   </a>
   <strong>
   </strong>
   (a typical metric for a binary classifier) is
   <strong>
    0.75
   </strong>
   (1 indicates a perfect classifier). Not great, but with all that sparse matrix and protobuf excitement, I didn’t spend much time tuning hyperparameters. Surely you can do better
  </p>
  <p>
   [01/29/2018 13:42:41 INFO 140015814588224] #test_score (algo-1) :
   <strong>
    binary_classification_accuracy
   </strong>
   <br/>
   [01/29/2018 13:42:41 INFO 140015814588224] #test_score (algo-1) :
   <strong>
    0.7159
   </strong>
   <br/>
   [01/29/2018 13:42:41 INFO 140015814588224] #test_score (algo-1) : binary_classification_cross_entropy
   <br/>
   [01/29/2018 13:42:41 INFO 140015814588224] #test_score (algo-1) : 0.581087609863
   <br/>
   [01/29/2018 13:42:41 INFO 140015814588224] #test_score (algo-1) :
   <strong>
    binary_f_1
   </strong>
   .000
   <br/>
   [01/29/2018 13:42:41 INFO 140015814588224] #test_score (algo-1) :
   <strong>
    0.74558968389
   </strong>
  </p>
  <p>
   We have one last step to cover: model deployment.
  </p>
  <h2>
   Deploying the model
  </h2>
  <p>
   All it takes to deploy the model is a simple API call. In the old days (6 months or so ago), this would have required quite a bit of work, even on AWS. Here, just call
   <em>
    deploy()
   </em>
   <strong>
    <em>
     .
    </em>
   </strong>
  </p>
  <p>
   We’re now ready to invoke the model’s HTTP endpoint thanks to the
   <em>
    predict()
   </em>
   API. The format for both request and response data is JSON, which requires us to provide a simple serializer to convert our sparse matrix samples to JSON.
  </p>
  <p>
   We’re now able to classify any movie for any user. Just build a new data set, process it the same way as the training and test set, and use
   <em>
    predict()
   </em>
   to get results. You should also experiment with different prediction thresholds (set prediction to 1 above a given score and to 0 under it) and see what value gives you the most efficient recommendations. The MovieLens data set also includes movie titles, so there’s plenty more to explore.
  </p>
  <h2>
   Conclusion
  </h2>
  <p>
   Built-in algorithms are a great way to get the job done quickly, without having to write any training code. There’s quite a bit of data preparation involved, but as you saw in this blog post, it’s key to make very large training jobs fast and scalable.
  </p>
  <p>
   If you’re curious about other
   <strong>
   </strong>
   Amazon SageMaker built-in algorithms, here are a couple of previous posts:
  </p>
  <ul>
   <li>
    <a href="https://medium.com/@julsimon/building-a-spam-classifier-pyspark-mllib-vs-sagemaker-xgboost-1980158a900f" rel="noopener noreferrer" target="_blank">
     Spam classification with XGBoost
    </a>
   </li>
   <li>
    <a href="https://medium.com/@julsimon/image-classification-on-amazon-sagemaker-9b66193c8b54" rel="noopener noreferrer" target="_blank">
     Image classification with Deep Learning
    </a>
   </li>
  </ul>
  <p>
   In addition, if you’d like to know more about recommendation systems, here are a few resources you may find interesting.
  </p>
  <ul>
   <li>
    “
    <a href="https://www.computer.org/csdl/mags/ic/2017/03/mic2017030012.html" rel="noopener noreferrer" target="_blank">
     <em>
      Two Decades of Recommender Systems at Amazon.com
     </em>
    </a>
    ” — Research paper
   </li>
   <li>
    <a href="https://github.com/amzn/amazon-dsstne" rel="noopener noreferrer" target="_blank">
     Amazon DSSTNE: Deep Scalable Sparse Tensor Network Engine
    </a>
    — GitHub
   </li>
   <li>
    “
    <a href="https://aws.amazon.com/blogs/big-data/generating-recommendations-at-amazon-scale-with-apache-spark-and-amazon-dsstne/" rel="noopener noreferrer" target="_blank">
     <em>
      Generating Recommendations at Amazon Scale with Apache Spark and Amazon DSSTNE
     </em>
    </a>
    ” — AWS blog
   </li>
   <li>
    “
    <a href="https://www.youtube.com/watch?v=TjaIKijl-IY" rel="noopener noreferrer" target="_blank">
     <em>
      A quick demo of Amazon DSSTNE
     </em>
    </a>
    ” — YouTube video
   </li>
   <li>
    “
    <a href="https://www.youtube.com/watch?v=cftJAuwKWkA" rel="noopener noreferrer" target="_blank">
     <em>
      Using MXNet for Recommendation Modeling at Scale (MAC306)
     </em>
    </a>
    ” — AWS re:Invent 2016 video
   </li>
   <li>
    “
    <a href="https://fr.slideshare.net/AmazonWebServices/building-content-recommendation-systems-using-apache-mxnet-and-gluon-mcl402-reinvent-2017" rel="noopener noreferrer" target="_blank">
     <em>
      Building Content Recommendation Systems Using Apache MXNet and Gluon (MCL402)
     </em>
    </a>
    ” — AWS re:Invent 2017 presentation
   </li>
  </ul>
  <p>
   As always, thank you for reading. Happy to answer questions on
   <a href="https://twitter.com/julsimon/" rel="noopener noreferrer" target="_blank">
    Twitter
   </a>
   .
  </p>
  <p>
   Several of my AWS colleagues provided excellent advice as well as debugging tips, so please let me thank Sireesha Muppala, Yuri Astashanok, David Arpin, and Guy Ernest.
  </p>
  <hr/>
  <h3>
   About the Author
  </h3>
  <p>
   <img alt="" class="alignleft size-full wp-image-3134" height="115" loading="lazy" src="image02.webp" width="100"/>
   <strong>
    Julien is the Artificial Intelligence &amp; Machine Learning Evangelist for EMEA
   </strong>
   . He focuses on helping developers and enterprises bring their ideas to life. In his spare time, he reads the works of JRR Tolkien again and again.
  </p>
  <p>
  </p>
  <p>
  </p>
  <p>
  </p>
  <!-- '"` -->
 
  <hr style="margin: 3em 0; border: none; border-top: 1px solid #eee;">
  
  <div style="background: #f8f9fa; padding: 2em; border-radius: 8px; margin-top: 3em;">
   <h3>About Julien Simon - AWS Expert</h3>
   <p>Julien Simon is a renowned AWS expert with extensive experience in machine learning, artificial intelligence, and cloud computing. During his tenure as Global Technical Evangelist for AI & ML at Amazon Web Services, he authored over 60 technical blog posts covering Amazon SageMaker, AWS AI services, and cloud innovations.</p>
   
   <h4>Key Areas of Expertise:</h4>
   <ul>
    <li>Amazon SageMaker and MLOps</li>
    <li>AWS AI/ML Services (Comprehend, Transcribe, Translate, Polly)</li>
    <li>Machine Learning Infrastructure and Best Practices</li>
    <li>Cloud Architecture and Optimization</li>
    <li>Enterprise AI Strategy and Implementation</li>
   </ul>
   
   <p><strong>Connect with Julien:</strong> <a href="https://twitter.com/julsimon" target="_blank">Twitter</a> | <a href="https://linkedin.com/in/juliensimon" target="_blank">LinkedIn</a> | <a href="https://github.com/juliensimon" target="_blank">GitHub</a></p>
  </div>
  
  <div style="text-align: center; margin-top: 2em; color: #666; font-size: 0.9em;">
   <p>© 2025 Julien Simon - AWS Expert | <a href="https://julien.org/">julien.org</a></p>
  </div>
 </body>
</html>

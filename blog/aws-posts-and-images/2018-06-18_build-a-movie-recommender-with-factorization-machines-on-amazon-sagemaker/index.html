<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  
  <!-- Primary Meta Tags -->
  <title>Build a movie recommender with factorization machines on Amazon SageMaker - Julien Simon | AWS Expert</title>
  <meta name="title" content="Build a movie recommender with factorization machines on Amazon SageMaker - Julien Simon | AWS Expert"/>
  <meta name="description" content="Expert analysis and technical deep-dive on build a movie recommender with factorization machines on amazon sagemaker by Julien Simon, leading AWS expert and former Global Technical Evangelist for AI & ML at Amazon Web Services."/>
  <meta name="keywords" content="AWS, Amazon Web Services, ['Build', 'a', 'movie'], machine learning, AI, cloud computing, Julien Simon, AWS expert"/>
  <meta name="author" content="Julien Simon"/>
  <meta name="robots" content="index, follow"/>
  
  <!-- Open Graph / Facebook -->
  <meta property="og:type" content="article"/>
  <meta property="og:url" content="https://julien.org/blog/build-a-movie-recommender-with-factorization-machines-on-amazon-sagemaker/"/>
  <meta property="og:title" content="Build a movie recommender with factorization machines on Amazon SageMaker - Julien Simon | AWS Expert"/>
  <meta property="og:description" content="Expert analysis and technical deep-dive on build a movie recommender with factorization machines on amazon sagemaker by Julien Simon, leading AWS expert and former Global Technical Evangelist for AI & ML at Amazon Web Services."/>
  <meta property="og:image" content="https://julien.org/assets/julien-simon-aws-expert.jpg"/>
  <meta property="og:site_name" content="Julien Simon - AWS Expert"/>
  <meta property="article:author" content="Julien Simon"/>
  <meta property="article:published_time" content="2018-06-18T00:00:00Z"/>
  <meta property="article:section" content="AWS"/>
  <meta property="article:tag" content="AWS, Amazon Web Services, Machine Learning, AI"/>
  
  <!-- Twitter -->
  <meta property="twitter:card" content="summary_large_image"/>
  <meta property="twitter:url" content="https://julien.org/blog/build-a-movie-recommender-with-factorization-machines-on-amazon-sagemaker/"/>
  <meta property="twitter:title" content="Build a movie recommender with factorization machines on Amazon SageMaker - Julien Simon | AWS Expert"/>
  <meta property="twitter:description" content="Expert analysis and technical deep-dive on build a movie recommender with factorization machines on amazon sagemaker by Julien Simon, leading AWS expert and former Global Technical Evangelist for AI & ML at Amazon Web Services."/>
  <meta property="twitter:image" content="https://julien.org/assets/julien-simon-aws-expert.jpg"/>
  <meta property="twitter:creator" content="@julsimon"/>
  
  <!-- Canonical URL -->
  <link rel="canonical" href="https://julien.org/blog/build-a-movie-recommender-with-factorization-machines-on-amazon-sagemaker/"/>
  
  <!-- Author and Publisher -->
  <link rel="author" href="https://julien.org/"/>
  <link rel="publisher" href="https://julien.org/"/>
  
  <!-- Structured Data -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "BlogPosting",
    "headline": "Build a movie recommender with factorization machines on Amazon SageMaker",
    "description": "Expert analysis and technical deep-dive on build a movie recommender with factorization machines on amazon sagemaker by Julien Simon, leading AWS expert and former Global Technical Evangelist for AI & ML at Amazon Web Services.",
    "image": "https://julien.org/assets/julien-simon-aws-expert.jpg",
    "author": {
      "@type": "Person",
      "name": "Julien Simon",
      "url": "https://julien.org/",
      "jobTitle": "AWS Expert & Former Global Technical Evangelist",
      "worksFor": {
        "@type": "Organization",
        "name": "Amazon Web Services"
      },
      "sameAs": [
        "https://twitter.com/julsimon",
        "https://linkedin.com/in/juliensimon",
        "https://github.com/juliensimon"
      ]
    },
    "publisher": {
      "@type": "Organization",
      "name": "Julien Simon",
      "url": "https://julien.org/",
      "logo": {
        "@type": "ImageObject",
        "url": "https://julien.org/assets/julien-simon-logo.jpg"
      }
    },
    "datePublished": "2018-06-18T00:00:00Z",
    "dateModified": "2018-06-18T00:00:00Z",
    "mainEntityOfPage": {
      "@type": "WebPage",
      "@id": "https://julien.org/blog/build-a-movie-recommender-with-factorization-machines-on-amazon-sagemaker/"
    },
    "url": "https://julien.org/blog/build-a-movie-recommender-with-factorization-machines-on-amazon-sagemaker/",
    "keywords": "AWS, Amazon Web Services, ['Build', 'a', 'movie'], machine learning, AI, cloud computing, Julien Simon, AWS expert",
    "articleSection": "AWS",
    "inLanguage": "en-US",
    "isPartOf": {
      "@type": "Blog",
      "name": "Julien Simon - AWS Expert Blog",
      "url": "https://julien.org/blog/"
    }
  }
  </script>
  
  <!-- Additional SEO Meta Tags -->
  <meta name="twitter:site" content="@julsimon"/>
  <meta name="twitter:creator" content="@julsimon"/>
  <meta name="theme-color" content="#FF9900"/>
  <meta name="msapplication-TileColor" content="#FF9900"/>
  <meta name="apple-mobile-web-app-capable" content="yes"/>
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent"/>
  
  <!-- Preconnect to external domains -->
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  
  <!-- Favicon -->
  <link rel="icon" type="image/x-icon" href="https://julien.org/favicon.ico"/>
  <link rel="apple-touch-icon" href="https://julien.org/apple-touch-icon.png"/>
  
  <style>
   body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
            max-width: 800px;
            margin: 0 auto;
            padding: 20px;
            line-height: 1.6;
            color: #333;
        }
        h1, h2, h3, h4, h5, h6 {
            color: #2c3e50;
            margin-top: 2em;
            margin-bottom: 0.5em;
        }
        h1 { font-size: 2.5em; }
        h2 { font-size: 2em; }
        h3 { font-size: 1.5em; }
        img {
            max-width: 100%;
            height: auto;
            border-radius: 8px;
            box-shadow: 0 4px 6px rgba(0,0,0,0.1);
            margin: 1em 0;
        }
        /* Image alignment classes for text wraparound */
        img.alignleft {
            float: left;
            margin: 0.5em 1.5em 1em 0;
        }
        img.alignright {
            float: right;
            margin: 0.5em 0 1em 1.5em;
        }
        img.aligncenter {
            display: block;
            margin: 1em auto;
            float: none;
        }
        img.alignnone {
            float: none;
            margin: 1em 0;
        }
        /* Clear floats after images */
        .wp-block-image::after,
        p:has(img.alignleft)::after,
        p:has(img.alignright)::after {
            content: "";
            display: table;
            clear: both;
        }
        pre {
            background: #f8f9fa;
            border: 1px solid #e9ecef;
            border-radius: 4px;
            padding: 1em;
            overflow-x: auto;
        }
        code {
            background: #f8f9fa;
            padding: 0.2em 0.4em;
            border-radius: 3px;
            font-family: 'Monaco', 'Consolas', monospace;
        }
        blockquote {
            border-left: 4px solid #3498db;
            margin: 1em 0;
            padding-left: 1em;
            font-style: italic;
            color: #7f8c8d;
        }
        a {
            color: #3498db;
            text-decoration: none;
        }
        a:hover {
            text-decoration: underline;
        }
        p {
            margin-bottom: 1em;
        }
        .author-bio {
            background: #f8f9fa;
            border-left: 4px solid #FF9900;
            padding: 1em;
            margin: 2em 0;
            border-radius: 4px;
        }
        .author-bio h3 {
            margin-top: 0;
            color: #FF9900;
        }
        .breadcrumb {
            color: #666;
            font-size: 0.9em;
            margin-bottom: 1em;
        }
        .breadcrumb a {
            color: #FF9900;
        }
  </style>
 </head>
 <body>
  <div class="breadcrumb">
   <a href="https://julien.org/">Home</a> &gt; 
   <a href="https://julien.org/blog/">Blog</a> &gt; 
   <a href="https://julien.org/blog/aws/">AWS</a> &gt; 
   Build a movie recommender with factorization machines on Amazon SageMaker
  </div>
  
  <h1>Build a movie recommender with factorization machines on Amazon SageMaker</h1>
  
  <div class="author-bio">
   <h3>About the Author</h3>
   <p><strong>Julien Simon</strong> is a leading AWS expert and former Global Technical Evangelist for AI & ML at Amazon Web Services (2018-2021). With over 20 years of experience in software engineering and machine learning, Julien has authored 60+ technical blog posts and is recognized as one of the foremost authorities on AWS AI/ML services, particularly Amazon SageMaker.</p>
   <p>Follow Julien on <a href="https://twitter.com/julsimon" target="_blank">Twitter</a> and <a href="https://linkedin.com/in/juliensimon" target="_blank">LinkedIn</a> for the latest insights on AWS, machine learning, and cloud computing.</p>
  </div>
  
  <p style="color: #666; font-style: italic; margin-bottom: 1em;">
   Published: 2018-06-18 | Originally published at <a href="https://aws.amazon.com/blogs/machine-learning/build-a-movie-recommender-with-factorization-machines-on-amazon-sagemaker/" target="_blank" rel="noopener noreferrer">AWS Blog</a>
  </p>
 <body>
  <h1>
   Build a movie recommender with factorization machines on Amazon SageMaker
  </h1>
  <p style="color: #666; font-style: italic; margin-bottom: 1em;">
   Published: 2018-06-18
  </p>
  <p style="color: #666; font-style: italic; margin-bottom: 2em;">
   Originally published at
   <a href="https://aws.amazon.com/blogs/machine-learning/build-a-movie-recommender-with-factorization-machines-on-amazon-sagemaker/">
    https://aws.amazon.com/blogs/machine-learning/build-a-movie-recommender-with-factorization-machines-on-amazon-sagemaker/
   </a>
  </p>
  <p>
   <a href="https://en.wikipedia.org/wiki/Recommender_system" rel="noopener noreferrer" target="_blank">
    Recommendation
   </a>
   is one of the most popular applications in machine learning (ML). In this blog post, Iâ€™ll show you how to build a movie recommendation model based on factorization machinesâ€Šâ€”â€Šone of the
   <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/algos.html" rel="noopener noreferrer" target="_blank">
    built-in algorithms
   </a>
   of
   <a href="https://aws.amazon.com/sagemaker" rel="noopener noreferrer" target="_blank">
    Amazon SageMaker
   </a>
   â€”â€Šand the popular
   <a href="https://grouplens.org/datasets/movielens/" rel="noopener noreferrer" target="_blank">
    MovieLens
   </a>
   dataset.
  </p>
  <h2>
   A word about factorization machines
  </h2>
  <p>
   Factorization Machines (FM) are aÂ supervised machine learning technique introduced in 2010 (
   <a href="https://www.csie.ntu.edu.tw/~b97053/paper/Rendle2010FM.pdf" rel="noopener noreferrer" target="_blank">
    research paper
   </a>
   , PDF). FM get their name from their ability toÂ reduce problem dimensionalityÂ thanks toÂ matrix factorization.
  </p>
  <p>
   Factorization machines can be used forÂ classificationÂ orÂ regressionÂ and are much moreÂ computationally efficientÂ onÂ large sparse data setsÂ than traditional algorithms like linear regression. This property is why FM are widely used forÂ recommendation. User count and item count are typically very large although the actual number of recommendations is very small (users donâ€™t rate all available items!).
  </p>
  <p>
   Hereâ€™s a simple example: Where aÂ sparse rating matrixÂ (dimension 4Ã—4) is factored into aÂ dense user matrixÂ (dimension 4Ã—2) and aÂ dense item matrix (2Ã—4). As you can see, theÂ number of factorsÂ (2) is smaller than the number of columns of the rating matrix (4). In addition, this multiplication also lets usÂ fill all blank valuesÂ in the rating matrix, which we can then use toÂ recommend new itemsÂ to any user.
  </p>
  <p>
   <img alt="" class="alignnone size-full wp-image-4400" height="256" src="image01.webp" style="margin: 20px 0px 20px 0px;border: 1px solid #cccccc" width="503"/>
  </p>
  <p>
   <em>
    Source: data-artisans.com
   </em>
  </p>
  <p>
   In this post, weâ€™re going to use Factorization Machines to build a movie recommender.
   <strong>
    You can download a companion Jupyter notebook from
    <a href="https://s3.amazonaws.com/aws-ml-blog/artifacts/factorization-machines/Factorization-Machines-Movielens.ipynb" rel="noopener noreferrer" target="_blank">
     S3
    </a>
    or
    <a href="https://github.com/juliensimon/dlnotebooks/blob/master/sagemaker/03-Factorization-Machines-Movielens.ipynb" rel="noopener noreferrer" target="_blank">
     Github
    </a>
    .
   </strong>
  </p>
  <h2>
   The MovieLens dataset
  </h2>
  <p>
   This
   <a href="https://grouplens.org/datasets/movielens/" rel="noopener noreferrer" target="_blank">
    dataset
   </a>
   is a great starting point for recommendation. It comes in multiples sizes. In this blog post weâ€™ll useÂ ml100k:Â 100,000 ratingsÂ fromÂ 943 usersÂ onÂ 1682 movies. As you can see, the ml100k rating matrix is quite sparse (93.6%Â to be precise) because Â it only holdsÂ 100,000Â ratings out of a possibleÂ 1,586,126Â (943*1682).
  </p>
  <p>
   Here are the first 10 lines in the data set: user 754 gave movie 595 a 2-star rating, and so on.
  </p>
  <p>
   # user id, movie id, rating, timestamp
   <br/>
   754Â Â Â Â Â Â Â Â  595Â Â Â Â Â Â Â Â  2Â Â Â Â Â Â Â Â Â Â Â Â  879452073
   <br/>
   932Â Â Â Â Â Â Â Â  157Â Â Â Â Â Â Â Â  4Â Â Â Â Â Â Â Â Â Â Â Â  891250667
   <br/>
   751Â Â Â Â Â Â Â Â  100Â Â Â Â Â Â Â Â  4Â Â Â Â Â Â Â Â Â Â Â Â  889132252
   <br/>
   101Â Â Â Â Â Â Â Â  820Â Â Â Â Â Â Â Â  3Â Â Â Â Â Â Â Â Â Â Â Â  877136954
   <br/>
   606Â Â Â Â Â Â Â Â  1277Â Â Â Â Â  3Â Â Â Â Â Â Â Â Â Â Â Â  878148493
   <br/>
   581Â Â Â Â Â Â Â Â  475Â Â Â Â Â Â Â Â  4Â Â Â Â Â Â Â Â Â Â Â Â  879641850
   <br/>
   13Â Â Â Â Â Â Â Â Â Â  50Â Â Â Â Â Â Â Â Â Â  5Â Â Â Â Â Â Â Â Â Â Â Â  882140001
   <br/>
   457Â Â Â Â Â Â Â Â  59Â Â Â Â Â Â Â Â Â Â  5Â Â Â Â Â Â Â Â Â Â Â Â  882397575
   <br/>
   111Â Â Â Â Â Â Â Â  321Â Â Â Â Â Â Â Â  3Â Â Â Â Â Â Â Â Â Â Â Â  891680076
   <br/>
   123Â Â Â Â Â Â Â Â  657Â Â Â Â Â Â Â Â  4Â Â Â Â Â Â Â Â Â Â Â Â  879872066
  </p>
  <h2>
   Data set preparation
  </h2>
  <p>
   As explained earlier, FM work best onÂ high-dimensionÂ datasets. As a consequence, weâ€™re going to
   <a href="https://www.quora.com/What-is-one-hot-encoding-and-when-is-it-used-in-data-science" rel="noopener noreferrer" target="_blank">
    one-hot encode
   </a>
   user IDs and movie IDs (weâ€™ll ignore timestamps). Thus, each sample in our data set will be aÂ 2,625 Boolean vectorÂ (943+1682) with only two values set to 1 with respect to the user ID and movie ID.
  </p>
  <p>
   Weâ€™re going to build aÂ binary recommenderÂ (that is, like/donâ€™t like). 4-star and 5-star ratings are set to 1. Lower ratings are set to 0.
  </p>
  <p>
   One last thing: the
   <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/fact-machines.html" rel="noopener noreferrer" target="_blank">
    FM implementation
   </a>
   in Amazon SageMaker requires training and test data to be stored in
   <em>
    float32
   </em>
   tensors in
   <a href="https://developers.google.com/protocol-buffers/" rel="noopener noreferrer" target="_blank">
    protobuf
   </a>
   format. (Yes, that sounds complicatedÂ ğŸ™‚ However, the
   <a href="https://github.com/aws/sagemaker-python-sdk" rel="noopener noreferrer" target="_blank">
    Amazon SageMaker SDK
   </a>
   provides a convenient utility function that takes cares of this, so donâ€™t worry too much about it.
  </p>
  <h2>
   The high-level view
  </h2>
  <p>
   Here are the steps you need to implement:
  </p>
  <ol>
   <li>
    Load the MovieLens training set and test set from disk.
   </li>
   <li>
    For each set, build a sparse matrix holding one-hot encoded data samples.
   </li>
   <li>
    For each set, build a label vector holding ratings.
   </li>
   <li>
    Write both sets to protobuf-encoded files.
   </li>
   <li>
    Copy these files to an Amazon S3 bucket.
   </li>
   <li>
    Configure and run a factorization machines training job on Amazon SageMaker.
   </li>
   <li>
    Deploy the corresponding model to an endpoint.
   </li>
   <li>
    Run some predictions.
   </li>
  </ol>
  <p>
   Letâ€™s get going!
  </p>
  <h2>
   Loading the MovieLens dataset
  </h2>
  <p>
   ml-100k contains multiple text files, but weâ€™re only going to use two of them to build our model:
  </p>
  <ul>
   <li>
    <em>
     ua.base
    </em>
    (90,570 samples) will be our training set.
   </li>
   <li>
    <em>
     ua.test
    </em>
    (9,430 samples) will be our test set.
   </li>
  </ul>
  <p>
   Both files have the sameÂ tab-separated format:
  </p>
  <ul>
   <li>
    <strong>
     user id
    </strong>
    (integer between 1 and 943)
   </li>
   <li>
    <strong>
     movie id
    </strong>
    (integer between 1 and 1682)
   </li>
   <li>
    <strong>
     rating
    </strong>
    (integer between 1 and 5)
   </li>
   <li>
    <strong>
     timestamp
    </strong>
    (epoch-based integer)
   </li>
  </ul>
  <p>
   As a consequence, weâ€™re going to build the following data structures:
  </p>
  <ul>
   <li>
    <strong>
     A training sparse matrix
    </strong>
    : 90,570 lines and 2,625 columns (943 one-hot encoded features for the user ID, plus 1682 one-hot encoded features for the movie ID)
   </li>
   <li>
    <strong>
     A training label array
    </strong>
    : 90,570 ratings
   </li>
   <li>
    <strong>
     A test sparse matrix
    </strong>
    : 9,430 lines and 2,625 columns
   </li>
   <li>
    <strong>
     A test label array
    </strong>
    : 9,430 ratings
   </li>
  </ul>
  <p>
   <strong>
    Reminder
   </strong>
   : Each sample must be a single one-hot encoded feature vector. Yes, you do need toÂ concatenate the one-hot encoded values for user ID, movie ID, and any additional feature you might add. Building a list of distinct vectors (one for the user ID, one for the movie ID, etc.) isnâ€™t the right way.
  </p>
  <p>
   Our training matrix is now even sparser: Of all 237,746,250 values (90,570*2,625), only 181,140 are non-zero (90,570*2). In other words, the matrix is
   <strong>
    99.92% sparse
   </strong>
   . Storing this as a dense matrix would be a massive waste of bothÂ storageÂ andÂ computing power.
  </p>
  <p>
   To avoid this, letâ€™s use a
   <a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.lil_matrix.html" rel="noopener noreferrer" target="_blank">
    <em>
     scipy.lil_matrix
    </em>
   </a>
   <em>
   </em>
   sparse matrix for samplesÂ and a
   <a href="http://www.numpy.org/" rel="noopener noreferrer" target="_blank">
    <em>
     numpy
    </em>
   </a>
   array for labels.
  </p>
  <p>
   We should check that we have approximately the same number of samples per class. An unbalanced data set is a serious problem for classifiers.
  </p>
  <div class="hide-language">
   <pre><code class="lang-code">print(np.count_nonzero(Y_train)/nbRatingsTrain)
0.55
print(np.count_nonzero(Y_test)/nbRatingsTest)
0.58</code></pre>
  </div>
  <p>
   Slightly unbalanced, but nothing bad. Letâ€™s move on!
  </p>
  <h2>
   Writing to protobufÂ files
  </h2>
  <p>
   Next, weâ€™re going to write the training set and the test set to twoÂ protobuf files stored inÂ Amazon S3. Fortunately, we can rely on the
   <em>
    write_spmatrix_to_sparse_tensor()
   </em>
   utility function. It writes our samples and labels into an in-memory protobuf-encoded sparse multi-dimensional array (AKA tensor).
  </p>
  <p>
   Then we commit the buffer toÂ Amazon S3. After this step is complete, weâ€™re done with data preparation, and we can now focus on our training job.
  </p>
  <h2>
   <strong>
    Troubleshooting tips for training
   </strong>
  </h2>
  <ul>
   <li>
    Are both samples and labels float32 values?
   </li>
   <li>
    Are samples stored in a sparse matrix (not a numpy array or anything else)?
   </li>
   <li>
    Are labels stored in a vector (not any kind of matrix)?
   </li>
   <li>
    Is write_spmatrix_to_sparse_tensor() undefined? It was added in
    <a href="https://github.com/aws/sagemaker-python-sdk/blob/master/CHANGELOG.md" rel="noopener noreferrer" target="_blank">
     SDK 1.0.2
    </a>
    , and you might need to upgrade the Amazon SageMaker SDK See the appendix at the end of the post.
   </li>
  </ul>
  <h2>
   Note: Upgrading to the latest Amazon SageMaker SDK.
  </h2>
  <ol>
   <li>
    Open your notebook instance.
   </li>
   <li>
    On your instance, open a Jupyter terminal.
   </li>
   <li>
    Activate the Conda environment where youâ€™d like to upgrade the SDK, for example:
   </li>
  </ol>
  <div class="hide-language">
   <pre><code class="lang-python">source activate python2
pip install -U sagemaker</code></pre>
  </div>
  <p>
   The following is our training set in Amazon S3: onlyÂ 5.5MB
   <strong>
    .
   </strong>
   Sparse matrices FTW!
  </p>
  <div class="hide-language">
   <pre><code class="lang-python">$ aws s3 ls s3://jsimon-sagemaker-us/sagemaker/fm-movielens/train/train.protobuf2018-01-28 16:50:29    5796480 train.protobuf</code></pre>
  </div>
  <h2>
   Running the trainingÂ job
  </h2>
  <p>
   Letâ€™s start by creating an
   <strong>
    Estimator
   </strong>
   based on the FM container available in our AWS Region. Then, we have to set some FM-specific
   <strong>
    hyperparameters
   </strong>
   (full list in the
   <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/fact-machines-hyperparameters.html" rel="noopener noreferrer" target="_blank">
    documentation
   </a>
   ):
  </p>
  <ul>
   <li>
    <em>
     feature_dim
    </em>
    : the number of features in each sample (2,625 in our case).
   </li>
   <li>
    <em>
     predictor_type
    </em>
    : â€˜
    <em>
     binary_classifier
    </em>
    â€™ is what weâ€™re going to use.
   </li>
   <li>
    <em>
     num_factors
    </em>
    : the common dimension for the user and item matrices (as explained in the example at the start of the post).
   </li>
  </ul>
  <p>
   The other ones used here are optional (and quite self-explanatory).
  </p>
  <p>
   Finally, letâ€™s run the training job. Calling the
   <em>
    fit()
   </em>
   API is all it takes, passing both the training and test sets hosted in S3. Simple and elegant.
  </p>
  <p>
   A few minutes later, training is complete. We can check out the
   <strong>
    training log
   </strong>
   either in the Jupyter notebook or in Amazon CloudWatch Logs (in the
   <em>
    /aws/sagemaker/trainingjobs
   </em>
   log group).
  </p>
  <p>
   After 50 epochs,Â test accuracy is 71.5%Â and the
   <a href="https://en.wikipedia.org/wiki/F1_score" rel="noopener noreferrer" target="_blank">
    F1 score
   </a>
   <strong>
   </strong>
   (a typical metric for a binary classifier) is
   <strong>
    0.75
   </strong>
   (1 indicates a perfect classifier). Not great, but with all that sparse matrix and protobuf excitement, I didnâ€™t spend much time tuning hyperparameters. Surely you can do better
  </p>
  <p>
   [01/29/2018 13:42:41 INFO 140015814588224] #test_score (algo-1) :
   <strong>
    binary_classification_accuracy
   </strong>
   <br/>
   [01/29/2018 13:42:41 INFO 140015814588224] #test_score (algo-1) :
   <strong>
    0.7159
   </strong>
   <br/>
   [01/29/2018 13:42:41 INFO 140015814588224] #test_score (algo-1) : binary_classification_cross_entropy
   <br/>
   [01/29/2018 13:42:41 INFO 140015814588224] #test_score (algo-1) : 0.581087609863
   <br/>
   [01/29/2018 13:42:41 INFO 140015814588224] #test_score (algo-1) :
   <strong>
    binary_f_1
   </strong>
   .000
   <br/>
   [01/29/2018 13:42:41 INFO 140015814588224] #test_score (algo-1) :
   <strong>
    0.74558968389
   </strong>
  </p>
  <p>
   We have one last step to cover:Â model deployment.
  </p>
  <h2>
   Deploying theÂ model
  </h2>
  <p>
   All it takes to deploy the model is aÂ simple API call. In the old days (6 months or so ago), this would have required quite a bit of work, even on AWS. Here, just call
   <em>
    deploy()
   </em>
   <strong>
    <em>
     .
    </em>
   </strong>
  </p>
  <p>
   Weâ€™re now ready to invoke the modelâ€™s HTTP endpoint thanks to the
   <em>
    predict()
   </em>
   API. The format for bothÂ request and response dataÂ isÂ JSON, which requires us to provide a simpleÂ serializerÂ to convert our sparse matrix samples to JSON.
  </p>
  <p>
   Weâ€™re now able to classify any movie for any user. Just build a new data set, process it the same way as the training and test set, and use
   <em>
    predict()
   </em>
   to get results. You should also experiment with differentÂ prediction thresholds (set prediction to 1 above a given score and to 0 under it) and see what value gives you the most efficient recommendations. The MovieLens data set also includes movie titles, so thereâ€™s plenty more to explore.
  </p>
  <h2>
   Conclusion
  </h2>
  <p>
   Built-in algorithms are a great way to get the job done quickly, without having to write any training code. Thereâ€™s quite a bit of data preparation involved, but as you saw in this blog post, itâ€™s key to make very large training jobsÂ fastÂ andÂ scalable.
  </p>
  <p>
   If youâ€™re curious about other
   <strong>
   </strong>
   Amazon SageMaker built-in algorithms, here are a couple of previous posts:
  </p>
  <ul>
   <li>
    <a href="https://medium.com/@julsimon/building-a-spam-classifier-pyspark-mllib-vs-sagemaker-xgboost-1980158a900f" rel="noopener noreferrer" target="_blank">
     Spam classification with XGBoost
    </a>
   </li>
   <li>
    <a href="https://medium.com/@julsimon/image-classification-on-amazon-sagemaker-9b66193c8b54" rel="noopener noreferrer" target="_blank">
     Image classification with Deep Learning
    </a>
   </li>
  </ul>
  <p>
   In addition, if youâ€™d like to know more aboutÂ recommendation systems, here are a few resources you may find interesting.
  </p>
  <ul>
   <li>
    â€œ
    <a href="https://www.computer.org/csdl/mags/ic/2017/03/mic2017030012.html" rel="noopener noreferrer" target="_blank">
     <em>
      Two Decades of Recommender Systems at Amazon.com
     </em>
    </a>
    â€â€Šâ€”â€ŠResearch paper
   </li>
   <li>
    <a href="https://github.com/amzn/amazon-dsstne" rel="noopener noreferrer" target="_blank">
     Amazon DSSTNE: Deep Scalable Sparse Tensor Network Engine
    </a>
    â€”â€ŠGitHub
   </li>
   <li>
    â€œ
    <a href="https://aws.amazon.com/blogs/big-data/generating-recommendations-at-amazon-scale-with-apache-spark-and-amazon-dsstne/" rel="noopener noreferrer" target="_blank">
     <em>
      Generating Recommendations at Amazon Scale with Apache Spark and Amazon DSSTNE
     </em>
    </a>
    â€â€Šâ€”â€ŠAWS blog
   </li>
   <li>
    â€œ
    <a href="https://www.youtube.com/watch?v=TjaIKijl-IY" rel="noopener noreferrer" target="_blank">
     <em>
      A quick demo of Amazon DSSTNE
     </em>
    </a>
    â€â€Šâ€”â€ŠYouTube video
   </li>
   <li>
    â€œ
    <a href="https://www.youtube.com/watch?v=cftJAuwKWkA" rel="noopener noreferrer" target="_blank">
     <em>
      Using MXNet for Recommendation Modeling at Scale (MAC306)
     </em>
    </a>
    â€â€Šâ€”â€ŠAWS re:Invent 2016 video
   </li>
   <li>
    â€œ
    <a href="https://fr.slideshare.net/AmazonWebServices/building-content-recommendation-systems-using-apache-mxnet-and-gluon-mcl402-reinvent-2017" rel="noopener noreferrer" target="_blank">
     <em>
      Building Content Recommendation Systems Using Apache MXNet and Gluon (MCL402)
     </em>
    </a>
    â€â€Šâ€”â€ŠAWS re:Invent 2017 presentation
   </li>
  </ul>
  <p>
   As always, thank you for reading. Happy to answer questions on
   <a href="https://twitter.com/julsimon/" rel="noopener noreferrer" target="_blank">
    Twitter
   </a>
   .
  </p>
  <p>
   Several of my AWS colleagues provided excellent advice as well as debugging tips, so please let me thankÂ Sireesha Muppala,Â Yuri Astashanok,Â David Arpin,Â andÂ Guy Ernest.
  </p>
  <hr/>
  <h3>
   About the Author
  </h3>
  <p>
   <img alt="" class="alignleft size-full wp-image-3134" height="115" loading="lazy" src="image02.webp" width="100"/>
   <strong>
    Julien is the Artificial Intelligence &amp; Machine Learning Evangelist for EMEA
   </strong>
   . He focuses on helping developers and enterprises bring their ideas to life. In his spare time, he reads the works of JRR Tolkien again and again.
  </p>
  <p>
  </p>
  <p>
  </p>
  <p>
  </p>
  <!-- '"` -->
 
  <hr style="margin: 3em 0; border: none; border-top: 1px solid #eee;">
  
  <div style="background: #f8f9fa; padding: 2em; border-radius: 8px; margin-top: 3em;">
   <h3>About Julien Simon - AWS Expert</h3>
   <p>Julien Simon is a renowned AWS expert with extensive experience in machine learning, artificial intelligence, and cloud computing. During his tenure as Global Technical Evangelist for AI & ML at Amazon Web Services, he authored over 60 technical blog posts covering Amazon SageMaker, AWS AI services, and cloud innovations.</p>
   
   <h4>Key Areas of Expertise:</h4>
   <ul>
    <li>Amazon SageMaker and MLOps</li>
    <li>AWS AI/ML Services (Comprehend, Transcribe, Translate, Polly)</li>
    <li>Machine Learning Infrastructure and Best Practices</li>
    <li>Cloud Architecture and Optimization</li>
    <li>Enterprise AI Strategy and Implementation</li>
   </ul>
   
   <p><strong>Connect with Julien:</strong> <a href="https://twitter.com/julsimon" target="_blank">Twitter</a> | <a href="https://linkedin.com/in/juliensimon" target="_blank">LinkedIn</a> | <a href="https://github.com/juliensimon" target="_blank">GitHub</a></p>
  </div>
  
  <div style="text-align: center; margin-top: 2em; color: #666; font-size: 0.9em;">
   <p>Â© 2025 Julien Simon - AWS Expert | <a href="https://julien.org/">julien.org</a></p>
  </div>
 </body>
</html>

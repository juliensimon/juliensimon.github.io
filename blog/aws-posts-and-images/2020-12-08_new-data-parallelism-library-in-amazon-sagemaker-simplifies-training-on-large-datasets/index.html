<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  
  <!-- Primary Meta Tags -->
  <title>New – Data Parallelism Library in Amazon SageMaker Simplifies Training on Large Datasets - Julien Simon | AWS Expert</title>
  <meta name="title" content="New – Data Parallelism Library in Amazon SageMaker Simplifies Training on Large Datasets - Julien Simon | AWS Expert"/>
  <meta name="description" content="Expert analysis and technical deep-dive on new – data parallelism library in amazon sagemaker simplifies training on large datasets by Julien Simon, leading AWS expert and former Global Technical Evangelist for AI & ML at Amazon Web Services."/>
  <meta name="keywords" content="AWS, Amazon Web Services, ['New', '–', 'Data'], machine learning, AI, cloud computing, Julien Simon, AWS expert"/>
  <meta name="author" content="Julien Simon"/>
  <meta name="robots" content="index, follow"/>
  
  <!-- Open Graph / Facebook -->
  <meta property="og:type" content="article"/>
  <meta property="og:url" content="https://julien.org/blog/new-data-parallelism-library-in-amazon-sagemaker-simplifies-training-on-large-datasets/"/>
  <meta property="og:title" content="New – Data Parallelism Library in Amazon SageMaker Simplifies Training on Large Datasets - Julien Simon | AWS Expert"/>
  <meta property="og:description" content="Expert analysis and technical deep-dive on new – data parallelism library in amazon sagemaker simplifies training on large datasets by Julien Simon, leading AWS expert and former Global Technical Evangelist for AI & ML at Amazon Web Services."/>
  <meta property="og:image" content="https://julien.org/assets/julien-simon-aws-expert.jpg"/>
  <meta property="og:site_name" content="Julien Simon - AWS Expert"/>
  <meta property="article:author" content="Julien Simon"/>
  <meta property="article:published_time" content="2020-12-08T00:00:00Z"/>
  <meta property="article:section" content="AWS"/>
  <meta property="article:tag" content="AWS, Amazon Web Services, Machine Learning, AI"/>
  
  <!-- Twitter -->
  <meta property="twitter:card" content="summary_large_image"/>
  <meta property="twitter:url" content="https://julien.org/blog/new-data-parallelism-library-in-amazon-sagemaker-simplifies-training-on-large-datasets/"/>
  <meta property="twitter:title" content="New – Data Parallelism Library in Amazon SageMaker Simplifies Training on Large Datasets - Julien Simon | AWS Expert"/>
  <meta property="twitter:description" content="Expert analysis and technical deep-dive on new – data parallelism library in amazon sagemaker simplifies training on large datasets by Julien Simon, leading AWS expert and former Global Technical Evangelist for AI & ML at Amazon Web Services."/>
  <meta property="twitter:image" content="https://julien.org/assets/julien-simon-aws-expert.jpg"/>
  <meta property="twitter:creator" content="@julsimon"/>
  
  <!-- Canonical URL -->
  <link rel="canonical" href="https://julien.org/blog/new-data-parallelism-library-in-amazon-sagemaker-simplifies-training-on-large-datasets/"/>
  
  <!-- Author and Publisher -->
  <link rel="author" href="https://julien.org/"/>
  <link rel="publisher" href="https://julien.org/"/>
  
  <!-- Structured Data -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "BlogPosting",
    "headline": "New – Data Parallelism Library in Amazon SageMaker Simplifies Training on Large Datasets",
    "description": "Expert analysis and technical deep-dive on new – data parallelism library in amazon sagemaker simplifies training on large datasets by Julien Simon, leading AWS expert and former Global Technical Evangelist for AI & ML at Amazon Web Services.",
    "image": "https://julien.org/assets/julien-simon-aws-expert.jpg",
    "author": {
      "@type": "Person",
      "name": "Julien Simon",
      "url": "https://julien.org/",
      "jobTitle": "AWS Expert & Former Global Technical Evangelist",
      "worksFor": {
        "@type": "Organization",
        "name": "Amazon Web Services"
      },
      "sameAs": [
        "https://twitter.com/julsimon",
        "https://linkedin.com/in/juliensimon",
        "https://github.com/juliensimon"
      ]
    },
    "publisher": {
      "@type": "Organization",
      "name": "Julien Simon",
      "url": "https://julien.org/",
      "logo": {
        "@type": "ImageObject",
        "url": "https://julien.org/assets/julien-simon-logo.jpg"
      }
    },
    "datePublished": "2020-12-08T00:00:00Z",
    "dateModified": "2020-12-08T00:00:00Z",
    "mainEntityOfPage": {
      "@type": "WebPage",
      "@id": "https://julien.org/blog/new-data-parallelism-library-in-amazon-sagemaker-simplifies-training-on-large-datasets/"
    },
    "url": "https://julien.org/blog/new-data-parallelism-library-in-amazon-sagemaker-simplifies-training-on-large-datasets/",
    "keywords": "AWS, Amazon Web Services, ['New', '–', 'Data'], machine learning, AI, cloud computing, Julien Simon, AWS expert",
    "articleSection": "AWS",
    "inLanguage": "en-US",
    "isPartOf": {
      "@type": "Blog",
      "name": "Julien Simon - AWS Expert Blog",
      "url": "https://julien.org/blog/"
    }
  }
  </script>
  
  <!-- Additional SEO Meta Tags -->
  <meta name="twitter:site" content="@julsimon"/>
  <meta name="twitter:creator" content="@julsimon"/>
  <meta name="theme-color" content="#FF9900"/>
  <meta name="msapplication-TileColor" content="#FF9900"/>
  <meta name="apple-mobile-web-app-capable" content="yes"/>
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent"/>
  
  <!-- Preconnect to external domains -->
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  
  <!-- Favicon -->
  <link rel="icon" type="image/x-icon" href="https://julien.org/favicon.ico"/>
  <link rel="apple-touch-icon" href="https://julien.org/apple-touch-icon.png"/>
  
  <style>
   body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
            max-width: 800px;
            margin: 0 auto;
            padding: 20px;
            line-height: 1.6;
            color: #333;
        }
        h1, h2, h3, h4, h5, h6 {
            color: #2c3e50;
            margin-top: 2em;
            margin-bottom: 0.5em;
        }
        h1 { font-size: 2.5em; }
        h2 { font-size: 2em; }
        h3 { font-size: 1.5em; }
        img {
            max-width: 100%;
            height: auto;
            border-radius: 8px;
            box-shadow: 0 4px 6px rgba(0,0,0,0.1);
            margin: 1em 0;
        }
        /* Image alignment classes for text wraparound */
        img.alignleft {
            float: left;
            margin: 0.5em 1.5em 1em 0;
        }
        img.alignright {
            float: right;
            margin: 0.5em 0 1em 1.5em;
        }
        img.aligncenter {
            display: block;
            margin: 1em auto;
            float: none;
        }
        img.alignnone {
            float: none;
            margin: 1em 0;
        }
        /* Clear floats after images */
        .wp-block-image::after,
        p:has(img.alignleft)::after,
        p:has(img.alignright)::after {
            content: "";
            display: table;
            clear: both;
        }
        pre {
            background: #f8f9fa;
            border: 1px solid #e9ecef;
            border-radius: 4px;
            padding: 1em;
            overflow-x: auto;
        }
        code {
            background: #f8f9fa;
            padding: 0.2em 0.4em;
            border-radius: 3px;
            font-family: 'Monaco', 'Consolas', monospace;
        }
        blockquote {
            border-left: 4px solid #3498db;
            margin: 1em 0;
            padding-left: 1em;
            font-style: italic;
            color: #7f8c8d;
        }
        a {
            color: #3498db;
            text-decoration: none;
        }
        a:hover {
            text-decoration: underline;
        }
        p {
            margin-bottom: 1em;
        }
        .author-bio {
            background: #f8f9fa;
            border-left: 4px solid #FF9900;
            padding: 1em;
            margin: 2em 0;
            border-radius: 4px;
        }
        .author-bio h3 {
            margin-top: 0;
            color: #FF9900;
        }
        .breadcrumb {
            color: #666;
            font-size: 0.9em;
            margin-bottom: 1em;
        }
        .breadcrumb a {
            color: #FF9900;
        }
  </style>
 </head>
 <body>
  <div class="breadcrumb">
   <a href="https://julien.org/">Home</a> &gt; 
   <a href="https://julien.org/blog/">Blog</a> &gt; 
   <a href="https://julien.org/blog/aws/">AWS</a> &gt; 
   AWS Blog Post
  </div>
  
  <h1>New – Data Parallelism Library in Amazon SageMaker Simplifies Training on Large Datasets</h1>
  
    <div class="author-bio">
   <h3>About the Author</h3>
   <p><strong>Julien Simon</strong> is an expert in Practical AI, currently serving as Chief Evangelist at Arcee AI. Named <strong>#1 AI Evangelist globally by AI Magazine in 2021</strong>, he champions cost-effective, privacy-first AI solutions through Small Language Models, challenging the industry trend toward expensive, large-scale alternatives.</p>
   <p>With over 30 years of technology leadership—including executive roles at AWS, Hugging Face, Criteo, and other major companies—Julien has delivered 650+ speaking engagements across 90+ cities in 38 countries. His practical approach empowers enterprises to achieve superior AI outcomes while maintaining cost efficiency and operational simplicity.</p>
   <p>Follow Julien on <a href="https://twitter.com/julsimon" target="_blank">Twitter</a> and <a href="https://linkedin.com/in/juliensimon" target="_blank">LinkedIn</a> for the latest insights on Practical AI, Small Language Models, and enterprise AI solutions.</p>
  </div>
  
  <p style="color: #666; font-style: italic; margin-bottom: 1em;">
   Published: 2020-12-08 | Originally published at <a href="https://aws.amazon.com/blogs/aws/managed-data-parallelism-in-amazon-sagemaker-simplifies-training-on-large-datasets/" target="_blank" rel="noopener noreferrer">AWS Blog</a>
  </p>
 <body>
  <p style="color: #666; font-style: italic; margin-bottom: 1em;">
   Published: 2020-12-08
  </p>
  <p style="color: #666; font-style: italic; margin-bottom: 2em;">
   Originally published at
   <a href="https://aws.amazon.com/blogs/aws/managed-data-parallelism-in-amazon-sagemaker-simplifies-training-on-large-datasets/">
    https://aws.amazon.com/blogs/aws/managed-data-parallelism-in-amazon-sagemaker-simplifies-training-on-large-datasets/
   </a>
  </p>
  <p>
   <span style="color: #000000;">
    Today, I’m particularly happy to announce that
    <a href="https://aws.amazon.com/sagemaker/">
     Amazon SageMaker
    </a>
    now supports a new data parallelism library that makes it easier to train models on datasets that may be as large as hundreds or thousands of gigabytes.
    <br/>
   </span>
  </p>
  <p>
   As data sets and models grow larger and more sophisticated, machine learning (ML) practitioners working on large distributed training jobs have to face increasingly long training times, even when using powerful instances such as the
   <a href="https://aws.amazon.com/ec2/">
    Amazon Elastic Compute Cloud (Amazon EC2)
   </a>
   <a href="https://aws.amazon.com/ec2/instance-types/p3/">
    p3
   </a>
   and
   <a href="https://aws.amazon.com/ec2/instance-types/p4/">
    p4
   </a>
   instances. For example, using a
   <strong>
    ml.p3dn.24xlarge
   </strong>
   instance equipped with 8 NVIDIA V100 GPUs, it takes over 6 hours to train advanced object detection models such as Mask RCNN and Faster RCNN on the publicly available COCO dataset. Likewise, training BERT, a state of the art natural language processing model, takes over 100 hours on the same instance. Some of our customers, such as autonomous vehicle companies, routinely deal with even larger training jobs that run for days on large GPU clusters.
  </p>
  <p>
   As you can imagine, these long training times are a severe bottleneck for ML projects, hurting productivity and slowing down innovation. Customers asked us for help, and we got to work.
  </p>
  <p>
   <span style="text-decoration: underline;">
    <strong>
     Introducing a Data Parallelism Library in Amazon SageMaker
     <br/>
    </strong>
   </span>
   <a href="https://aws.amazon.com/sagemaker/">
    Amazon SageMaker
   </a>
   now helps ML teams reduce distributed training time and cost, thanks to the SageMaker data parallelism library. Available for TensorFlow and PyTorch, the data parallelism library implements a more efficient distribution of computation, optimizes network communication, and fully utilizes our fastest
   <a href="https://aws.amazon.com/ec2/instance-types/p3/">
    p3
   </a>
   and
   <a href="https://aws.amazon.com/ec2/instance-types/p3/">
    p4
   </a>
   GPU instances.
  </p>
  <p>
   Up to 90% of GPU resources can now be used for training, not for data transfer. Distributed training jobs can achieve up near-liner scaling efficiency, regardless of the number of GPUs involved. In other words, if a training job runs for 8 hours on a single instance, it will only take approximately 1 hour on 8 instances, with minimal cost increase.
   <span title="Amazon SageMaker">
    SageMaker
   </span>
   effectively eliminates any trade-off between training cost and training time, allowing ML teams to get results sooner, iterate faster, and accelerate innovation.
  </p>
  <p>
   During his keynote at AWS re:Invent 2020, Swami Sivasubramanian demonstrated the fastest training times to date for T5-3B and Mask-RCNN.
  </p>
  <ul>
   <li style="padding-bottom: 0.5em;">
    The T5-3B model has 3 billion parameters, achieves state-of-the-art accuracy on natural language processing benchmarks, and usually takes weeks of effort to train and tune for performance. We trained this model in 6 days on 256
    <strong>
     ml.p4d.24xlarge
    </strong>
    instances.
   </li>
   <li style="padding-bottom: 0.5em;">
    Mask-RCNN continues to be a popular instance segmentation model used by our customers. Last year at re:Invent, we trained Mask-RCNN in 26 minutes on PyTorch, and in 27 minutes on TensorFlow. This year, we recorded the fastest training time to date for Mask-RCNN at 6:12 minutes on TensorFlow, and 6:45 minutes on PyTorch.
   </li>
  </ul>
  <p>
   Before we explain how
   <a href="https://aws.amazon.com/sagemaker/">
    Amazon SageMaker
   </a>
   is able to achieve such speedups, let’s first explain how data parallelism works, and why it’s hard to scale.
  </p>
  <p>
   <strong>
    <span style="text-decoration: underline;">
     A Primer on Data Parallelism
    </span>
   </strong>
   <br/>
   If you’re training a model on a single GPU, its full internal state is available locally: model parameters, optimizer parameters, gradients (parameter updates computed by backpropagation), and so on. However, things are different when you distribute a training job to a cluster of GPUs.
  </p>
  <p>
   Using a technique named “data parallelism,” the training set is split in mini-batches that are evenly distributed across GPUs. Thus, each GPU only trains the model on a fraction of the total data set. Obviously, this means that the model state will be slightly different on each GPU, as they will process different batches. In order to ensure training convergence, the model state needs to be regularly updated on all nodes. This can be done synchronously or asynchronously:
  </p>
  <ul>
   <li style="padding-bottom: 0.5em;">
    Synchronous training: all GPUs report their gradient updates either to all other GPUs (many-to-many communication), or to a central parameter server that redistributes them (many-to-one, followed by one-to-many). As all updates are applied simultaneously, the model state is in sync on all GPUs, and the next mini-batch can be processed.
   </li>
   <li>
    Asynchronous training: gradient updates are sent to all other nodes, or to a central server. However, they are applied immediately, meaning that model state will differ from one GPU to the next.
   </li>
  </ul>
  <p>
   Unfortunately, these techniques don’t scale very well. As the number of GPUs increases, a parameter server will inevitably become a bottleneck. Even without a parameter server, network congestion soon becomes a problem, as
   <code>
    n
   </code>
   GPUs need to exchange
   <code>
    n*(n-1)
   </code>
   messages after each iteration, for a total amount of
   <code>
    n*(n-1)*model size
   </code>
   bytes. For example, ResNet-50 is a popular model used in computer vision applications. With its 26 million parameters, each 32-bit gradient update takes about 100 megabytes. With 8 GPUs, each iteration requires sending and receiving 56 updates, for a total of 5.6 gigabytes. Even with a fast network, this will cause some overhead, and slow down training.
  </p>
  <p>
   A significant step forward was taken in 2017 thanks to the
   <a href="https://github.com/horovod/horovod">
    Horovod
   </a>
   project. Horovod implemented an optimized communication algorithm for distributed training named “ring-allreduce,” which was soon integrated with popular deep learning libraries.
  </p>
  <p>
   In a nutshell, ring-allreduce is a decentralized asynchronous algorithm. There is no parameter server: nodes are organized in a directed cycle graph (to put it simply, a one-way ring). For each iteration, a node receives a gradient update from its predecessor. Once a node has processed its own batch, it applies both updates (its own and the one it received), and sends the results to its neighbor. With
   <code>
    n
   </code>
   GPUs, each GPU processes
   <code>
    2*(n-1)
   </code>
   messages before all GPUs have been updated. Accordingly, the total amount of data exchanged per GPU is
   <code>
    2*(n-1)*model size
   </code>
   , which is much better than
   <code>
    n*(n-1)*model size
   </code>
   .
  </p>
  <p>
   Still, as datasets keep growing, the network bottleneck issue often rises again. Enter
   <span title="Amazon SageMaker">
    SageMaker
   </span>
   and its new AllReduce algorithm.
  </p>
  <p>
   <span style="text-decoration: underline;">
    <strong>
     A New Data Parallelism Algorithm in Amazon SageMaker
    </strong>
   </span>
   <br/>
   With the AllReduce algorithm, GPUs don’t talk to one another any more. Each GPU stores its gradient updates in GPU memory. When a certain threshold is exceeded, these updates are sharded, and sent to parameter servers running on the CPUs of the GPU instances. This removes the need for dedicated parameter servers.
  </p>
  <p>
   Each CPU is responsible for a subset of the model parameters, and it receives updates coming from all GPUs. For example, with 3 training instances equipped with a single GPU, each GPU in the training cluster would send a third of its gradient updates to each one of the three CPUs.
  </p>
  <p>
   <a href="https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2020/11/27/dp1.png">
    <img alt="Illustration" class="aligncenter wp-image-44390 size-large" height="672" src="image01.webp" width="1024"/>
   </a>
  </p>
  <p>
   Then, each CPU would apply all the gradient updates that it received, and it would distribute the consolidated result back to all GPUs.
  </p>
  <p>
   <img alt="Illustration" class="aligncenter wp-image-44392 size-large" height="672" loading="lazy" src="image02.webp" width="1024"/>
  </p>
  <p>
   Now that we understand how this algorithm works, let’s see how you can use it with your own code, without having to manage any infrastructure.
  </p>
  <p>
   <strong>
    <span style="text-decoration: underline;">
     Training with Data Parallelism in Amazon SageMaker
    </span>
   </strong>
   <br/>
   The SageMaker data parallelism API is designed for ease of use, and should provide seamless integration with existing distributed training toolkits. In most cases, all you have to change in your training code is the
   <code>
    import
   </code>
   statement for Horovod (TensorFlow), or for Distributed Data Parallel (PyTorch).
  </p>
  <p>
   For PyTorch, this would look like this.
  </p>
  <pre><code class="lang-python">import smdistributed.dataparallel.torch.parallel.distributed as dist
dist.init_process_group()
</code></pre>
  <p>
   Then, I need to pin each GPU to a single data parallelism process.
  </p>
  <pre><code class="lang-python">torch.cuda.set_device(dist.get_local_rank())</code></pre>
  <p>
   Then, I define my model as usual, for example:
  </p>
  <pre><code class="lang-python">class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.conv1 = nn.Conv2d(1, 32, 3, 1)
        self.conv2 = nn.Conv2d(32, 64, 3, 1)
        self.dropout1 = nn.Dropout2d(0.25)
        self.dropout2 = nn.Dropout2d(0.5)
        self.fc1 = nn.Linear(9216, 128)
        self.fc2 = nn.Linear(128, 10)
...</code></pre>
  <p>
   Finally, I instantiate my model, and use it to create a
   <code>
    DistributedDataParallel
   </code>
   object like so:
  </p>
  <pre><code class="lang-python">import torch
from smdistributed.dataparallel.torch.parallel.distributed import DistributedDataParallel as DDP
device = torch.device("cuda")
model = DDP(Net().to(device))</code></pre>
  <p>
   The rest of the code is vanilla PyTorch, and I can train it using the
   <code>
    PyTorch
   </code>
   estimator available in the
   <a href="https://sagemaker.readthedocs.io/en/stable/">
    SageMaker SDK
   </a>
   (make sure to use the latest version). Here, I’m using an
   <strong>
    ml.p3.16xlarge
   </strong>
   instance with 8 NVIDIA V100 GPUs.
  </p>
  <pre><code class="lang-python">from sagemaker.pytorch import PyTorch
estimator = PyTorch(
    entry_point='train_pytorch.py',
    role=sagemaker.get_execution_role(),
    framework_version='1.6.0',
    py_version='py3',
    instance_count=1,
    instance_type='ml.p3.16xlarge',
    distribution={'smdistributed':{'dataparallel':{enabled': True}}}
)
estimator.fit()</code></pre>
  <p>
   From then on,
   <span title="Amazon SageMaker">
    SageMaker
   </span>
   takes over and provisions all required infrastructure. You can focus on other tasks while your training job runs.
  </p>
  <p>
   <strong>
    <span style="text-decoration: underline;">
     Getting Started
    </span>
   </strong>
   <br/>
   If your training jobs last for hours or days on multiple GPUs, we believe that the SageMaker Data Parallelism library can save you time and money, and help you experiment and innovate quicker. It’s available today at in all regions where
   <span title="Amazon SageMaker">
    SageMaker
   </span>
   is available, at no additional cost.
  </p>
  <p>
   <a href="https://sagemaker-examples.readthedocs.io/en/latest/training/distributed_training/index.html">
    Examples
   </a>
   are available to get you started quickly.
   <a href="https://console.aws.amazon.com/sagemaker/">
    Give them a try
   </a>
   , and let us know what you think. We’re always looking forward to your feedback, either through your usual AWS support contacts, or on the
   <a href="https://forums.aws.amazon.com/forum.jspa?forumID=285">
    AWS Forum
   </a>
   for SageMaker.
  </p>
  <a href="https://aws.amazon.com/developer/community/evangelists/julien-simon/">
   - Julien
  </a>
  <!-- '"` -->
 
  
  <hr style="margin: 3em 0; border: none; border-top: 1px solid #eee;">
  
  <div style="background: #f8f9fa; padding: 2em; border-radius: 8px; margin-top: 3em;">
   <h3>About Julien Simon - Leading Voice in Practical AI</h3>
   <p>Julien Simon is an expert in Practical AI, currently serving as Chief Evangelist at Arcee AI. Named <strong>#1 AI Evangelist globally by AI Magazine in 2021</strong>, he champions cost-effective, privacy-first AI solutions through Small Language Models, challenging the industry trend toward expensive, large-scale alternatives.</p>
   
   <p>With over 30 years of technology leadership—including executive roles at AWS, Hugging Face, Criteo, and other major companies—Julien has delivered 650+ speaking engagements across 90+ cities in 38 countries. His practical approach empowers enterprises to achieve superior AI outcomes while maintaining cost efficiency and operational simplicity.</p>
   
   <h4>Key Areas of Expertise:</h4>
   <ul>
    <li>Practical AI and Small Language Models</li>
    <li>Enterprise AI Strategy and Implementation</li>
    <li>Amazon SageMaker and AWS AI/ML Services</li>
    <li>Cost-Effective AI Solutions</li>
    <li>Privacy-First AI Deployment</li>
   </ul>
   
   <p><strong>Connect with Julien:</strong> <a href="https://twitter.com/julsimon" target="_blank">Twitter</a> | <a href="https://linkedin.com/in/juliensimon" target="_blank">LinkedIn</a> | <a href="https://github.com/juliensimon" target="_blank">GitHub</a></p>
  </div>
  
  <div style="text-align: center; margin-top: 2em; color: #666; font-size: 0.9em;">
   <p>© 2025 Julien Simon - Leading Voice in Practical AI | <a href="https://julien.org/">julien.org</a></p>
  </div>
  </body>
 </html>

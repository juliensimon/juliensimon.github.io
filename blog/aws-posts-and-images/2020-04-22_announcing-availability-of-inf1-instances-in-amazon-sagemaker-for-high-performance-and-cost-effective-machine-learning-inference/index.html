<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  
  <!-- Primary Meta Tags -->
  <title>
<!-- Umami Analytics -->
<script defer src="https://cloud.umami.is/script.js" data-website-id="27550dad-d418-4f5d-ad1b-dab573da1020"></script>
<link rel="dns-prefetch" href="//cloud.umami.is">Announcing availability of Inf1 instances in Amazon SageMaker for high performance and cost-effective machine learning inference - Julien Simon | AWS Expert</title>
  <meta name="title" content="Announcing availability of Inf1 instances in Amazon SageMaker for high performance and cost-effective machine learning inference - Julien Simon | AWS Expert"/>
  <meta name="description" content="Expert analysis and technical deep-dive on announcing availability of inf1 instances in amazon sagemaker for high performance and cost-effective machine learning inference by Julien Simon, leading AWS expert and former Global Technical Evangelist for AI & ML at Amazon Web Services."/>
  <meta name="keywords" content="AWS, Amazon Web Services, ['Announcing', 'availability', 'of'], machine learning, AI, cloud computing, Julien Simon, AWS expert"/>
  <meta name="author" content="Julien Simon"/>
  <meta name="robots" content="index, follow"/>
  
  <!-- Open Graph / Facebook -->
  <meta property="og:type" content="article"/>
  <meta property="og:url" content="https://julien.org/blog/announcing-availability-of-inf1-instances-in-amazon-sagemaker-for-high-performance-and-cost-effective-machine-learning-inference/"/>
  <meta property="og:title" content="Announcing availability of Inf1 instances in Amazon SageMaker for high performance and cost-effective machine learning inference - Julien Simon | AWS Expert"/>
  <meta property="og:description" content="Expert analysis and technical deep-dive on announcing availability of inf1 instances in amazon sagemaker for high performance and cost-effective machine learning inference by Julien Simon, leading AWS expert and former Global Technical Evangelist for AI & ML at Amazon Web Services."/>
  <meta property="og:image" content="https://julien.org/assets/julien-simon-aws-expert.jpg"/>
  <meta property="og:site_name" content="Julien Simon - AWS Expert"/>
  <meta property="article:author" content="Julien Simon"/>
  <meta property="article:published_time" content="2020-04-22T00:00:00Z"/>
  <meta property="article:section" content="AWS"/>
  <meta property="article:tag" content="AWS, Amazon Web Services, Machine Learning, AI"/>
  
  <!-- Twitter -->
  <meta property="twitter:card" content="summary_large_image"/>
  <meta property="twitter:url" content="https://julien.org/blog/announcing-availability-of-inf1-instances-in-amazon-sagemaker-for-high-performance-and-cost-effective-machine-learning-inference/"/>
  <meta property="twitter:title" content="Announcing availability of Inf1 instances in Amazon SageMaker for high performance and cost-effective machine learning inference - Julien Simon | AWS Expert"/>
  <meta property="twitter:description" content="Expert analysis and technical deep-dive on announcing availability of inf1 instances in amazon sagemaker for high performance and cost-effective machine learning inference by Julien Simon, leading AWS expert and former Global Technical Evangelist for AI & ML at Amazon Web Services."/>
  <meta property="twitter:image" content="https://julien.org/assets/julien-simon-aws-expert.jpg"/>
  <meta property="twitter:creator" content="@julsimon"/>
  
  <!-- Canonical URL -->
  <link rel="canonical" href="https://julien.org/blog/announcing-availability-of-inf1-instances-in-amazon-sagemaker-for-high-performance-and-cost-effective-machine-learning-inference/"/>
  
  <!-- Author and Publisher -->
  <link rel="author" href="https://julien.org/"/>
  <link rel="publisher" href="https://julien.org/"/>
  
  <!-- Structured Data -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "BlogPosting",
    "headline": "Announcing availability of Inf1 instances in Amazon SageMaker for high performance and cost-effective machine learning inference",
    "description": "Expert analysis and technical deep-dive on announcing availability of inf1 instances in amazon sagemaker for high performance and cost-effective machine learning inference by Julien Simon, leading AWS expert and former Global Technical Evangelist for AI & ML at Amazon Web Services.",
    "image": "https://julien.org/assets/julien-simon-aws-expert.jpg",
    "author": {
      "@type": "Person",
      "name": "Julien Simon",
      "url": "https://julien.org/",
      "jobTitle": "AWS Expert & Former Global Technical Evangelist",
      "worksFor": {
        "@type": "Organization",
        "name": "Amazon Web Services"
      },
      "sameAs": [
        "https://twitter.com/julsimon",
        "https://linkedin.com/in/juliensimon",
        "https://github.com/juliensimon"
      ]
    },
    "publisher": {
      "@type": "Organization",
      "name": "Julien Simon",
      "url": "https://julien.org/",
      "logo": {
        "@type": "ImageObject",
        "url": "https://julien.org/assets/julien-simon-logo.jpg"
      }
    },
    "datePublished": "2020-04-22T00:00:00Z",
    "dateModified": "2020-04-22T00:00:00Z",
    "mainEntityOfPage": {
      "@type": "WebPage",
      "@id": "https://julien.org/blog/announcing-availability-of-inf1-instances-in-amazon-sagemaker-for-high-performance-and-cost-effective-machine-learning-inference/"
    },
    "url": "https://julien.org/blog/announcing-availability-of-inf1-instances-in-amazon-sagemaker-for-high-performance-and-cost-effective-machine-learning-inference/",
    "keywords": "AWS, Amazon Web Services, ['Announcing', 'availability', 'of'], machine learning, AI, cloud computing, Julien Simon, AWS expert",
    "articleSection": "AWS",
    "inLanguage": "en-US",
    "isPartOf": {
      "@type": "Blog",
      "name": "Julien Simon - AWS Expert Blog",
      "url": "https://julien.org/blog/"
    }
  }
  </script>
  
  <!-- Additional SEO Meta Tags -->
  <meta name="twitter:site" content="@julsimon"/>
  <meta name="twitter:creator" content="@julsimon"/>
  <meta name="theme-color" content="#FF9900"/>
  <meta name="msapplication-TileColor" content="#FF9900"/>
  <meta name="apple-mobile-web-app-capable" content="yes"/>
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent"/>
  
  <!-- Preconnect to external domains -->
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  
  <!-- Favicon -->
  <link rel="icon" type="image/x-icon" href="https://julien.org/favicon.ico"/>
  <link rel="apple-touch-icon" href="https://julien.org/apple-touch-icon.png"/>
  
  <style>
   body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
            max-width: 800px;
            margin: 0 auto;
            padding: 20px;
            line-height: 1.6;
            color: #333;
        }
        h1, h2, h3, h4, h5, h6 {
            color: #2c3e50;
            margin-top: 2em;
            margin-bottom: 0.5em;
        }
        h1 { font-size: 2.5em; }
        h2 { font-size: 2em; }
        h3 { font-size: 1.5em; }
        img {
            max-width: 100%;
            height: auto;
            border-radius: 8px;
            box-shadow: 0 4px 6px rgba(0,0,0,0.1);
            margin: 1em 0;
        }
        /* Image alignment classes for text wraparound */
        img.alignleft {
            float: left;
            margin: 0.5em 1.5em 1em 0;
        }
        img.alignright {
            float: right;
            margin: 0.5em 0 1em 1.5em;
        }
        img.aligncenter {
            display: block;
            margin: 1em auto;
            float: none;
        }
        img.alignnone {
            float: none;
            margin: 1em 0;
        }
        /* Clear floats after images */
        .wp-block-image::after,
        p:has(img.alignleft)::after,
        p:has(img.alignright)::after {
            content: "";
            display: table;
            clear: both;
        }
        pre {
            background: #f8f9fa;
            border: 1px solid #e9ecef;
            border-radius: 4px;
            padding: 1em;
            overflow-x: auto;
        }
        code {
            background: #f8f9fa;
            padding: 0.2em 0.4em;
            border-radius: 3px;
            font-family: 'Monaco', 'Consolas', monospace;
        }
        blockquote {
            border-left: 4px solid #3498db;
            margin: 1em 0;
            padding-left: 1em;
            font-style: italic;
            color: #7f8c8d;
        }
        a {
            color: #3498db;
            text-decoration: none;
        }
        a:hover {
            text-decoration: underline;
        }
        p {
            margin-bottom: 1em;
        }
        .author-bio {
            background: #f8f9fa;
            border-left: 4px solid #FF9900;
            padding: 1em;
            margin: 2em 0;
            border-radius: 4px;
        }
        .author-bio h3 {
            margin-top: 0;
            color: #FF9900;
        }
        
  </style>
 </head>
 <body>
  <div style="margin-bottom: 1em;">
  <a href="../../../aws-blog-posts.html" style="color: #FF9900; text-decoration: none; font-size: 0.9em;">← Back to AWS Blog Posts</a>
</div>
  
  <h1>Announcing availability of Inf1 instances in Amazon SageMaker for high performance and cost-effective machine learning inference</h1>
  
    
  
  <p style="color: #666; font-style: italic; margin-bottom: 1em;">
   Published: 2020-04-22 | Originally published at <a href="https://aws.amazon.com/blogs/machine-learning/announcing-availability-of-inf1-instances-in-amazon-sagemaker-for-high-performance-and-cost-effective-machine-learning-inference/" target="_blank" rel="noopener noreferrer">AWS Blog</a>
  </p>
 <body>
  <p>
   <a href="https://aws.amazon.com/sagemaker/" rel="noopener noreferrer" target="_blank">
    Amazon SageMaker
   </a>
   is a fully managed service that provides every developer and data scientist with the ability to build, train, and deploy machine learning (ML) models quickly. Tens of thousands of
   <a href="https://aws.amazon.com/sagemaker/customers/" rel="noopener noreferrer" target="_blank">
    customers
   </a>
   , including Intuit, Voodoo, ADP, Cerner, Dow Jones, and Thompson Reuters, use Amazon SageMaker to remove the heavy lifting from each step of the ML process.
  </p>
  <p>
   When it comes to deploying ML models for real-time prediction, Amazon SageMaker provides you with a large selection of AWS
   <a href="https://aws.amazon.com/sagemaker/pricing/instance-types/" rel="noopener noreferrer" target="_blank">
    instance types
   </a>
   , from small CPU instances to multi-GPU instances. This lets you find the right cost/performance ratio for your prediction infrastructure. Today we announce the availability of
   <a href="https://aws.amazon.com/ec2/instance-types/inf1/" rel="noopener noreferrer" target="_blank">
    Inf1
   </a>
   instances in Amazon SageMaker to deliver high performance, low latency, and cost-effective inference.
  </p>
  <h2>
   A primer of Amazon EC2 Inf1 instances
  </h2>
  <p>
   The
   <a href="https://aws.amazon.com/ec2/instance-types/inf1/" rel="noopener noreferrer" target="_blank">
    Amazon EC2 Inf1
   </a>
   instances were
   <a href="https://aws.amazon.com/blogs/aws/amazon-ec2-update-inf1-instances-with-aws-inferentia-chips-for-high-performance-cost-effective-inferencing/" rel="noopener noreferrer" target="_blank">
    launched
   </a>
   at AWS re:Invent 2019. Inf1 instances are powered by
   <a href="https://aws.amazon.com/machine-learning/inferentia/" rel="noopener noreferrer" target="_blank">
    AWS Inferentia
   </a>
   , a custom chip built from the ground up by AWS to accelerate machine learning inference workloads. When compared to G4 instances,
   <a href="https://aws.amazon.com/ec2/instance-types/inf1" rel="noopener noreferrer" target="_blank">
    Inf1
   </a>
   instances offer up to three times the inferencing throughput and up to 45% lower cost per inference.
  </p>
  <p>
   Inf1 instances are available in multiple sizes, with 1, 4, or 16 AWS Inferentia chips. An AWS Inferentia chip contains four NeuronCores. Each implements a high-performance systolic array matrix multiply engine, which massively speeds up typical deep learning operations such as convolution and transformers. NeuronCores are also equipped with a large on-chip cache, which helps cut down on external memory accesses and saves I/O time in the process.
  </p>
  <p>
   When several AWS Inferentia chips are available on an Inf1 instance, you can partition a model across them and store it entirely in cache memory. Alternatively, to serve multi-model predictions from a single Inf1 instance, you can partition the NeuronCores of an AWS Inferentia chip across several models.
  </p>
  <p>
   To run machine learning models on Inf1 instances, you need to compile models to a hardware-optimized representation using the
   <a href="https://github.com/aws/aws-neuron-sdk" rel="noopener noreferrer" target="_blank">
    AWS Neuron SDK
   </a>
   . Since the launch of Inf1 instances, AWS has released five versions of the AWS Neuron SDK that focused on performance improvements and new features, with plans to add more on a regular cadence. For example, image classification (ResNet-50) performance has improved by more than 2X, from 1100 to 2300 images/sec on a single AWS Inferentia chip. This performance improvement translates to 45% lower cost per inference as compared to G4 instances. Support for object detection models starting with Single Shot Detection (SSD) was also added, with Mask R-CNN coming soon.
  </p>
  <p>
   Now let us show you how you can easily compile, load and run models on ml.Inf1 instances in Amazon SageMaker.
  </p>
  <h2>
   Using Inf1 instances in Amazon SageMaker
  </h2>
  <p>
   Compiling and deploying models for Inf1 instances in Amazon SageMaker is straightforward thanks to
   <a href="https://aws.amazon.com/sagemaker/neo/" rel="noopener noreferrer" target="_blank">
    Amazon SageMaker Neo
   </a>
   . The AWS Neuron SDK is integrated with Amazon SageMaker Neo to run your model optimally on Inf1 instances in Amazon SageMaker. You only need to complete the following steps:
  </p>
  <ol>
   <li>
    Train your model as usual.
   </li>
   <li>
    Compile your model for the Inf1 architecture with Amazon SageMaker Neo.
   </li>
   <li>
    Deploy your model on Inf1 instances in Amazon SageMaker.
   </li>
  </ol>
  <p>
   In the following example use case, you train a simple TensorFlow image classifier on the MNIST dataset, like in this
   <a href="https://github.com/awslabs/amazon-sagemaker-examples/blob/master/sagemaker_neo_compilation_jobs/tensorflow_distributed_mnist/tensorflow_distributed_mnist_neo.ipynb" rel="noopener noreferrer" target="_blank">
    sample notebook
   </a>
   on GitHub. The training code would look something like the following:
  </p>
  <div class="hide-language">
   <pre><code class="lang-python">from sagemaker.tensorflow import TensorFlow
mnist_estimator = TensorFlow(entry_point='mnist.py', ...)
mnist_estimator.fit(inputs)</code></pre>
  </div>
  <p>
   To compile the model for an Inf1 instance, you make a single API call and select
   <code>
    ml_inf1
   </code>
   as the deployment target. See the following code:
  </p>
  <div class="hide-language">
   <pre><code class="lang-python"># S3 bucket where the compiled model is saved
output_path ='/'.join(mnist_estimator.output_path.split('/')[:-1])

# Compile the model for Inf1 instances
optimized_estimator = mnist_estimator.compile_model(target_instance_family='ml_inf1',
						input_shape={'data':[1, 784]}, # Batch size 1,28x28 pixels flattened
						output_path=output_path,
						framework='tensorflow',
						framework_version='1.15.0')
</code></pre>
  </div>
  <p>
   Once the machine learning model has been compiled, you deploy the model on an Inf1 instance in Amazon SageMaker using the optimized estimator from Amazon SageMaker Neo. Under the hood, when creating the inference endpoint, Amazon SageMaker automatically selects a container with the
   <a href="https://github.com/neo-ai/neo-ai-dlr" rel="noopener noreferrer" target="_blank">
    Neo Deep Learning Runtime
   </a>
   , a lightweight runtime that will load and invoke the optimized model for inference.
  </p>
  <div class="hide-language">
   <pre><code class="lang-python">optimized_predictor = optimized_estimator.deploy(initial_instance_count = 1,
						instance_type = 'ml.inf1.xlarge')</code></pre>
  </div>
  <p>
   That’s it! After you deploy the model, you can invoke the endpoint and receive predictions in real time with low latency. You can find a full example on
   <a href="https://github.com/awslabs/amazon-sagemaker-examples/blob/master/sagemaker_neo_compilation_jobs/deploy_tensorflow_model_on_Inf1_instance/tensorflow_distributed_mnist_neo_inf1.ipynb" rel="noopener noreferrer" target="_blank">
    Github.
   </a>
  </p>
  <h2>
   Getting Started
  </h2>
  <p>
   Inf1 instances in Amazon SageMaker are available in four sizes: ml.inf1.xlarge, ml.inf1.2xlarge, ml.inf1.6xlarge, and ml.inf1.24xlarge. Machine learning models developed using TensorFlow and MxNet frameworks can be compiled with Amazon SageMaker Neo to run optimally on Inf1 instances and deployed on Inf1 instances in Amazon SageMaker for real-time inference. You can start using Inf1 instances in Amazon SageMaker today in the US East (N. Virginia) and US West (Oregon) Regions.
  </p>
  <hr/>
  <h3>
   About the Author
  </h3>
  <p>
   <strong>
    Julien is the Artificial Intelligence &amp; Machine Learning Evangelist for EMEA
   </strong>
   . He focuses on helping developers and enterprises bring their ideas to life. In his spare time, he reads the works of JRR Tolkien again and again.
  </p>
  <p>
   <strong>
    Julien Simon
   </strong>
   is an Artificial Intelligence &amp; Machine Learning Evangelist for EMEA, Julien focuses on helping developers and enterprises bring their ideas to life.
  </p>
  <p>
  </p>
  <p>
  </p>
  <p>
  </p>
  <!-- '"` -->
 
  
  </body>
 </html>

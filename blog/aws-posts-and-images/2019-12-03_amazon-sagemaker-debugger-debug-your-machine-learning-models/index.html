<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  
  <!-- Primary Meta Tags -->
  <title>Amazon SageMaker Debugger – Debug Your Machine Learning Models - Julien Simon | AWS Expert</title>
  <meta name="title" content="Amazon SageMaker Debugger – Debug Your Machine Learning Models - Julien Simon | AWS Expert"/>
  <meta name="description" content="Expert analysis and technical deep-dive on amazon sagemaker debugger – debug your machine learning models by Julien Simon, leading AWS expert and former Global Technical Evangelist for AI & ML at Amazon Web Services."/>
  <meta name="keywords" content="AWS, Amazon Web Services, ['Amazon', 'SageMaker', 'Debugger'], machine learning, AI, cloud computing, Julien Simon, AWS expert"/>
  <meta name="author" content="Julien Simon"/>
  <meta name="robots" content="index, follow"/>
  
  <!-- Open Graph / Facebook -->
  <meta property="og:type" content="article"/>
  <meta property="og:url" content="https://julien.org/blog/amazon-sagemaker-debugger-debug-your-machine-learning-models/"/>
  <meta property="og:title" content="Amazon SageMaker Debugger – Debug Your Machine Learning Models - Julien Simon | AWS Expert"/>
  <meta property="og:description" content="Expert analysis and technical deep-dive on amazon sagemaker debugger – debug your machine learning models by Julien Simon, leading AWS expert and former Global Technical Evangelist for AI & ML at Amazon Web Services."/>
  <meta property="og:image" content="https://julien.org/assets/julien-simon-aws-expert.jpg"/>
  <meta property="og:site_name" content="Julien Simon - AWS Expert"/>
  <meta property="article:author" content="Julien Simon"/>
  <meta property="article:published_time" content="2019-12-03T00:00:00Z"/>
  <meta property="article:section" content="AWS"/>
  <meta property="article:tag" content="AWS, Amazon Web Services, Machine Learning, AI"/>
  
  <!-- Twitter -->
  <meta property="twitter:card" content="summary_large_image"/>
  <meta property="twitter:url" content="https://julien.org/blog/amazon-sagemaker-debugger-debug-your-machine-learning-models/"/>
  <meta property="twitter:title" content="Amazon SageMaker Debugger – Debug Your Machine Learning Models - Julien Simon | AWS Expert"/>
  <meta property="twitter:description" content="Expert analysis and technical deep-dive on amazon sagemaker debugger – debug your machine learning models by Julien Simon, leading AWS expert and former Global Technical Evangelist for AI & ML at Amazon Web Services."/>
  <meta property="twitter:image" content="https://julien.org/assets/julien-simon-aws-expert.jpg"/>
  <meta property="twitter:creator" content="@julsimon"/>
  
  <!-- Canonical URL -->
  <link rel="canonical" href="https://julien.org/blog/amazon-sagemaker-debugger-debug-your-machine-learning-models/"/>
  
  <!-- Author and Publisher -->
  <link rel="author" href="https://julien.org/"/>
  <link rel="publisher" href="https://julien.org/"/>
  
  <!-- Structured Data -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "BlogPosting",
    "headline": "Amazon SageMaker Debugger – Debug Your Machine Learning Models",
    "description": "Expert analysis and technical deep-dive on amazon sagemaker debugger – debug your machine learning models by Julien Simon, leading AWS expert and former Global Technical Evangelist for AI & ML at Amazon Web Services.",
    "image": "https://julien.org/assets/julien-simon-aws-expert.jpg",
    "author": {
      "@type": "Person",
      "name": "Julien Simon",
      "url": "https://julien.org/",
      "jobTitle": "AWS Expert & Former Global Technical Evangelist",
      "worksFor": {
        "@type": "Organization",
        "name": "Amazon Web Services"
      },
      "sameAs": [
        "https://twitter.com/julsimon",
        "https://linkedin.com/in/juliensimon",
        "https://github.com/juliensimon"
      ]
    },
    "publisher": {
      "@type": "Organization",
      "name": "Julien Simon",
      "url": "https://julien.org/",
      "logo": {
        "@type": "ImageObject",
        "url": "https://julien.org/assets/julien-simon-logo.jpg"
      }
    },
    "datePublished": "2019-12-03T00:00:00Z",
    "dateModified": "2019-12-03T00:00:00Z",
    "mainEntityOfPage": {
      "@type": "WebPage",
      "@id": "https://julien.org/blog/amazon-sagemaker-debugger-debug-your-machine-learning-models/"
    },
    "url": "https://julien.org/blog/amazon-sagemaker-debugger-debug-your-machine-learning-models/",
    "keywords": "AWS, Amazon Web Services, ['Amazon', 'SageMaker', 'Debugger'], machine learning, AI, cloud computing, Julien Simon, AWS expert",
    "articleSection": "AWS",
    "inLanguage": "en-US",
    "isPartOf": {
      "@type": "Blog",
      "name": "Julien Simon - AWS Expert Blog",
      "url": "https://julien.org/blog/"
    }
  }
  </script>
  
  <!-- Additional SEO Meta Tags -->
  <meta name="twitter:site" content="@julsimon"/>
  <meta name="twitter:creator" content="@julsimon"/>
  <meta name="theme-color" content="#FF9900"/>
  <meta name="msapplication-TileColor" content="#FF9900"/>
  <meta name="apple-mobile-web-app-capable" content="yes"/>
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent"/>
  
  <!-- Preconnect to external domains -->
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  
  <!-- Favicon -->
  <link rel="icon" type="image/x-icon" href="https://julien.org/favicon.ico"/>
  <link rel="apple-touch-icon" href="https://julien.org/apple-touch-icon.png"/>
  
  <style>
   body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
            max-width: 800px;
            margin: 0 auto;
            padding: 20px;
            line-height: 1.6;
            color: #333;
        }
        h1, h2, h3, h4, h5, h6 {
            color: #2c3e50;
            margin-top: 2em;
            margin-bottom: 0.5em;
        }
        h1 { font-size: 2.5em; }
        h2 { font-size: 2em; }
        h3 { font-size: 1.5em; }
        img {
            max-width: 100%;
            height: auto;
            border-radius: 8px;
            box-shadow: 0 4px 6px rgba(0,0,0,0.1);
            margin: 1em 0;
        }
        /* Image alignment classes for text wraparound */
        img.alignleft {
            float: left;
            margin: 0.5em 1.5em 1em 0;
        }
        img.alignright {
            float: right;
            margin: 0.5em 0 1em 1.5em;
        }
        img.aligncenter {
            display: block;
            margin: 1em auto;
            float: none;
        }
        img.alignnone {
            float: none;
            margin: 1em 0;
        }
        /* Clear floats after images */
        .wp-block-image::after,
        p:has(img.alignleft)::after,
        p:has(img.alignright)::after {
            content: "";
            display: table;
            clear: both;
        }
        pre {
            background: #f8f9fa;
            border: 1px solid #e9ecef;
            border-radius: 4px;
            padding: 1em;
            overflow-x: auto;
        }
        code {
            background: #f8f9fa;
            padding: 0.2em 0.4em;
            border-radius: 3px;
            font-family: 'Monaco', 'Consolas', monospace;
        }
        blockquote {
            border-left: 4px solid #3498db;
            margin: 1em 0;
            padding-left: 1em;
            font-style: italic;
            color: #7f8c8d;
        }
        a {
            color: #3498db;
            text-decoration: none;
        }
        a:hover {
            text-decoration: underline;
        }
        p {
            margin-bottom: 1em;
        }
        .author-bio {
            background: #f8f9fa;
            border-left: 4px solid #FF9900;
            padding: 1em;
            margin: 2em 0;
            border-radius: 4px;
        }
        .author-bio h3 {
            margin-top: 0;
            color: #FF9900;
        }
        
  </style>
 </head>
 <body>
  <div style="margin-bottom: 1em;">
  <a href="../../../aws-blog-posts.html" style="color: #FF9900; text-decoration: none; font-size: 0.9em;">← Back to AWS Blog Posts</a>
</div>
  
  <h1>Amazon SageMaker Debugger – Debug Your Machine Learning Models</h1>
  
    
  
  <p style="color: #666; font-style: italic; margin-bottom: 1em;">
   Published: 2019-12-03 | Originally published at <a href="https://aws.amazon.com/blogs/aws/amazon-sagemaker-debugger-debug-your-machine-learning-models/" target="_blank" rel="noopener noreferrer">AWS Blog</a>
  </p>
 <body>
  <p>
   Today, we’re extremely happy to announce Amazon SageMaker Debugger, a new capability of
   <a href="https://aws.amazon.com/sagemaker/">
    Amazon SageMaker
   </a>
   that automatically identifies complex issues developing in machine learning (ML) training jobs.
  </p>
  <p>
   Building and training ML models is a mix of science and craft (some would even say witchcraft). From collecting and preparing data sets to experimenting with different algorithms to figuring out optimal training parameters (the dreaded hyperparameters), ML practitioners need to clear quite a few hurdles to deliver high-performance models. This is the very reason why be built
   <a href="https://aws.amazon.com/sagemaker/">
    Amazon SageMaker
   </a>
   : a modular, fully managed service that simplifies and speeds up ML workflows.
  </p>
  <p>
   As I keep finding out, ML seems to be one of Mr. Murphy’s favorite hangouts, and everything that may possibly go wrong often does! In particular, many obscure issues can happen during the training process, preventing your model from correctly extracting and learning patterns present in your data set. I’m not talking about software bugs in ML libraries (although they do happen too): most failed training jobs are caused by an inappropriate initialization of parameters, a poor combination of hyperparameters, a design issue in your own code, etc.
  </p>
  <p>
   To make things worse, these issues are rarely visible immediately: they grow over time, slowly but surely ruining your training process, and yielding low accuracy models. Let’s face it, even if you’re a
   <em>
    bonafide
   </em>
   expert, it’s devilishly difficult and time-consuming to identify them and hunt them down, which is why we built
   <a href="https://aws.amazon.com/sagemaker/debugger">
    Amazon SageMaker Debugger
   </a>
   .
  </p>
  <p>
   Let me tell you more.
  </p>
  <p>
   <strong>
    <span style="text-decoration: underline;">
     Introducing Amazon SageMaker Debugger
     <br/>
    </span>
   </strong>
   In your existing training code for TensorFlow, Keras, Apache MXNet, PyTorch and XGBoost, you can use the new
   <span title="Amazon SageMaker Debugger">
    SageMaker Debugger
   </span>
   SDK to save internal model state at periodic intervals; as you can guess, it will be stored in
   <a href="https://aws.amazon.com/s3/">
    Amazon Simple Storage Service (Amazon S3)
   </a>
   .
  </p>
  <p>
   This state is composed of:
  </p>
  <ul>
   <li>
    The parameters being learned by the model, e.g. weights and biases for neural networks,
   </li>
   <li>
    The changes applied to these parameters by the optimizer, aka gradients,
   </li>
   <li>
    The optimization parameters themselves,
   </li>
   <li>
    Scalar values, e.g. accuracies and losses,
   </li>
   <li>
    The output of each layer,
   </li>
   <li>
    Etc.
   </li>
  </ul>
  <p>
   Each specific set of values – say, the sequence of gradients flowing over time through a specific neural network layer – is saved independently, and referred to as a tensor. Tensors are organized in collections (weights, gradients, etc.), and you can decide which ones you want to save during training. Then, using the
   <span title="Amazon SageMaker">
    SageMaker
   </span>
   SDK and its estimators, you configure your training job as usual, passing additional parameters defining the rules you want
   <span title="Amazon SageMaker Debugger">
    SageMaker Debugger
   </span>
   to apply.
  </p>
  <p>
   A rule is a piece of Python code that analyses tensors for the model in training, looking for specific unwanted conditions. Pre-defined rules are available for common problems such as exploding/vanishing tensors (parameters reaching NaN or zero values), exploding/vanishing gradients, loss not changing, and more. Of course, you can also write your own rules.
  </p>
  <p>
   Once the
   <span title="Amazon SageMaker">
    SageMaker
   </span>
   estimator is configured, you can launch the training job. Immediately, it fires up a debug job for each rule that you configured, and they start inspecting available tensors. If a debug job detects a problem, it stops and logs additional information. A
   <a href="https://aws.amazon.com/blogs/aws/new-cloudwatch-events-track-and-respond-to-changes-to-your-aws-resources/">
    CloudWatch Events
   </a>
   event is also sent, should you want to trigger additional automated steps.
  </p>
  <p>
   So now you know that your deep learning job suffers from say, vanishing gradients. With a little brainstorming and experience, you’ll know where to look: maybe the neural network is too deep? Maybe your learning rate is too small? As the internal state has been saved to S3, you can now use the
   <span title="Amazon SageMaker Debugger">
    SageMaker Debugger
   </span>
   SDK to explore the evolution of tensors over time, confirm your hypothesis and fix the root cause.
  </p>
  <p>
   Let’s see
   <span title="Amazon SageMaker Debugger">
    SageMaker Debugger
   </span>
   in action with a quick demo.
  </p>
  <p>
   <span style="text-decoration: underline;">
    <strong>
     Debugging Machine Learning Models with Amazon SageMaker Debugger
     <br/>
    </strong>
   </span>
   At the core of
   <span title="Amazon SageMaker Debugger">
    SageMaker Debugger
   </span>
   is the ability to capture tensors during training. This requires a little bit of instrumentation in your training code, in order to select the tensor collections you want to save, the frequency at which you want to save them, and whether you want to save the values themselves or a reduction (mean, average, etc.).
  </p>
  <p>
   For this purpose, the
   <span title="Amazon SageMaker Debugger">
    SageMaker Debugger
   </span>
   SDK provides simple APIs for each framework that it supports. Let me show you how this works with a simple TensorFlow script, trying to fit a 2-dimension linear regression model. Of course, you’ll find more examples in this Github
   <a href="https://github.com/awslabs/amazon-sagemaker-examples">
    repository
   </a>
   .
  </p>
  <p>
   Let’s take a look at the initial code:
  </p>
  <pre><code class="lang-python">import argparse
import numpy as np
import tensorflow as tf
import random

parser = argparse.ArgumentParser()
parser.add_argument('--model_dir', type=str, help="S3 path for the model")
parser.add_argument('--lr', type=float, help="Learning Rate", default=0.001)
parser.add_argument('--steps', type=int, help="Number of steps to run", default=100)
parser.add_argument('--scale', type=float, help="Scaling factor for inputs", default=1.0)

args = parser.parse_args()

with tf.name_scope('initialize'):
    # 2-dimensional input sample
    x = tf.placeholder(shape=(None, 2), dtype=tf.float32)
    # Initial weights: [10, 10]
    w = tf.Variable(initial_value=[[10.], [10.]], name='weight1')
    # True weights, i.e. the ones we're trying to learn
    w0 = [[1], [1.]]
with tf.name_scope('multiply'):
    # Compute true label
    y = tf.matmul(x, w0)
    # Compute "predicted" label
    y_hat = tf.matmul(x, w)
with tf.name_scope('loss'):
    # Compute loss
    loss = tf.reduce_mean((y_hat - y) ** 2, name="loss")

optimizer = tf.train.AdamOptimizer(args.lr)
optimizer_op = optimizer.minimize(loss)

with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    for i in range(args.steps):
        x_ = np.random.random((10, 2)) * args.scale
        _loss, opt = sess.run([loss, optimizer_op], {x: x_})
        print (f'Step={i}, Loss={_loss}')
</code></pre>
  <p>
   Let’s train this script using the TensorFlow
   <code>
    <a href="https://sagemaker.readthedocs.io/en/stable/using_tf.html">
     Estimator
    </a>
   </code>
   . I’m using SageMaker
   <a href="https://aws.amazon.com/blogs/machine-learning/use-the-amazon-sagemaker-local-mode-to-train-on-your-notebook-instance/">
    local mode
   </a>
   , which is a great way to quickly iterate on experimental code.
  </p>
  <pre><code class="lang-python">bad_hyperparameters = {'steps': 10, 'lr': 100, 'scale': 100000000000}

estimator = TensorFlow(
    role=sagemaker.get_execution_role(),
    base_job_name='debugger-simple-demo',
    train_instance_count=1,
    train_instance_type='local',
    entry_point='script-v1.py',
    framework_version='1.13.1',
    py_version='py3',
    script_mode=True,
    hyperparameters=bad_hyperparameters)</code></pre>
  <p>
   Looking at the training log, things did not go well.
  </p>
  <p>
   <code>
    Step=0, Loss=7.883463958023267e+23
   </code>
   <br/>
   <code>
    algo-1-hrvqg_1 | Step=1, Loss=9.502028841062608e+23
   </code>
   <br/>
   <code>
    algo-1-hrvqg_1 | Step=2, Loss=nan
   </code>
   <br/>
   <code>
    algo-1-hrvqg_1 | Step=3, Loss=nan
   </code>
   <br/>
   <code>
    algo-1-hrvqg_1 | Step=4, Loss=nan
   </code>
   <br/>
   <code>
    algo-1-hrvqg_1 | Step=5, Loss=nan
   </code>
   <br/>
   <code>
    algo-1-hrvqg_1 | Step=6, Loss=nan
   </code>
   <br/>
   <code>
    algo-1-hrvqg_1 | Step=7, Loss=nan
   </code>
   <br/>
   <code>
    algo-1-hrvqg_1 | Step=8, Loss=nan
   </code>
   <br/>
   <code>
    algo-1-hrvqg_1 | Step=9, Loss=nan
   </code>
  </p>
  <p>
   Loss does not decrease at all, and even goes to infinity… This looks like an exploding tensor problem, which is one of the built-in rules defined in
   <span title="Amazon SageMaker Debugger">
    SageMaker Debugger
   </span>
   . Let’s get to work.
  </p>
  <p>
   <strong>
    <span style="text-decoration: underline;">
     Using the Amazon SageMaker Debugger SDK
     <br/>
    </span>
   </strong>
   In order to capture tensors, I need to instrument the training script with:
  </p>
  <ul>
   <li>
    A
    <code>
     SaveConfig
    </code>
    object specifying the frequency at which tensors should be saved,
   </li>
   <li>
    A
    <code>
     SessionHook
    </code>
    object attached to the TensorFlow session, putting everything together and saving required tensors during training,
   </li>
   <li>
    An (optional)
    <code>
     ReductionConfig
    </code>
    object, listing tensor reductions that should be saved instead of full tensors,
   </li>
   <li>
    An (optional) optimizer wrapper to capture gradients.
   </li>
  </ul>
  <p>
   Here’s the updated code, with extra command line arguments for
   <span title="Amazon SageMaker Debugger">
    SageMaker Debugger
   </span>
   parameters.
  </p>
  <pre><code class="lang-python">import argparse
import numpy as np
import tensorflow as tf
import random
import smdebug.tensorflow as smd

parser = argparse.ArgumentParser()
parser.add_argument('--model_dir', type=str, help="S3 path for the model")
parser.add_argument('--lr', type=float, help="Learning Rate", default=0.001 )
parser.add_argument('--steps', type=int, help="Number of steps to run", default=100 )
parser.add_argument('--scale', type=float, help="Scaling factor for inputs", default=1.0 )
parser.add_argument('--debug_path', type=str, default='/opt/ml/output/tensors')
parser.add_argument('--debug_frequency', type=int, help="How often to save tensor data", default=10)
feature_parser = parser.add_mutually_exclusive_group(required=False)
feature_parser.add_argument('--reductions', dest='reductions', action='store_true', help="save reductions of tensors instead of saving full tensors")
feature_parser.add_argument('--no_reductions', dest='reductions', action='store_false', help="save full tensors")
args = parser.parse_args()
args = parser.parse_args()

reduc = smd.ReductionConfig(reductions=['mean'], abs_reductions=['max'], norms=['l1']) if args.reductions else None

hook = smd.SessionHook(out_dir=args.debug_path,
                       include_collections=['weights', 'gradients', 'losses'],
                       save_config=smd.SaveConfig(save_interval=args.debug_frequency),
                       reduction_config=reduc)

with tf.name_scope('initialize'):
    # 2-dimensional input sample
    x = tf.placeholder(shape=(None, 2), dtype=tf.float32)
    # Initial weights: [10, 10]
    w = tf.Variable(initial_value=[[10.], [10.]], name='weight1')
    # True weights, i.e. the ones we're trying to learn
    w0 = [[1], [1.]]
with tf.name_scope('multiply'):
    # Compute true label
    y = tf.matmul(x, w0)
    # Compute "predicted" label
    y_hat = tf.matmul(x, w)
with tf.name_scope('loss'):
    # Compute loss
    loss = tf.reduce_mean((y_hat - y) ** 2, name="loss")
    hook.add_to_collection('losses', loss)

optimizer = tf.train.AdamOptimizer(args.lr)
optimizer = hook.wrap_optimizer(optimizer)
optimizer_op = optimizer.minimize(loss)

hook.set_mode(smd.modes.TRAIN)

with tf.train.MonitoredSession(hooks=[hook]) as sess:
    for i in range(args.steps):
        x_ = np.random.random((10, 2)) * args.scale
        _loss, opt = sess.run([loss, optimizer_op], {x: x_})
        print (f'Step={i}, Loss={_loss}')
</code></pre>
  <p>
   I also need to modify the TensorFlow
   <code>
    Estimator
   </code>
   , to use the
   <span title="Amazon SageMaker Debugger">
    SageMaker Debugger
   </span>
   -enabled training container and to pass additional parameters.
  </p>
  <pre><code class="lang-python">bad_hyperparameters = {'steps': 10, 'lr': 100, 'scale': 100000000000, 'debug_frequency': 1}

from sagemaker.debugger import Rule, rule_configs
estimator = TensorFlow(
    role=sagemaker.get_execution_role(),
    base_job_name='debugger-simple-demo',
    train_instance_count=1,
    train_instance_type='ml.c5.2xlarge',
    image_name=cpu_docker_image_name,
    entry_point='script-v2.py',
    framework_version='1.15',
    py_version='py3',
    script_mode=True,
    hyperparameters=bad_hyperparameters,
    rules = [Rule.sagemaker(rule_configs.exploding_tensor())]
)

estimator.fit()
2019-11-27 10:42:02 Starting - Starting the training job...
2019-11-27 10:42:25 Starting - Launching requested ML instances
********* Debugger Rule Status *********
*
* ExplodingTensor: InProgress 
*
****************************************</code></pre>
  <p>
   Two jobs are running: the actual training job, and a debug job checking for the rule defined in the
   <code>
    Estimator
   </code>
   . Quickly, the debug job fails!
  </p>
  <p>
   Describing the training job, I can get more information on what happened.
  </p>
  <pre><code class="lang-python">description = client.describe_training_job(TrainingJobName=job_name)
print(description['DebugRuleEvaluationStatuses'][0]['RuleConfigurationName'])
print(description['DebugRuleEvaluationStatuses'][0]['RuleEvaluationStatus'])

ExplodingTensor
IssuesFound
</code></pre>
  <p>
   Let’s take a look at the saved tensors.
  </p>
  <p>
   <span style="text-decoration: underline;">
    <strong>
     Exploring Tensors
     <br/>
    </strong>
   </span>
   I can easily grab the tensors saved in S3 during the training process.
   <span style="text-decoration: underline;">
    <strong>
     <br/>
    </strong>
   </span>
  </p>
  <pre><code class="lang-python">s3_output_path = description["DebugConfig"]["DebugHookConfig"]["S3OutputPath"]
trial = create_trial(s3_output_path)
</code></pre>
  <p>
   Let’s list available tensors.
  </p>
  <p>
   <code class="lang-python">
    trial.tensors()
   </code>
  </p>
  <p>
   <code class="lang-python">
    ['loss/loss:0',
    <br/>
    'gradients/multiply/MatMul_1_grad/tuple/control_dependency_1:0',
    <br/>
    'initialize/weight1:0']
   </code>
  </p>
  <p>
   All values are
   <a href="https://numpy.org">
    numpy
   </a>
   arrays, and I can easily iterate over them.
  </p>
  <pre><code class="lang-python">tensor = 'gradients/multiply/MatMul_1_grad/tuple/control_dependency_1:0'
for s in list(trial.tensor(tensor).steps()):
    print("Value: ", trial.tensor(tensor).step(s).value)

Value:  [[1.1508383e+23] [1.0809098e+23]]
Value:  [[1.0278440e+23] [1.1347468e+23]]
Value:  [[nan] [nan]]
Value:  [[nan] [nan]]
Value:  [[nan] [nan]]
Value:  [[nan] [nan]]
Value:  [[nan] [nan]]
Value:  [[nan] [nan]]
Value:  [[nan] [nan]]
Value:  [[nan] [nan]]</code></pre>
  <p>
   As tensor names include the TensorFlow scope defined in the training code, I can easily see that something is wrong with my matrix multiplication.
  </p>
  <pre><code class="lang-python"># Compute true label
y = tf.matmul(x, w0)
# Compute "predicted" label
y_hat = tf.matmul(x, w)</code></pre>
  <p>
   Digging a little deeper, the
   <code>
    x
   </code>
   input is modified by a scaling parameter, which I set to
   <code>
    100000000000
   </code>
   in the Estimator. The learning rate doesn’t look sane either. Bingo!
  </p>
  <pre><code class="lang-python">x_ = np.random.random((10, 2)) * args.scale

bad_hyperparameters = {'steps': 10, 'lr': 100, 'scale': 100000000000, 'debug_frequency': 1}</code></pre>
  <p>
   As you probably knew all along, setting these hyperparameters to more reasonable values will fix the training issue.
  </p>
  <p>
   <strong>
    <span style="text-decoration: underline;">
     Now Available!
    </span>
   </strong>
   <br/>
   We believe
   <a href="https://aws.amazon.com/sagemaker/debugger">
    Amazon SageMaker Debugger
   </a>
   will help you find and solve training issues quicker, so it’s now your turn to go bug hunting.
  </p>
  <p>
   <a href="https://aws.amazon.com/sagemaker/debugger">
    Amazon SageMaker Debugger
   </a>
   is available today in all commercial regions where
   <a href="https://aws.amazon.com/sagemaker/">
    Amazon SageMaker
   </a>
   is available. Give it a try and please send us feedback, either on the
   <a href="https://forums.aws.amazon.com/forum.jspa?forumID=285">
    AWS forum
   </a>
   for
   <a href="https://aws.amazon.com/sagemaker/">
    Amazon SageMaker
   </a>
   , or through your usual AWS support contacts.
  </p>
  <a href="https://aws.amazon.com/developer/community/evangelists/julien-simon/">
   - Julien
  </a>
  <p>
  </p>
  <p>
  </p>
  <!-- '"` -->
  <hr/>
  <h3>
   About the Author
  </h3>
  <p>
   <strong>
    Julien is the Artificial Intelligence &amp; Machine Learning Evangelist for EMEA
   </strong>
   . He focuses on helping developers and enterprises bring their ideas to life. In his spare time, he reads the works of JRR Tolkien again and again.
  </p>
  <p>
  </p>
  <p>
  </p>
  <p>
  </p>
  <!-- '"` -->
 
  
  </body>
 </html>
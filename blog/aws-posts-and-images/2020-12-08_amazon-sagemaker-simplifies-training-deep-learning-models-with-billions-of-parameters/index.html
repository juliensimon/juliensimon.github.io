<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  
  <!-- Primary Meta Tags -->
  <title>Amazon SageMaker Simplifies Training Deep Learning Models With Billions of Parameters - Julien Simon | AWS Expert</title>
  <meta name="title" content="Amazon SageMaker Simplifies Training Deep Learning Models With Billions of Parameters - Julien Simon | AWS Expert"/>
  <meta name="description" content="Expert analysis and technical deep-dive on amazon sagemaker simplifies training deep learning models with billions of parameters by Julien Simon, leading AWS expert and former Global Technical Evangelist for AI & ML at Amazon Web Services."/>
  <meta name="keywords" content="AWS, Amazon Web Services, ['Amazon', 'SageMaker', 'Simplifies'], machine learning, AI, cloud computing, Julien Simon, AWS expert"/>
  <meta name="author" content="Julien Simon"/>
  <meta name="robots" content="index, follow"/>
  
  <!-- Open Graph / Facebook -->
  <meta property="og:type" content="article"/>
  <meta property="og:url" content="https://julien.org/blog/amazon-sagemaker-simplifies-training-deep-learning-models-with-billions-of-parameters/"/>
  <meta property="og:title" content="Amazon SageMaker Simplifies Training Deep Learning Models With Billions of Parameters - Julien Simon | AWS Expert"/>
  <meta property="og:description" content="Expert analysis and technical deep-dive on amazon sagemaker simplifies training deep learning models with billions of parameters by Julien Simon, leading AWS expert and former Global Technical Evangelist for AI & ML at Amazon Web Services."/>
  <meta property="og:image" content="https://julien.org/assets/julien-simon-aws-expert.jpg"/>
  <meta property="og:site_name" content="Julien Simon - AWS Expert"/>
  <meta property="article:author" content="Julien Simon"/>
  <meta property="article:published_time" content="2020-12-08T00:00:00Z"/>
  <meta property="article:section" content="AWS"/>
  <meta property="article:tag" content="AWS, Amazon Web Services, Machine Learning, AI"/>
  
  <!-- Twitter -->
  <meta property="twitter:card" content="summary_large_image"/>
  <meta property="twitter:url" content="https://julien.org/blog/amazon-sagemaker-simplifies-training-deep-learning-models-with-billions-of-parameters/"/>
  <meta property="twitter:title" content="Amazon SageMaker Simplifies Training Deep Learning Models With Billions of Parameters - Julien Simon | AWS Expert"/>
  <meta property="twitter:description" content="Expert analysis and technical deep-dive on amazon sagemaker simplifies training deep learning models with billions of parameters by Julien Simon, leading AWS expert and former Global Technical Evangelist for AI & ML at Amazon Web Services."/>
  <meta property="twitter:image" content="https://julien.org/assets/julien-simon-aws-expert.jpg"/>
  <meta property="twitter:creator" content="@julsimon"/>
  
  <!-- Canonical URL -->
  <link rel="canonical" href="https://julien.org/blog/amazon-sagemaker-simplifies-training-deep-learning-models-with-billions-of-parameters/"/>
  
  <!-- Author and Publisher -->
  <link rel="author" href="https://julien.org/"/>
  <link rel="publisher" href="https://julien.org/"/>
  
  <!-- Structured Data -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "BlogPosting",
    "headline": "Amazon SageMaker Simplifies Training Deep Learning Models With Billions of Parameters",
    "description": "Expert analysis and technical deep-dive on amazon sagemaker simplifies training deep learning models with billions of parameters by Julien Simon, leading AWS expert and former Global Technical Evangelist for AI & ML at Amazon Web Services.",
    "image": "https://julien.org/assets/julien-simon-aws-expert.jpg",
    "author": {
      "@type": "Person",
      "name": "Julien Simon",
      "url": "https://julien.org/",
      "jobTitle": "AWS Expert & Former Global Technical Evangelist",
      "worksFor": {
        "@type": "Organization",
        "name": "Amazon Web Services"
      },
      "sameAs": [
        "https://twitter.com/julsimon",
        "https://linkedin.com/in/juliensimon",
        "https://github.com/juliensimon"
      ]
    },
    "publisher": {
      "@type": "Organization",
      "name": "Julien Simon",
      "url": "https://julien.org/",
      "logo": {
        "@type": "ImageObject",
        "url": "https://julien.org/assets/julien-simon-logo.jpg"
      }
    },
    "datePublished": "2020-12-08T00:00:00Z",
    "dateModified": "2020-12-08T00:00:00Z",
    "mainEntityOfPage": {
      "@type": "WebPage",
      "@id": "https://julien.org/blog/amazon-sagemaker-simplifies-training-deep-learning-models-with-billions-of-parameters/"
    },
    "url": "https://julien.org/blog/amazon-sagemaker-simplifies-training-deep-learning-models-with-billions-of-parameters/",
    "keywords": "AWS, Amazon Web Services, ['Amazon', 'SageMaker', 'Simplifies'], machine learning, AI, cloud computing, Julien Simon, AWS expert",
    "articleSection": "AWS",
    "inLanguage": "en-US",
    "isPartOf": {
      "@type": "Blog",
      "name": "Julien Simon - AWS Expert Blog",
      "url": "https://julien.org/blog/"
    }
  }
  </script>
  
  <!-- Additional SEO Meta Tags -->
  <meta name="twitter:site" content="@julsimon"/>
  <meta name="twitter:creator" content="@julsimon"/>
  <meta name="theme-color" content="#FF9900"/>
  <meta name="msapplication-TileColor" content="#FF9900"/>
  <meta name="apple-mobile-web-app-capable" content="yes"/>
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent"/>
  
  <!-- Preconnect to external domains -->
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  
  <!-- Favicon -->
  <link rel="icon" type="image/x-icon" href="https://julien.org/favicon.ico"/>
  <link rel="apple-touch-icon" href="https://julien.org/apple-touch-icon.png"/>
  
  <style>
   body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
            max-width: 800px;
            margin: 0 auto;
            padding: 20px;
            line-height: 1.6;
            color: #333;
        }
        h1, h2, h3, h4, h5, h6 {
            color: #2c3e50;
            margin-top: 2em;
            margin-bottom: 0.5em;
        }
        h1 { font-size: 2.5em; }
        h2 { font-size: 2em; }
        h3 { font-size: 1.5em; }
        img {
            max-width: 100%;
            height: auto;
            border-radius: 8px;
            box-shadow: 0 4px 6px rgba(0,0,0,0.1);
            margin: 1em 0;
        }
        /* Image alignment classes for text wraparound */
        img.alignleft {
            float: left;
            margin: 0.5em 1.5em 1em 0;
        }
        img.alignright {
            float: right;
            margin: 0.5em 0 1em 1.5em;
        }
        img.aligncenter {
            display: block;
            margin: 1em auto;
            float: none;
        }
        img.alignnone {
            float: none;
            margin: 1em 0;
        }
        /* Clear floats after images */
        .wp-block-image::after,
        p:has(img.alignleft)::after,
        p:has(img.alignright)::after {
            content: "";
            display: table;
            clear: both;
        }
        pre {
            background: #f8f9fa;
            border: 1px solid #e9ecef;
            border-radius: 4px;
            padding: 1em;
            overflow-x: auto;
        }
        code {
            background: #f8f9fa;
            padding: 0.2em 0.4em;
            border-radius: 3px;
            font-family: 'Monaco', 'Consolas', monospace;
        }
        blockquote {
            border-left: 4px solid #3498db;
            margin: 1em 0;
            padding-left: 1em;
            font-style: italic;
            color: #7f8c8d;
        }
        a {
            color: #3498db;
            text-decoration: none;
        }
        a:hover {
            text-decoration: underline;
        }
        p {
            margin-bottom: 1em;
        }
        .author-bio {
            background: #f8f9fa;
            border-left: 4px solid #FF9900;
            padding: 1em;
            margin: 2em 0;
            border-radius: 4px;
        }
        .author-bio h3 {
            margin-top: 0;
            color: #FF9900;
        }
        .breadcrumb {
            color: #666;
            font-size: 0.9em;
            margin-bottom: 1em;
        }
        .breadcrumb a {
            color: #FF9900;
        }
  </style>
 </head>
 <body>
  <div class="breadcrumb">
   <a href="https://julien.org/">Home</a> &gt; 
   <a href="https://julien.org/blog/">Blog</a> &gt; 
   <a href="https://julien.org/blog/aws/">AWS</a> &gt; 
   AWS Blog Post
  </div>
  
  <h1>Amazon SageMaker Simplifies Training Deep Learning Models With Billions of Parameters</h1>
  
    <div class="author-bio">
   <h3>About the Author</h3>
   <p><strong>Julien Simon</strong> is an expert in Practical AI, currently serving as Chief Evangelist at Arcee AI. Named <strong>#1 AI Evangelist globally by AI Magazine in 2021</strong>, he champions cost-effective, privacy-first AI solutions through Small Language Models, challenging the industry trend toward expensive, large-scale alternatives.</p>
   <p>With over 30 years of technology leadership—including executive roles at AWS, Hugging Face, Criteo, and other major companies—Julien has delivered 650+ speaking engagements across 90+ cities in 38 countries. His practical approach empowers enterprises to achieve superior AI outcomes while maintaining cost efficiency and operational simplicity.</p>
   <p>Follow Julien on <a href="https://twitter.com/julsimon" target="_blank">Twitter</a> and <a href="https://linkedin.com/in/juliensimon" target="_blank">LinkedIn</a> for the latest insights on Practical AI, Small Language Models, and enterprise AI solutions.</p>
  </div>
  
  <p style="color: #666; font-style: italic; margin-bottom: 1em;">
   Published: 2020-12-08 | Originally published at <a href="https://aws.amazon.com/blogs/aws/amazon-sagemaker-simplifies-training-deep-learning-models-with-billions-of-parameters/" target="_blank" rel="noopener noreferrer">AWS Blog</a>
  </p>
 <body>
  <p>
   Today, I’m extremely happy to announce that
   <a href="https://aws.amazon.com/sagemaker/">
    Amazon SageMaker
   </a>
   simplifies the training of very large deep learning models that were previously difficult to train due to hardware limitations.
  </p>
  <p>
   In the last 10 years, a subset of machine learning named deep learning (DL) has taken the world by storm. Based on neural networks, DL algorithms have an extraordinary ability to extract information patterns hidden in vast amounts of unstructured data, such as images, videos, speech, or text. Indeed, DL has quickly achieved impressive results on a variety of complex human-like tasks, especially on computer vision and natural language processing. In fact, innovation has never been faster, as DL keeps improving its results on reference tasks like the ImageNet Large Scale Visual Recognition Challenge (ILSVRC), the General Language Understanding Evaluation (GLUE), or the Stanford Question Answering Dataset (SQUAD).
  </p>
  <p>
   In order to tackle ever more complex tasks, DL researchers are designing increasingly sophisticated models, adding more neuron layers and more connections to improve pattern extraction and prediction accuracy, with a direct impact on model size. For example, you would get very good results on image classification with a 100-megabyte ResNet-50 model. For more difficult tasks such as object detection or instance segmentation, you would have to use larger models such as Mask R-CNN or YOLO v4, weighing in at about 250 megabytes.
  </p>
  <p>
   As you can guess, model growth also impacts the amount of time and hardware resources required for model training, which is why Graphical Processing Units (GPU) have long been the preferred option to train and fine-tune large DL models. Thanks to their massively parallel architecture and their large on-board memory, they make it possible to use a technique called mini-batch training. By sending several data samples at once to the GPU, instead of sending them one by one, communication overhead is reduced, and training jobs are greatly accelerated. For example, the NVIDIA A100 available on the Amazon Elastic Compute Cloud (EC2)
   <a href="https://aws.amazon.com/ec2/instance-types/p4/">
    p4
   </a>
   family has over 7,000 compute cores and 40 gigabytes of fast onboard memory. Surely, that should be enough to train large batches of data on very large models, shouldn’t it?
  </p>
  <p>
   Well, it’s not. Natural language processing behemoths such as OpenAI GPT-2 (1.5 billion parameters), T5-3B (3 billion parameters) and GPT-3 (175 billion parameters) consume tens or even hundreds of gigabytes of GPU memory. Likewise, state-of-the-art models working on high-resolution 3D images can be too large to fit in GPU memory, even with a batch size of 1…
  </p>
  <p>
   Trying to square the circle, DL researchers use a combination of techniques, such as the following:
  </p>
  <ul>
   <li style="padding-bottom: 0.5em;">
    Buy more powerful GPUs, although we just saw that this is simply not an option for some models.
   </li>
   <li style="padding-bottom: 0.5em;">
    Work with less powerful models, and sacrifice accuracy.
   </li>
   <li style="padding-bottom: 0.5em;">
    Implement gradient checkpointing, a technique that relies on saving intermediate training results to disk instead of keeping everything in memory, at the expense of a 20-30% training slowdown.
   </li>
   <li>
    Implement model parallelism, that is to say split the model manually, and train its (smaller) pieces on different GPUs. Needless to say, this is an extremely difficult, time-consuming, and uncertain task, even for expert practitioners.
   </li>
  </ul>
  <p>
   Customers have told us that none of the above was a satisfactory solution to working with very large models. They asked us for a simpler and more cost-effective solution, and we got to work.
  </p>
  <p>
   <span style="text-decoration: underline;">
    <strong>
     Introducing SageMaker’s New Model Parallelism Library
    </strong>
   </span>
   <br/>
   The model parallelism library in
   <span title="Amazon SageMaker">
    SageMaker
   </span>
   automatically and efficiently partitions models across several GPUs, eliminating the need for accuracy compromises or for complex manual work. In addition, thanks to this scale-out approach to model training, not only can you work with very large models without any memory bottleneck, you can also leverage a large number of smaller and more cost-effective GPUs.
  </p>
  <p>
   At launch, this is supported for TensorFlow and PyTorch, and it only requires minimal changes in your code. When you launch a training job, you can specify whether your model should be optimized for speed or for memory usage. Then, Amazon SageMaker runs an initial profiling job on your behalf in order to analyze the compute and memory requirements of your model. This information is then fed to a partitioning algorithm which decides how to split the model and how to map model partitions to GPUs, while minimizing communication. The outcome of the partitioning decision is saved to a file, which is passed as input to the actual training job.
  </p>
  <p>
   As you can see,
   <span title="Amazon SageMaker">
    SageMaker
   </span>
   takes care of everything. If you’d like, you could also manually profile and partition the model, then train on
   <span title="Amazon SageMaker">
    SageMaker
   </span>
   .
  </p>
  <p>
   Before we look at the code, I’d like to give you a quick overview of the internals.
  </p>
  <p>
   <strong>
    <span style="text-decoration: underline;">
     Training with Model Partitions and Microbatches
     <br/>
    </span>
   </strong>
   As model partitions running on different GPUs expect forward pass inputs from each other (activation values), processing training mini-batches across a sequence of partitions would only keep one partition busy at all times, while stalling the other ones.
  </p>
  <p>
   To avoid this inefficient behavior, mini-batches are split into microbatches that are processed in parallel on the different GPUs. For example, GPU #1 could be forward propagating microbatch
   <code>
    n
   </code>
   , while GPU #2 could do the same for microbatch
   <code>
    n+1
   </code>
   . Activation values can be stored, and passed to the next partition whenever it’s ready to accept them.
  </p>
  <p>
   For back propagation, partitions also expect input values from each other (gradients). As a partition can’t simultaneously run forward and backward propagation, we could wait for all GPUs to complete the forward pass on their own microbatch, before letting them run the corresponding backward pass. This simple mode is available in
   <a href="https://aws.amazon.com/sagemaker/">
    Amazon SageMaker
   </a>
   .
  </p>
  <p>
   There’s an even more efficient option, called interleaved mode. Here,
   <span title="Amazon SageMaker">
    SageMaker
   </span>
   replicates partitions according to the number of microbatches. For example, working with 2 microbatches, each GPU would run two copies of the partition it has received. Each copy would collaborate with partitions running on other GPUs, either for forward or backpropagation.
  </p>
  <p>
   Here’s how things could look like, with 4 different microbatches being processed by 2 duplicated partitions.
  </p>
  <p>
   <a href="https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2020/11/20/rubik.png">
    <img alt="Illustration" class="aligncenter wp-image-43497 size-large" height="476" src="image01.webp" width="1024"/>
   </a>
  </p>
  <p>
   To sum things up, interleaving the forward and backward passes of different microbatches is how
   <span title="Amazon SageMaker">
    SageMaker
   </span>
   maximimes GPU utilization.
  </p>
  <p>
   Now, let’s see how we can put this to work with TensorFlow.
  </p>
  <p>
   <span style="text-decoration: underline;">
    <strong>
     Implementing Model Parallelism in Amazon SageMaker
     <br/>
    </strong>
   </span>
   Thanks to the SageMaker model parallelism library, you can easily implement model parallelism in your own TensorFlow code (the process is similar for PyTorch). Here’s what you need to do:
  </p>
  <ul>
   <li style="padding-bottom: 0.5em;">
    Define and initialize the partitioning configuration.
   </li>
   <li style="padding-bottom: 0.5em;">
    Make your model a subclass of the
    <code>
     DistributedModel
    </code>
    class, using standard Keras subclassing.
   </li>
   <li style="padding-bottom: 0.5em;">
    Write and decorate with
    <code>
     @smp.step
    </code>
    a training function that represents a forward and backward step for the model. This function will be pipelined according to the architecture described in the previous section.
   </li>
   <li>
    Optionally, do the same for an evaluation function that will also be pipelined.
   </li>
  </ul>
  <p>
   Let’s apply this to a simple convolution network training on the MNIST dataset, using an
   <strong>
    ml.p3.8xlarge
   </strong>
   instance equipped with 4 NVIDIA V100 GPUs.
  </p>
  <p>
   First, I initialize the model parallelism API.
  </p>
  <pre><code class="lang-python">import smdistributed.modelparallel.tensorflow as smp
smp.init()</code></pre>
  <p>
   Then, I subclass
   <code>
    DistributedModel
   </code>
   and build my model.
  </p>
  <pre><code class="lang-python">class MyModel(smp.DistributedModel):
    def __init__(self):
        super(MyModel, self).__init__()
        self.conv = Conv2D(32, 3, activation="relu")
        self.flatten = Flatten()
        self.dense1 = Dense(128)
        self.dense2 = Dense(10)
. . .</code></pre>
  <p>
   This is what the training function looks like.
  </p>
  <pre><code class="lang-python">@smp.step
def forward_backward(images, labels):
    predictions = model(images, training=True)
    loss = loss_obj(labels, predictions)
    grads = optimizer.get_gradients(loss, model.trainable_variables)
    return grads, loss</code></pre>
  <p>
   Then, I can train as usual with the TensorFlow estimator available in the
   <a href="https://sagemaker.readthedocs.io/en/stable/">
    SageMaker SDK
   </a>
   . I only need to add the model parallelism configuration: 2 partitions (hence training on 2 GPUs), and 2 microbatches (hence 2 copies of each partition) with interleaving.
  </p>
  <pre><code class="lang-python">smd_mp_estimator = TensorFlow(
    entry_point="tf2.py",
    role=role,
    framework_version='2.3.1',
    pv_version='py3',
    instance_count=1,
    instance_type='ml.p3.16xlarge',
    distribution={
        "smdistributed": {
            "modelparallel": {
                "enabled":True,
                "parameters": {
                    "microbatches": 2,
                    "partitions": 2,
                    "pipeline": "interleaved",
                    "optimize": "memory",
                    "horovod": True, 
                }
             }
         },
        "mpi": {
            "enabled": True,
            "processes_per_host": 2, # Pick your processes_per_host
            "custom_mpi_options": mpioptions
        },
    }
)</code></pre>
  <p>
   <span style="text-decoration: underline;">
    <strong>
     Getting Started
    </strong>
    <br/>
   </span>
   As you can see, model parallelism makes it easier to train very large state-of-the-art deep learning models. It’s available today in all regions where Amazon SageMaker is available, at no additional cost.
  </p>
  <p>
   <a href="https://sagemaker-examples.readthedocs.io/en/latest/training/distributed_training/index.html">
    Examples
   </a>
   are available to get you started right away.
   <a href="http://console.aws.amazon.com/sagemaker" rel="noopener noreferrer" target="_blank">
    Give them a try
   </a>
   , and let us know what you think. We’re always looking forward to your feedback, either through your usual AWS support contacts, or on the
   <a href="https://forums.aws.amazon.com/forum.jspa?forumID=285">
    AWS Forum
   </a>
   for SageMaker.
  </p>
  <a href="https://aws.amazon.com/developer/community/evangelists/julien-simon/">
   - Julien
  </a>
  <!-- '"` -->
 
  
  </body>
 </html>

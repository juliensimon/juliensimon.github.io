<!DOCTYPE html><html lang="en"><head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>CSLMs have arrived</title>
    <meta name="author" content="Julien Simon">
    <meta name="date" content="2025-08-16">
    <meta name="source" content="https://julsimon.medium.com/cslms-have-arrived-36ef90789cfb">
    <link rel="stylesheet" href="../../../css/minimal-blog-styles.css">
</head>
<body>
    <h1>CSLMs have arrived</h1>
    <div class="meta">
        <p><strong>Author:</strong> Julien Simon</p>
        <p><strong>Date:</strong> 2025-08-16</p>
        <p><strong>Source:</strong> <a href="https://julsimon.medium.com/cslms-have-arrived-36ef90789cfb">https://julsimon.medium.com/cslms-have-arrived-36ef90789cfb</a></p>
    </div>
    <div class="content">
        <article><p style="margin-bottom: 1.5em;"><a href="https://www.julien.org" style="color: #6366f1; text-decoration: none;">â† julien.org</a></p><div><div><span></span><section><div><div></div><div><div><div><div><h1 id="bd57">CSLMs have arrived</h1></div><div><h2 id="b94c">Crazy Small Language Models ğŸ˜‰</h2><div></div></div><p id="331a">CSLMs (Crazy Small Language Models) have arrived: you read it here first.</p><p id="8099">â€<a href="https://www.sciencedirect.com/science/article/pii/S2666827025000416" target="_blank"><em>D3: A Small Language Model for Drug-Drug Interaction prediction and comparison with Large Language Models</em></a>â€ introduces a 70 MILLION (yes, six zeros, not nine) Llama-like model that is on par with Llama-3.1 70B for drug interaction prediction. It was trained and fine-tuned from scratch in 2.5 hours on a single A100.</p><figure><div><div><img alt="" height="400" src="image02.webp" width="700"></div></div></figure><p id="cd8b">This research is yet another proof that itâ€™s reasonably easy to build excellent â€” and even SOTA â€” models at a tiny fraction of the time, cost, and energy required by larger models. All it takes is a well-defined business problem, some good quality data, and a mind immune to LLM marketing bullshit.</p><p id="17b1">Hereâ€™s <a href="https://huggingface.co/blog/large-language-models" target="_blank">what I wrote</a> in October 2021 as I joined Hugging Face:<br>â€œ<em>Large language model size has been increasing 10x every year for the last few years. This is starting to look like another Mooreâ€™s Law. Weâ€™ve been there before, and we should know that this road leads to diminishing returns, higher cost, more complexity, and new risks. Exponentials tend not to end well. Remember Meltdown and Spectre? Do we want to find out what that looks like for AI? Instead of chasing trillion-parameter models (place your bets), wouldnâ€™t we all be better off if we built practical and efficient solutions that all developers could use to solve real-world problems?</em>â€</p><p id="3e9b">Those of you who paid attention instead of spending silly money on prompt engineering bootcamps, OpenAI PoCs, and GPU clusters are quite likely to be in a good spot right now ğŸ˜‰</p><p id="f890">More projects like this one, please. Share them, tag me, and Iâ€™ll be happy to promote them. If you need help building these, please ping me and letâ€™s discuss how Arcee AI can assist.</p></div></div></div></div></section></div></div></article>
    </div>

</body></html>
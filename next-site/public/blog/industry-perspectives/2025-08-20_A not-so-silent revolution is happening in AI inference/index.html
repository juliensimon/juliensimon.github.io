<!DOCTYPE html><html lang="en"><head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>A not-so-silent revolution is happening in AI inference</title>
    <meta name="author" content="Julien Simon">
    <meta name="date" content="2025-08-20">
    <meta name="source" content="https://julsimon.medium.com/a-not-so-silent-revolution-is-happening-in-ai-inference-936ef4b9ba88">
    <link rel="stylesheet" href="../../../css/minimal-blog-styles.css">
</head>
<body>
    <h1>A not-so-silent revolution is happening in AI inference</h1>
    <div class="meta">
        <p><strong>Author:</strong> Julien Simon</p>
        <p><strong>Date:</strong> 2025-08-20</p>
        <p><strong>Source:</strong> <a href="https://julsimon.medium.com/a-not-so-silent-revolution-is-happening-in-ai-inference-936ef4b9ba88">https://julsimon.medium.com/a-not-so-silent-revolution-is-happening-in-ai-inference-936ef4b9ba88</a></p>
    </div>
    <div class="content">
        <article><p style="margin-bottom: 1.5em;"><a href="https://www.julien.org" style="color: #6366f1; text-decoration: none;">â† julien.org</a></p><div><div><span></span><section><div><div></div><div><div><div><div><h1 id="3fd9">A not-so-silent revolution is happening in AI inference</h1><div></div></div><p id="6ebe">The combination of smaller yet more efficient language models, advances in open-source inference frameworks, and hardware acceleration is enabling an unprecedented pace of innovation for CPU inference.</p><p id="a12f">A little over a month ago, we published a blog post sharing numbers for <a href="https://huggingface.co/arcee-ai/AFM-4.5B" target="_blank">AFM-4.5B</a> inference on Intel Corporation, Arm, and Qualcomm (see: â€œ<a href="https://www.arcee.ai/blog/is-running-language-models-on-cpu-really-viable" target="_blank">Is Running Language Models on CPU Really Viable?</a>â€).</p><p id="a1fc">I reran the Intel and Arm benchmarks in the same configuration, and all numbers have improved across the board, with some increases of up to 50%. Youâ€™ll find numbers at the end of the post.</p><p id="0f32">Same model. Same chips. What happened? Llama.cpp is on fire, thatâ€™s what. New features, such as <a href="https://github.com/ggml-org/llama.cpp/pull/14363" target="_blank">splitting the KV cache across sequence decoding</a>, are delivering double-digit gains overnight.</p><p id="5add">Takeaways:</p><p id="55f6">â¡ï¸ If youâ€™re been using GPU inference indiscriminately, itâ€™s time to reconsider. Your existing CPU servers may be an extremely cost-effective option, and they can also run your app!</p><p id="0425">â¡ï¸ If youâ€™re not rebuilding llama.cpp every day, youâ€™re doing it wrong ğŸ˜‚</p><p id="58b2">â¡ï¸ We used to pick between â€˜fast but less imprecise 4-bitâ€™ vs â€˜slow but more precise 8-bitâ€™. Thatâ€™s not so true anymore. 8-bit models are now fast enough (whatever that means to you) for many use cases.</p><p id="9170">â¡ï¸ The speedup on larger batch sizes definitely invalidates my long-standing advice of â€œCPU inference only really makes sense at batch size 1â€. Now, I would consider larger batch sizes, especially for non-interactive workloads, and experiment to find the sweet spot between thread count, latency, and throughput.</p><p id="3d01">What truly puts a smile on my face is that these arenâ€™t even the bleeding-edge CPUs. Thereâ€™s much more speed coming.</p><p id="f739">And this story ends on laptops and devices for minimal latency, maximum cost optimization, and full privacy. Many of us know that already ğŸ˜€</p><figure><div><img alt="" height="591" src="image01.webp" width="486"></div></figure><figure><div><img alt="" height="611" src="image02.webp" width="479"></div></figure></div></div></div></div></section></div></div></article>
    </div>

</body></html>
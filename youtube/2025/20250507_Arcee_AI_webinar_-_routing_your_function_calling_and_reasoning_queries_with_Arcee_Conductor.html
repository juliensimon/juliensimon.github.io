<!DOCTYPE html>

<html lang="en">
<head>
<meta content="Arcee AI webinar   routing your function calling and reasoning queries with Arcee Conductor - In this video, we show you how Arcee Conductor (https://www.arcee.ai/product/arcee-conductor) can now automatically route each function calling or reasoning que..." name="description"/><meta content="Arcee AI webinar   routing your function calling and reasoning queries with Arcee Conductor - Julien Simon" property="og:title"/><meta content="Arcee AI webinar   routing your function calling and reasoning queries with Arcee Conductor - In this video, we show you how Arcee Conductor (https://www.arcee.ai/product/arcee-conductor) can now automatically route each function calling or reasoning que..." property="og:description"/><meta content="https://www.julien.org/youtube/2025/20250507_Arcee_AI_webinar_-_routing_your_function_calling_and_reasoning_queries_with_Arcee_Conductor.html" property="og:url"/><meta content="video" property="og:type"/><meta content="summary_large_image" name="twitter:card"/><meta content="Arcee AI webinar   routing your function calling and reasoning queries with Arcee Conductor - Julien Simon" name="twitter:title"/><meta content="Arcee AI webinar   routing your function calling and reasoning queries with Arcee Conductor - In this video, we show you how Arcee Conductor (https://www.arcee.ai/product/arcee-conductor) can now automatically route each function calling or reasoning que..." name="twitter:description"/><link href="https://www.julien.org/youtube/2025/20250507_Arcee_AI_webinar_-_routing_your_function_calling_and_reasoning_queries_with_Arcee_Conductor.html" rel="canonical"/><meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<title>Arcee AI webinar   routing your function calling and reasoning queries with Arcee Conductor - Julien Simon</title>
<style>
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
            line-height: 1.6;
            margin: 0;
            padding: 20px;
            background-color: #f5f5f5;
            color: #333;
        }
        .container {
            max-width: 800px;
            margin: 0 auto;
            background: white;
            padding: 30px;
            border-radius: 10px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
        }
        h1 {
            color: #2c3e50;
            margin-bottom: 10px;
            font-size: 2.2em;
        }
        .date {
            color: #7f8c8d;
            font-size: 1.1em;
            margin-bottom: 30px;
            font-weight: 500;
        }
        .video-container {
            position: relative;
            width: 100%;
            height: 0;
            padding-bottom: 56.25%; /* 16:9 aspect ratio */
            margin-bottom: 30px;
        }
        .video-container iframe {
            position: absolute;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
            border: none;
            border-radius: 8px;
        }
        .description {
            background: #f8f9fa;
            padding: 20px;
            border-radius: 8px;
            margin-bottom: 30px;
            white-space: pre-wrap;
            font-size: 1em;
        }
        .description a {
            color: #3498db;
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s;
        }
        .description a:hover {
            color: #2980b9;
            text-decoration: underline;
        }
        .transcript {
            background: #f8f9fa;
            padding: 20px;
            border-radius: 8px;
            margin-bottom: 30px;
            white-space: pre-wrap;
            font-size: 1em;
        }
        .transcript h2 {
            color: #2c3e50;
            margin-top: 0;
            margin-bottom: 15px;
            font-size: 1.5em;
        }
        .tags {
            margin-bottom: 30px;
        }
        .tags h2 {
            color: #2c3e50;
            margin-bottom: 15px;
            font-size: 1.5em;
        }
        .tag {
            display: inline-block;
            background: #3498db;
            color: white;
            padding: 6px 12px;
            margin: 4px;
            border-radius: 20px;
            font-size: 0.9em;
            font-weight: 500;
        }
        .links {
            display: flex;
            gap: 20px;
            flex-wrap: wrap;
        }
        .link {
            display: inline-block;
            padding: 12px 24px;
            background: #3498db;
            color: white;
            text-decoration: none;
            border-radius: 6px;
            font-weight: 500;
            transition: background-color 0.3s;
        }
        .link:hover {
            background: #2980b9;
        }
        .link.youtube {
            background: #e74c3c;
        }
        .link.youtube:hover {
            background: #c0392b;
        }
        @media (max-width: 600px) {
            .container {
                padding: 20px;
                margin: 10px;
            }
            h1 {
                font-size: 1.8em;
            }
            .links {
                flex-direction: column;
            }
        }
    </style>
</head>
<body>
<div class="container">
<h1>Arcee AI webinar   routing your function calling and reasoning queries with Arcee Conductor</h1>
<div class="date">May 07, 2025</div>
<div class="video-container">
<iframe allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="" src="https://www.youtube.com/embed/JfyvMxMPfJY">
</iframe>
</div>
<div class="description">In this video, we show you how Arcee Conductor (<a href="https://www.arcee.ai/product/arcee-conductor)" rel="noopener noreferrer" target="_blank">https://www.arcee.ai/product/arcee-conductor)</a> can now automatically route each function calling or reasoning query to the best SLM/LLM, efficiently delivering precise and cost-effective results for any task.

Sign up for Conductor and get $20 of free inference credits!

If you’d like to understand how Arcee AI can help your organization build scalable and cost-efficient AI solutions, don't hesitate to contact sales@arcee.ai or book a demo at <a href="https://www.arcee.ai/book-a-demo." rel="noopener noreferrer" target="_blank">https://www.arcee.ai/book-a-demo.</a> 

⭐️⭐️⭐️ Don't forget to subscribe to be notified of future videos. You can also follow me on Medium at <a href="https://julsimon.medium.com" rel="noopener noreferrer" target="_blank">https://julsimon.medium.com</a> or Substack at <a href="https://julsimon.substack.com." rel="noopener noreferrer" target="_blank">https://julsimon.substack.com.</a> ⭐️⭐️⭐️

* Arcee Conductor: <a href="https://conductor.arcee.ai" rel="noopener noreferrer" target="_blank">https://conductor.arcee.ai</a>
* Arcee Conductor product page: <a href="https://www.arcee.ai/product/arcee-conductor" rel="noopener noreferrer" target="_blank">https://www.arcee.ai/product/arcee-conductor</a></div>
<div class="transcript">
<h2>Transcript</h2>
            We should be live. Good morning, good afternoon, or good evening, depending on where you are. We're super happy to be back with another Arcee webinar. It looks like we have the Arcee Avengers again today. There's something happening there. The four of us, I don't know. Are we going to save the world? Maybe the world of AI, who knows? Today, we're back with Arcee Conductor. Last time we met, we talked about Conductor and explained what Conductor was—our new inference platform that automatically picks the best SLM and the best LLM for each query using model routing. Lucas and Fernando explained all about that, and Andrew did some demos. Today, we're back to talk about model routing in Conductor, but we have new features. Before we dive into the features, let's do a quick overview, and then we'll get into what we have to say today. Andrew, want to start?

Absolutely. For those of you I haven't met, nice to meet you. My name is Andrew Walker. I lead our field engineering team here at Arcee and have the pleasure of working across all of our product suites. We directly work with customers, solving their use cases, and taking them from early pre-sales to actual production implementation. This gives us a good visibility into what actually works and what customers can achieve with this technology, which has been very cool, especially with these new Conductor features. I'm excited to share them with you. And that's why you're going to do the demos today.

Absolutely. Lucas, please.

Hey, my name is Lucas. I run the labs team with Fernando, which is mostly our research organization, but I've been more involved on the product side as we bring these research innovations to customers. We're figuring out how to easily bring meaningful value to users in this AI ecosystem. There's a lot of value you can get, like summarizing emails and writing code, but how do you get something meaningful for the average end user? It's been an exciting challenge. Fernando, my research partner and co-lead, will fill you in. Fernando?

Great. I'm Fernando, and I co-lead the labs with Lucas. In a few words, I'm the math nerd of the team. I try to turn papers into algorithms and bring some innovation, thinking out of the box with our brilliant team. Every team needs one. And you're still in Brazil, right?

Yes, I'm still in Brazil. Okay. So I'm still jealous. We have a good team. Good morning to everyone joining. Why not post a little message telling us where you are in the world? We have people from the US, Fernando in Brazil, and I'm in Paris today. So, let's share your locations and say hi.

What are we going to cover today? We're talking about Conductor and two recent features we added: function calling and reasoning. With model routing, we're not just talking about which models have supported these features for a while, but how these two things plus model routing work together. As a quick intro, what do we mean by function calling and reasoning? Previously, we showed you playing Q&amp;A with Conductor—asking the model a question and getting an answer, which is text generation as we know it. Function calling is still text generation, but instead of asking the model to answer a question, we pass a list of tools or functions available to the model to answer the question. The model answers by suggesting which tool or function to call to get the answer. We then call it, get a result, and often pass the result back to a language model to write a better story. In a nutshell, function calling is about relying on the model to tell us which tool to use to access the knowledge, not just relying on the model for knowledge. There are plenty of models for this, which is why we need model routing to pick the best and most cost-efficient one.

Reasoning is another approach to text generation where we don't just get the answer from the model but also see how the model progresses through the answer. The model is trained to elaborate on its reasoning, sometimes disagreeing with itself and exploring other paths. Eventually, the answer converges to something, and we get to see the reasoning process. This is good reading and gives deeper insights into your question, helping you understand the problem better. There are plenty of reasoning models, which is why we need model routing. Hi, everyone. We have folks from Europe, which is great. Let's dive into function calling first. I'll keep an eye on the clock to make sure we cover both. Lucas, take us deeper into the function calling rabbit hole. What's the point? What's your definition? What's complicated about it?

Function calling started to be explored around the time ChatGPT came out, when companies began training models to act like assistants. You can train them to call tools by giving them a string of text that, instead of responding to the user, triggers an action. The point of function calling was exciting because it allowed models to utilize data and take actions outside their training. Over time, models have become really good at function calling. If you've used ChatGPT or similar tools, you might have seen it searching the internet, using a Python interpreter to do calculations, or making graphs. There are nuances: tool use versus function calling. Function calling is often stricter, forcing the model to do a specific action, while tooling gives the model a list of tools and descriptions to decide when to use them. We support both, enabling more agentic tasks. Fernando, without going into math, can you tell us a bit about training these models and the datasets required? Querying for basic Q&amp;A is straightforward, but function calling and code are different. Can you shed some light on this?

Sure. When enhancing our routing models for coding tasks, we realized that patterns in the dataset while training for the best coding model emerged as an emerging capability. We observed that our routing scheme was powerful for function calling. When you provide the function definition in the system prompt and train the model to recognize coding patterns, it's similar to recognizing function signatures. This joint knowledge allows us to have a strong model that can recognize complex signatures and route them to more complex models, and simpler signatures to simpler models. That's the whole point.

Before we go into the demo, Andrew, get ready. Lucas, can you give our listeners some pointers on the tools they can expect to trigger and what languages work better for function calling with Arcee, SLMs, and models in general? What's a safe place to start?

There's the OpenAI API tool use format, a JSON schema you fill out and deliver to the model. If you're doing function calling with our auto tools mode, use the OpenAI tool call format. This standardizes it so Claude, our models, and OpenAI models all use the same format. You can also do function calling with specific prompting, but the OpenAI spec is the easiest way. I'll post the link in the chat while Andrew is setting up the demo. Andrew, ready?

Absolutely. The demo will show the OpenAI schema for function calling, so you'll get an inside look. I'll share my screen and jump into some code. Can everyone see my screen? Are you all looking at a Python notebook? Not yet, but maybe it's me. No, I can't see. Fernando?

No, I can't see. Let's try a full screen share. Hopefully, that will help. How about now? What happens if I try... Nora, you may need to allow it to go. It might pop up in the green room, and you have to click it. Let's give it a minute. StreamYard is a bit weird, and we're streaming to LinkedIn and YouTube. Someone is seeing your screen, just not us. The good news is we're much better at AI and training models than operating live streams. Let me try this. Oh, there we go. Yes. Thank you, Nora. Shout out to our incredible team. We're so busy doing AI, we need to build up our streaming skills. Now, Andrew, please continue.

Let's jump into it. We'll show how to do function calling that utilizes the model router within Conductor using the OpenAI schema. First, we'll install what we need, like the Yahoo Finance API for our demo. Conductor uses an OpenAI-compatible endpoint, so we'll get that installed. We'll set everything up by importing the necessary libraries and setting up our clients. For connecting to Conductor, specify a new base URL and your API key from the Conductor UI. For function calling, define the functions you want to execute. These are simple functions, but they can be anything. The important thing is knowing the function names and required arguments. I put the link in the chat. It's really just the function name, parameter name, and that's about it. It's user-friendly.

We have three functions for this demo: getting the stock price of a specific company, getting the CEO name of a company, and getting a company summary. Think of these as functions your company would call for a specific project. They could be APIs or anything. Before we get to what the model returns, let's look at what happens when you call the function. For example, for the stock price, we pull the latest stock price for Amazon. For the CEO name, we pull the CEO of Ford. For the company summary, we get a summary for Verizon. When we provide the models with the relevant information, we define the tools they have access to. The function names should be familiar: get stock price, get CEO name, and so on. The description and parameters are important. The description helps the model decide which function to call based on the prompt. The parameters specify what is needed to execute the function. For get stock price, we need the company name and stock symbol, both required. We provide this for all three functions.

We define two functions to show what's happening with each model. The first uses auto tools for calling the function, utilizing the auto tool functionality within Conductor for model routing. We extract the specific function called from the model. The model itself is not calling the Yahoo API; it tells you which function to call. You extract that from the answer and run the Python call in the notebook. The model isn't just returning text; it's not calling anything. There's no magic. One lucky thing for you is that you now know more about function calling than 99% of people. The model returns the function to call, the program executes the function, and the result gets passed back. That's a key distinction to remember.

The last function we define is where we call the first model for the function call and pass the result to another model. We can utilize the initial auto mode of Conductor, which routes between different text generation models. You can use the model routing capabilities of Conductor in multiple areas. We define a quick function for printing results. Now, we have different prompts. Let's ask, "What's the latest price of McDonald's?" We run the initial tool call. The model gets the tool definitions and selects which tool to call based on the prompt. We asked for the latest price of McDonald's, and it correctly identified the get stock price function, returning the arguments needed. On the application side, we execute the function and get the response. Notice, you don't get a long answer from the model. This first step is just calling the function. The second step is passing the result back to a second model call, leveraging the model's built-in knowledge to give a better answer. We used the auto mode here, and it called our smallest and most cost-effective model, Blitz. McDonald's is a well-known company, so every model can give a better answer, not just a dry API response. With a bit of prompting, you can add URLs and other details.

Do you want to try another prompt and then switch to reasoning? What about the population of the capital of New Zealand?

That's interesting because this prompt is not relevant to any of the tools we provided. One great thing with function calling is that if the relevant tool isn't required, you don't need to execute it. Here, no tool was called, and we get a direct answer from the model. This is useful because users shouldn't know if a query will be tool-answered or model-answered. The two-step process is cool. We have a question from Arvind: How does the model decide whether to use its pre-training knowledge or choose a new function to obtain the info?

The main factor is that it didn't have a relevant tool for the question, like a get population tool. When you add function calling to a model, if there's a relevant tool and it's trained well, it will almost always opt to use the tool instead of its internal knowledge. We assume that if developers give the model access to a tool, they want it to be used. Even if it could answer with its own knowledge, it leans towards the tool because there's a reason it was given that tool. That's the short answer.

Andrew showed the tool description with sample questions. If you give some examples, the accuracy jumps, and you get much better success. Shall we move to reasoning?

Sure. Any last thoughts on function calling before we switch? Fernando, anything to add?

We are researching how to teach the model when to use tools, not just deterministically. For multi-hop questions, you must link answers from one tool to another. This blending step is a hot topic. You can trigger multiple tools per query, and the fun stuff happens when they chain. That's when models become truly agentic. For the next webinar, we'll explore more.

Let's switch to reasoning. I gave a quick explanation, but Lucas, tell us the smart one.

Reasoning is about training models using reinforcement learning to think before answering. When I say "think," it means the model does a bunch of thoughts before responding. For example, to answer a user request, it needs to do X, Y, and Z, cross-reference, and maybe correct itself. While it takes longer, it has been shown to drastically improve, especially on coding, mathematics, and science-related queries. We want to ensure that if you just want a general model response, you get that. If you want to ensure you're routing models that can do well with tool use, we have auto tool. If you want a longer, more expensive response, we route to a reasoning model. We want to be sure you get those longer responses when you want them.

Can I play devil's advocate and double-click on reasoning? We're still talking about generating the next token, right? What's different this time? Is it just clever training and reinforcement learning, or are we closer to mimicking how a human would answer the question?

The whole reason this works is that you have a model trained on a vast amount of text. Newer models like LLaMA and Qwen are trained on 40 trillion tokens, far more than the internet. The question is, if these models have all this knowledge, why can't they solve new problems? Mathematically, whenever it's looking at the next token, it only gets access to a small number of the total tokens and information it learned during training. By giving it more time to think, you increase the chances that it finds tokens and pieces of knowledge it otherwise wouldn't if asked to do it in real-time. It's exploring its knowledge more.

Fernando, before Andrew shows us some prompts, what is challenging or specific about SLMs for reasoning?

The most challenging part of making a smaller reasoner is the narrower bottleneck. The best scenario for reasoning is not spitting out a token every time because each token collapses into a single path. The beauty of the search space is that when you spit out a token, you can recover from a wrong sample with more compute time. For example, models start with "mmm, wait, but I'm not sure" to increase the search space and recover from catastrophic faults. In larger models, a lot of processing happens internally, making it easier. In smaller models, we compensate with longer streams of time. The trade-off is that smaller models confabulate more to reach an answer, but longer chains of thought increase the likelihood of spitting out something meaningless. The optimal stopping point for smaller models is tricky. Specializing smaller models in specific tasks helps mitigate this.

Fernando, can you clarify the difference between reasoning models, O3, Sonnet, Gemini 2.5 Pro operations, and agentic applications using React?

React is a simplistic pattern matching algorithm or prompt style to teach the model how to link a sequence of actions given a prompt. A pure reasoning model focuses on expanding test-time compute to fix intermediate wrong steps and move forward. Both can be combined. You can have a reasoning model prompted to work following the React pattern. This could be a good topic for another webinar or blog post.

Andrew, show us some reasoning prompts.

Sure. Sharing the screen again. This time it's going to work. We're getting better. For those who have used Conductor before, the chat interface is similar. You still have all our models, and now you'll see two additional router modes: reasoning and tools. We'll start with reasoning. Arcee Maestro is our 32 billion parameter SLM that excels at reasoning. I'll give it a common prompt where executives, CFOs, and high-ups want to provide a lot of data and ask a recommendation question. Here, we provide target data, including prices, EBITDA, Synergy, and criteria the board of directors is interested in. It's asking to state the name of the target that meets all the criteria. The model will reason through the problem, generating a lot more text than a regular language model. The expected answer was Beta LTD, and it got to that answer. Notice the number of tokens generated and the cost, which is very low compared to other reasoning models. You get reasoning capabilities at a much cheaper cost using Maestro.

Now, let's look at auto reasoning mode. We'll start with a simple prompt but one where we might want to use a reasoning model. We're asking for productivity tips with specific requirements, like flying midday or afternoon and wanting to be productive on the plane. The model generates thoughts and text. With regular auto reasoning in Conductor, you see which model it was sent to, the explanation, and the specific type of classification. Here, it was an analytical task in the travel and transportation domain with a complexity of one. Since it's a simple prompt, it gets routed to the smallest model, Maestro, reducing cost and improving efficiency.

If you want to use this with the API, you can do it the same way Andrew showed, just using auto reasoning instead of auto tool. We just published a blog post on this, which I shared in the chat. We have just a minute left. Lucas, how does all of this relate to agentic workflows?

Conductor is built for building agents, giving you the best model for the right task. We handle agent orchestration, and you handle building the app the agent interfaces with. Adding reasoning and tool use expands this further. We're working on more to automate your automation. We're very excited.

Fernando, anything to add?

No, perfect explanation.

Andrew, anything to add?

I think they covered it. If Fernando says there's nothing left, there's nothing. It would be dangerous to explore more and get lost.

Thank you so much. Hopefully, we're still alive. Thank you, everyone, for watching. Check out our website at arcee.ai for details, the blogs at blog.arcee.ai, and conductor.arcee.ai. If you sign up now, you'll get $20 in free inference credits, and $20 with SLMs will take you a long way. You can run the notebooks and more. That's it for today. Thank you, my Avengers: Lucas, not on fire, Fernando in Brazil, and Andrew for the demos. Thanks to Nora and our marketing team for setting this up and showing us how to share screens. We'll see you soon with more. Until next time, my friends. Keep rocking. Bye.
        </div>
<div class="tags">
<h2>Tags</h2>
<span class="tag">AI Model Routing</span><span class="tag">Function Calling in AI</span><span class="tag">Reasoning Models AI</span>
</div>
<div class="links"><a class="link" href="../../../youtube.html">← Back to YouTube Overview</a></div>
</div>
</body>
</html>
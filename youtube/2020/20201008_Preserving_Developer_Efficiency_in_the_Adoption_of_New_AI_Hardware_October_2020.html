<!DOCTYPE html>

<html lang="en">
<head>
<meta content="Preserving Developer Efficiency in the Adoption of New AI Hardware October 2020 - Talk at AI Hardware Summit 2020
https://aihardwaresummit.com/events/ai-hardware-summit-2020

⭐️⭐️⭐️ Don't forget to subscribe to be notified of future videos ⭐️..." name="description"/><meta content="Preserving Developer Efficiency in the Adoption of New AI Hardware October 2020 - Julien Simon" property="og:title"/><meta content="Preserving Developer Efficiency in the Adoption of New AI Hardware October 2020 - Talk at AI Hardware Summit 2020
https://aihardwaresummit.com/events/ai-hardware-summit-2020

⭐️⭐️⭐️ Don't forget to subscribe to be notified of future videos ⭐️..." property="og:description"/><meta content="https://www.julien.org/youtube/2020/20201008_Preserving_Developer_Efficiency_in_the_Adoption_of_New_AI_Hardware_October_2020.html" property="og:url"/><meta content="video" property="og:type"/><meta content="summary_large_image" name="twitter:card"/><meta content="Preserving Developer Efficiency in the Adoption of New AI Hardware October 2020 - Julien Simon" name="twitter:title"/><meta content="Preserving Developer Efficiency in the Adoption of New AI Hardware October 2020 - Talk at AI Hardware Summit 2020
https://aihardwaresummit.com/events/ai-hardware-summit-2020

⭐️⭐️⭐️ Don't forget to subscribe to be notified of future videos ⭐️..." name="twitter:description"/><link href="https://www.julien.org/youtube/2020/20201008_Preserving_Developer_Efficiency_in_the_Adoption_of_New_AI_Hardware_October_2020.html" rel="canonical"/><meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<title>Preserving Developer Efficiency in the Adoption of New AI Hardware October 2020 - Julien Simon</title>
<style>
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
            line-height: 1.6;
            margin: 0;
            padding: 20px;
            background-color: #f5f5f5;
            color: #333;
        }
        .container {
            max-width: 800px;
            margin: 0 auto;
            background: white;
            padding: 30px;
            border-radius: 10px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
        }
        h1 {
            color: #2c3e50;
            margin-bottom: 10px;
            font-size: 2.2em;
        }
        .date {
            color: #7f8c8d;
            font-size: 1.1em;
            margin-bottom: 30px;
            font-weight: 500;
        }
        .video-container {
            position: relative;
            width: 100%;
            height: 0;
            padding-bottom: 56.25%; /* 16:9 aspect ratio */
            margin-bottom: 30px;
        }
        .video-container iframe {
            position: absolute;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
            border: none;
            border-radius: 8px;
        }
        .description {
            background: #f8f9fa;
            padding: 20px;
            border-radius: 8px;
            margin-bottom: 30px;
            white-space: pre-wrap;
            font-size: 1em;
        }
        .description a {
            color: #3498db;
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s;
        }
        .description a:hover {
            color: #2980b9;
            text-decoration: underline;
        }
        .transcript {
            background: #f8f9fa;
            padding: 20px;
            border-radius: 8px;
            margin-bottom: 30px;
            white-space: pre-wrap;
            font-size: 1em;
        }
        .transcript h2 {
            color: #2c3e50;
            margin-top: 0;
            margin-bottom: 15px;
            font-size: 1.5em;
        }
        .tags {
            margin-bottom: 30px;
        }
        .tags h2 {
            color: #2c3e50;
            margin-bottom: 15px;
            font-size: 1.5em;
        }
        .tag {
            display: inline-block;
            background: #3498db;
            color: white;
            padding: 6px 12px;
            margin: 4px;
            border-radius: 20px;
            font-size: 0.9em;
            font-weight: 500;
        }
        .links {
            display: flex;
            gap: 20px;
            flex-wrap: wrap;
        }
        .link {
            display: inline-block;
            padding: 12px 24px;
            background: #3498db;
            color: white;
            text-decoration: none;
            border-radius: 6px;
            font-weight: 500;
            transition: background-color 0.3s;
        }
        .link:hover {
            background: #2980b9;
        }
        .link.youtube {
            background: #e74c3c;
        }
        .link.youtube:hover {
            background: #c0392b;
        }
        @media (max-width: 600px) {
            .container {
                padding: 20px;
                margin: 10px;
            }
            h1 {
                font-size: 1.8em;
            }
            .links {
                flex-direction: column;
            }
        }
    </style>
</head>
<body>
<div class="container">
<h1>Preserving Developer Efficiency in the Adoption of New AI Hardware October 2020</h1>
<div class="date">October 08, 2020</div>
<div class="video-container">
<iframe allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="" src="https://www.youtube.com/embed/VANVOMvTzJA">
</iframe>
</div>
<div class="description">Talk at AI Hardware Summit 2020
<a href="https://aihardwaresummit.com/events/ai-hardware-summit-2020" rel="noopener noreferrer" target="_blank">https://aihardwaresummit.com/events/ai-hardware-summit-2020</a>

⭐️⭐️⭐️ Don't forget to subscribe to be notified of future videos ⭐️⭐️⭐️

In this talk, I explain the role played by hardware in AI innovation. Then, I share my views on how new hardware can be successfully introduced and adopted by the developer community. Finally, I take three examples from the AWS portfolio: FPGA instances, model compilation with Amazon SageMaker Neo, and inference at scale with the AWS Inferentia custom chip.</div>
<div class="transcript">
<h2>Transcript</h2>
            Good morning, everybody. My name is Julien and I work for AWS as a tech evangelist focusing on AI and machine learning. In this session, I would like to discuss how hardware innovation is critical to machine learning and how we can all help developers adopt new AI chips. As we all know, machine learning is eating the world. At AWS, we see customers from literally every vertical, every size and shape—startups, enterprises, nonprofits, government, education—adopting machine learning to build better services, better products, and improve the user experience for their customers. So let's try to step back and see how this actually happened. I don't think this happened by accident. In fact, machine learning and deep learning, a really popular subset of machine learning, have happened because several planets aligned.

The first one is the availability of large digital data sets. As you probably know, machine learning and deep learning are not really new. Neural networks have been around since the late 40s. So why now? Well, in the good old days, it was really difficult to collect, store, and share digital data. Until quite recently, it was particularly expensive to do that as well. In recent years, this has completely changed. One obvious reason is the internet and its mountains of user-generated content—email posts, product reviews, hotel reviews, trip reviews, images, videos, and much more. It's quite easy to go and either scrape the web for those or use your own user-generated data on your own websites. And on top of that, everything is either web or mobile these days. So customers can use transactions that now happen on the web and on mobile apps. This tends to generate quite a lot of data, whether it's search queries, page views, product views, clicks, purchases, etc. All that data can easily be stored and processed later on. And of course, we have open datasets which can be academic datasets or just datasets put together by the community. Datasets like ImageNet, which has been critical for computer vision, WMT, which is really important for machine translation, and of course medical datasets and others are now easily accessible on the web.

The second reason is the availability of scalable and cost-effective infrastructure. As you know, storing data, training models, deploying models requires quite a lot of infrastructure. For decades, it was difficult to get the amount of storage and compute power required to train large models. If you could do that, it was probably an expensive proposition, making it inaccessible to smaller companies and individual researchers. Commodity hardware has changed this—servers, storage, networking, all that stuff is now much, much cheaper. Plus, cloud computing is now much cheaper, making it all accessible on demand, just paying as you go, very easy to manage using managed services, and without any compromise on security, high availability, etc. So it's never been easier to fire up your training cluster or your deployment cluster and just pay for what you use, with no upfront costs. In fact, this is backed by research from Nucleus published last year that says 96% of deep learning workloads are actually running in the cloud. And out of those 96%, 89% is running on AWS. So I think this goes to show how cloud has completely changed the game for machine learning and deep learning and how it's pretty important these days.

The third reason why machine learning and deep learning exploded is the availability of open-source tools. Going back to the pioneers like Torch and Theano in the early 2000s, and then Hadoop and Spark, which are not machine learning tools per se, but are big data tools that are really critical to building machine learning workflows in a lot of cases. And then, of course, all the popular libraries that we use every day—TensorFlow, Keras, MXNet, PyTorch, Chainer, and more—have completely simplified the process of building, training, and deploying models. Last but not least, hardware acceleration has made it possible to train large, complex models on really large datasets. NVIDIA GPUs and CUDA have been instrumental, but CPU architecture has made a lot of progress as well. Intel AVX helps speed up machine learning on CPU platforms. And then, of course, the slightly more exotic platforms like FPGA and ASICs have also popped up in recent years.

To prove my point, let's try and put all that history together. Let's start our trip in 1999, when NVIDIA launched their first GPU. At the time, they were used for 3D gaming. Six years later, the first paper showing how to use GPUs to accelerate machine learning was published. Six years is a long time, but it was a breakthrough. A little bit later, AWS was launched. Services like Amazon EC2, Amazon S3 went live. For the first time, cloud-based compute and storage was available to customers. A few months later, CUDA 1.0 was released. This is a major milestone because it made it so much easier to write accelerated code with GPUs using high-level languages. Just two years later, another breakthrough happened where GPUs were used to accelerate deep neural networks. A few months later, Theano 0.3 was the first open-source library to support GPU training with CUDA. For the first time, you could just grab an open-source library and pretty easily train machine learning and deep learning code on GPUs without having to write any GPU-specific code. This was a breakthrough. At about the same time, AWS launched its first GPU instances, the first cloud-based GPU instances. Pretty much immediately after that, we saw the explosion of machine learning and deep learning and speed. Specifically, deep learning started to achieve superhuman performance on computer vision tasks like ImageNet and also on handwriting recognition and other tasks. Imagine that between the release of Theano with GPU support and superhuman performance, there's only one year. Compare that to the six years it took for the first machine learning algorithm to actually leverage GPUs. So the pace is definitely accelerating thanks to open-source tools, thanks to accelerated computing, and the combination of everything inside cloud platforms.

The train kept rolling. In 2015 and 2016, a string of amazing libraries were released—TensorFlow, Keras, PyTorch, MXNet, and others—giving developers more options, more language support, etc., to build machine learning and deep learning models using open-source tools. We also launched the first FPGA-based instances in the cloud shortly after, and a little more than a year ago, we launched Inferentia, a custom chip designed for high throughput and low-cost inference. We'll talk about those a little more later in the session. So that's the story. There's a pretty clear relationship between accelerated computing, tooling, scalable, and cost-effective infrastructure in the cloud. All those things work together and help us build projects and build them faster, innovate faster. The situation today is really this: state-of-the-art models are published by researchers. They are implemented sometimes by the researchers themselves, sometimes by the community, literally reading from the paper, training them, and sharing them openly on GitHub or on the AWS Marketplace, which has a collection of hundreds of machine learning models that you can try. You can just go and grab those models and within minutes, deploy and test them on your infrastructure of choice on AWS, whether it's CPU, GPU, FPGA, Inferentia. And you can start testing them at minimal cost, figuring out if they work for you.

Of course, that's the story so far, but I'm sure new hardware platforms will come. So how do we help those platforms be successful with developers? What does it take for those platforms to be adopted by developers? So I think we can make the following observation. Hardware innovation is critical for machine learning performance. This point is well understood. However, most machine learning practitioners don't know much, if anything, about hardware. They're data scientists and software engineers. They generally don't care much for IT and infrastructure. Asking them to understand the finer points of chips is just asking for too much. They don't care for it. They don't want to see it. In fact, the hardware should ideally be completely transparent to them. It should be a parameter that they set and just enjoy the performance.

Developer-friendly tools are paramount for adoption. This point is probably the most important one. You need to invest a lot in your tools. Everyone out there is providing some sort of SDK that abstracts the hardware, and it could be a reasonably high level, but it's really not enough. You're asking too much. You're asking developers who are ML-focused and ML-obsessed to learn your APIs, and all of them will require some hardware knowledge. That's not going to work. The best way to go is that this SDK must work with the tools and languages that ML practitioners use. So either through direct integration, like CUDA was integrated into all the machine learning libraries I mentioned before, making it obvious and instantaneous to use GPUs for training—just set a parameter and you're done. Sometimes it's not possible, especially if you have edge-based platforms, so you will need some toolchain between model training and model deployment. But please make it as simple as possible and as easy as possible to use, even if you have no hardware knowledge. It should be just a couple of calls, a couple of APIs, and everything should be hidden away from developers. Otherwise, they won't adopt it.

And of course, cloud is everywhere. You saw that earlier stat. So you need to have a strategy to make your tools available in the cloud in the easiest way possible. All the above requires that you understand pretty well how people will use your tools—where they work, how they work, what the workflows are, etc. So don't assume that you know best. You need to engage with existing and potential users. You need to talk to them, collect feedback on the languages they use, the tools they use, the workflows they use, and act accordingly. Silicon companies are not the best place to understand the machine learning workflow. So please go and figure out what people want from you. Don't assume that you know. The ML community is really the ML communities, plural, because scientists, researchers, data scientists, ML engineers, MLOps all use different tools, different workflows, and have different goals sometimes. One size does not really fit all. So you need to have a modular approach that helps the widest range of people use your tools. It's great if you have fantastic tools for researchers, but if deploying models is a nightmare, MLOps will push back, and your adoption won't be as good. Understand who you're dealing with, who are your customers, who are your personas, and talk to all of them to understand how they work. The best way to do this is to go where those people are—go to meetups, which are online these days, but it doesn't matter. Write blog posts, get some feedback, get some comments on what you're working on, push some sample code to GitHub, show people how it's done, and help them get started. Maybe hire developer advocates who will know how to speak with those communities and get some feedback.

Let's look at three examples in recent AWS history. I'm not claiming we're doing everything right, but we have those mechanisms in place—collecting feedback, etc.—and hopefully, the way we've built those FPGA instances, SageMaker, Neo, and Inferentia will help you understand and point you in the right direction for developer adoption.

The first example is deploying FPGA instances in the cloud. In AWS, they're called F1 instances. They are basically regular servers with Xilinx FPGAs, from 1 to 8 FPGAs inside of them. You fire up the server just like you would fire up any EC2 server. You start from an Amazon Machine Image (AMI), which fires up, let's say, Ubuntu or Amazon Linux. And then using a collection of tools that we provide, you can load what we call Amazon FPGA images, so the binary file that runs on the FPGA. Those FPGAs can communicate with the application running on the server CPU and can also access external memory present on the server. So it's really FPGAs inside an EC2 instance. When it comes to FPGAs, people use well-known tools like Xilinx Vivado. What we've done is built a developer AMI. Everything is pre-installed in there, and you have a free license. So you just fire up the server, pay for the server, use that FPGA image, wait for a few minutes for the instance to come up, open Vivado, and get to work. No time wasted installing tools, no time wasted learning new tools if you're involved with Xilinx FPGAs; you're using Vivado already. The same familiar tools, and you can just get to work really quickly.

When it comes to actually working with the FPGA chip, we designed an FPGA SDK, which is available on that AMI and is also open source on GitHub. This is a good example of hardware innovation, packaging well-known tools that FPGA developers use, and building a high-level SDK that abstracts some of the complexity of the FPGA, making it possible to use OpenCL, C, C++ to write FPGA code. So even if you're not an FPGA expert, you can get the job done. You can run some examples from that GitHub repository and figure out what FPGA can do for you.

The second example I'd like to mention is a service called Amazon SageMaker Neo. You may have heard about Amazon SageMaker. It's a fully managed service for machine learning on AWS. It includes lots of different capabilities. The problem that Neo solves is optimizing models for hardware platforms. So you train, build, and train your model as usual. You use your favorite algorithm in TensorFlow, PyTorch, or one of the built-in algorithms in SageMaker. Then you can compile it using Neo. The compilation process in Neo is pretty complicated. First, we parse the model, put it into a common format. Then the tool will look at the graph and the tensors and try to optimize both based on the capabilities of the hardware platform we're targeting. Finally, the tool generates machine code using a low-level compiler. Machine learning practitioners don't want to hear anything about it. So the way it actually works is you call a single API called compile model. You set the instance family, so in this case, the Raspberry Pi. You define the shape, the input shape of the model. Here it's a computer vision model taking a three-channel 224 by 224 image. And finally, you define the library that was used to train the model, MXNet 1.51, and the location of the compiled model. That's all it takes. All the complexity is hidden inside.

When it comes to deploying, we actually use what we call the deep learning runtime, which is also open source. It's a really compact runtime that you can use to load a model and predict with it. There's complexity involved in how the model is loaded and on what actual hardware it's loaded, but we don't want to know. If we're deploying on AWS managed infrastructure, for example, with SageMaker, it's a one-liner called deploy, and you tell SageMaker what kind of EC2 instance you want to use for deployment. Everything is taken care of. The model will be loaded with the deep learning runtime, and you don't need to know the first thing about it. If you're deploying on an embedded platform, then you simply need to install the runtime there, import it, and just load your model and predict. So Neo is a good example of a very complex process—compiling machine learning models for different platforms—made super simple.

The last example I want to talk about is Inferentia. Inferentia is a custom chip designed by AWS for high throughput, low-cost predictions. We could talk about the hardware for hours. It's very interesting and fascinating. If you're a hardware person, this is great. But if you're a developer, you don't really care much. You just want to leverage it for high throughput and low-cost predictions. You do that by using an SDK called the Neuron SDK, which is available on GitHub. The Neuron SDK is a compiler that will compile a TensorFlow, MXNet, or PyTorch model that you train as usual. You will grab the compiled model and use the Neuron Runtime to execute it on an Inferentia instance. Of course, you have profiling tools as well. It supports the major frameworks, so you don't need to change the way you're working with your models and libraries. Train as usual and then use Neuron to compile and load models on Inferentia chips. It's just a couple of lines of code. We provide documentation and examples, and you can run our examples and get started in minutes, using the state-of-the-art Inferentia chip without knowing anything about hardware, which is really where we're trying to get.

So I hope those three examples show you some insight on how to build hardware platforms that developers can adopt to make their own projects more successful, more efficient, more cost-effective, and more scalable. It's all about development tools, it's all about open source, and it's all about trying to disrupt as little as possible the way your users are working, just inserting your technology into their existing workflows and showing them the benefits of that.

It's the end of the session already, unfortunately. I'll be happy to answer questions. Some resources to get you started: resources on F1 instances and FPGAs, resources on SageMaker and SageMaker Neo, resources on Inferentia, my blog, and my YouTube channel as well if you're interested in machine learning on AWS in general. The last thing I want to mention is if you've never heard about SageMaker, I really recommend that you take a look. It's a really popular service. We have tens of thousands of customers using it. I published a book about SageMaker just a few weeks ago, and you can go and grab it with a pretty sweet discount on Amazon and Packt until November 11th. So don't wait. This will get you started on machine learning on AWS and SageMaker. Thank you very much for inviting me today. I hope you learned a few things and you can ping me anytime in the future if you have questions. And until then, I wish you a very good day and I hope you enjoy the rest of the conference. Thank you very much.
        </div>
<div class="tags">
<h2>Tags</h2>
<span class="tag">MachineLearning</span><span class="tag">CloudComputing</span><span class="tag">HardwareInnovation</span><span class="tag">AWS</span><span class="tag">DeveloperTools</span>
</div>
<div class="links"><a class="link" href="../../youtube.html">← Back to YouTube Overview</a></div>
</div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Decouvrez l IA et le ML sur AWS pour les developpeurs   Startup Day Paris 2018</title>

<!-- Umami Analytics -->
<script defer src="https://cloud.umami.is/script.js" data-website-id="27550dad-d418-4f5d-ad1b-dab573da1020"></script>
<link rel="dns-prefetch" href="//cloud.umami.is">
    <style>
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
            line-height: 1.6;
            margin: 0;
            padding: 20px;
            background-color: #f5f5f5;
            color: #333;
        }
        .container {
            max-width: 800px;
            margin: 0 auto;
            background: white;
            padding: 30px;
            border-radius: 10px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
        }
        h1 {
            color: #2c3e50;
            margin-bottom: 10px;
            font-size: 2.2em;
        }
        .date {
            color: #7f8c8d;
            font-size: 1.1em;
            margin-bottom: 30px;
            font-weight: 500;
        }
        .video-container {
            position: relative;
            width: 100%;
            height: 0;
            padding-bottom: 56.25%; /* 16:9 aspect ratio */
            margin-bottom: 30px;
        }
        .video-container iframe {
            position: absolute;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
            border: none;
            border-radius: 8px;
        }
        .description {
            background: #f8f9fa;
            padding: 20px;
            border-radius: 8px;
            margin-bottom: 30px;
            white-space: pre-wrap;
            font-size: 1em;
        }
        .description a {
            color: #3498db;
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s;
        }
        .description a:hover {
            color: #2980b9;
            text-decoration: underline;
        }
        .transcript {
            background: #f8f9fa;
            padding: 20px;
            border-radius: 8px;
            margin-bottom: 30px;
            white-space: pre-wrap;
            font-size: 1em;
        }
        .transcript h2 {
            color: #2c3e50;
            margin-top: 0;
            margin-bottom: 15px;
            font-size: 1.5em;
        }
        .tags {
            margin-bottom: 30px;
        }
        .tags h2 {
            color: #2c3e50;
            margin-bottom: 15px;
            font-size: 1.5em;
        }
        .tag {
            display: inline-block;
            background: #3498db;
            color: white;
            padding: 6px 12px;
            margin: 4px;
            border-radius: 20px;
            font-size: 0.9em;
            font-weight: 500;
        }
        .links {
            display: flex;
            gap: 20px;
            flex-wrap: wrap;
        }
        .link {
            display: inline-block;
            padding: 12px 24px;
            background: #3498db;
            color: white;
            text-decoration: none;
            border-radius: 6px;
            font-weight: 500;
            transition: background-color 0.3s;
        }
        .link:hover {
            background: #2980b9;
        }
        .link.youtube {
            background: #e74c3c;
        }
        .link.youtube:hover {
            background: #c0392b;
        }
        @media (max-width: 600px) {
            .container {
                padding: 20px;
                margin: 10px;
            }
            h1 {
                font-size: 1.8em;
            }
            .links {
                flex-direction: column;
            }
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>Decouvrez l IA et le ML sur AWS pour les developpeurs   Startup Day Paris 2018</h1>
        <div class="date">October 07, 2018</div>
        
        <div class="video-container">
            <iframe src="https://www.youtube.com/embed/CLItrF2guXQ" 
                    allowfullscreen 
                    allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture">
            </iframe>
        </div>
        
        <div class="description">Slides : <a href="http://chilp.it/91e9537" target="_blank" rel="noopener noreferrer">http://chilp.it/91e9537</a>

Retrouvez les autres événements AWS sur aws.amazon.com/fr/events/</div>
        
        <div class="transcript">
            <h2>Transcript</h2>
            Bonjour à tous, je m'appelle Julien, je suis tech-évangéliste chez AWS. Je suis basé à Paris, je voyage beaucoup, je m'excuse d'avance, ça me fait assez bizarre de faire cette présentation en français, donc si je bafouille, c'est juste le manque d'habitude. Dans cette présentation, on va balayer les services de machine learning et d'IA qui sont disponibles sur AWS. On a la chance d'avoir avec nous aujourd'hui Marc de Whatis, une startup innovante qui fait des choses vraiment intelligentes et vraiment pointues sur le deep learning, et il vous parlera de son cas d'usage. Avant ça, je voudrais juste vous rappeler, si c'est nécessaire, qu'Amazon fait du machine learning depuis pratiquement le début. Amazon a été créé en 1995, et très tôt, même sur les premières versions du site Amazon.com, on a trouvé du contenu personnalisé, et puis très vite de la recommandation, etc. Donc le côté personnalisé du site Amazon est arrivé très tôt dans l'histoire. Et puis au fur et à mesure des années, le machine learning s'est propagé littéralement dans tous les recoins d'Amazon.

Sur les aspects logistiques, vous avez certainement déjà vu les vidéos des robots Amazon qui se déplacent de manière autonome dans les centres logistiques, qui transportent les étagères contenant les produits, etc. Depuis un peu plus d'un an maintenant, on expérimente la livraison par drone. Ce ne sont pas des drones télécommandés, ce sont des drones autonomes qui utilisent des modèles pour la navigation, la reconnaissance d'image, etc. Amazon Alexa, la famille de dispositifs Echo qui est maintenant disponible en France, s'appuie sur du machine learning pour faire la reconnaissance vocale, la synthèse vocale, la sélection des skills que vous invoquez, etc. Plus récemment, Amazon Go, dont vous avez probablement entendu parler, est un nouveau type de magasin. On en a deux à Seattle, qui sont ouverts au public. La particularité de ces magasins, c'est qu'il n'y a plus de caisse, on ne fait plus la queue. On rentre, on s'identifie avec une application mobile, on prend des produits, on les met dans son sac, on sort, et plus tard, on reçoit la facture sur le mobile. Et ça marche parce qu'au plafond, il y a tout un tas de petites caméras qui font de la reconnaissance en temps réel, de la reconnaissance produit, et qui regardent ce que vous avez pris, ce que vous avez mis dans le sac, ce que vous avez ressorti du sac et reposé, le cas échéant, et qui vous facture le bon montant.

Sur cette expérience de machine learning chez Amazon, qui s'est accumulée sur les 20 dernières années, même un peu plus, la mission d'AWS est de construire un ensemble de services qui permettent à tout le monde d'utiliser du machine learning dans ses applications, quel que soit le niveau d'expertise. C'est-à-dire, si vous êtes débutant, on va vous proposer des services de haut niveau, faciles à utiliser. Si vous êtes plus spécialisé, vous pourrez entraîner vos propres modèles, déployer et tweaker tous vos paramètres et avoir le contrôle complet sur le processus d'entraînement. On a la faiblesse de penser que c'est sur AWS que tournent le plus de projets de machine learning. On retrouve des noms très connus qu'on s'attendrait à trouver, Netflix, Capcom, Capital One, qui est une des dix plus grandes banques américaines, et puis des grosses startups, je ne sais pas si on peut encore les qualifier de startups, mais des Pinterest, des Tinder, etc. On se demande bien ce que Tinder fait avec du machine learning. Et tout ça, en fait, ça ne tenait pas sur un slide. Je ne parle ici que des références publiques, et vous voyez qu'on a un mix de très grosses sociétés, de secteurs publics, il y a aussi beaucoup de startups, qui à des niveaux divers et à des niveaux techniques divers utilisent nos services de machine learning. Je pense qu'assez vite il va me falloir un troisième slide, parce qu'il n'y a plus beaucoup de place sur celui-là.

La stack aujourd'hui ressemble à ça, donc on a trois couches. Une première couche de haut niveau qu'on appelle les services applicatifs, application services, qui sont des services basés sur des API qui ne nécessitent aucun entraînement, qui ne nécessitent pas de jeu de données. Tout ce que vous devez faire, c'est, dans le cas de Recognition, par exemple, qui est notre service d'analyse d'images, vous passez une image à une API de Recognition et en temps réel, vous recevez une réponse. Donc, des services de haut niveau qui s'appuient tous sur du machine learning, mais qui ne nécessitent aucune expertise, c'est juste une API que vous appelez. Pour les gens qui ont envie d'entraîner des modèles et de contrôler finement le processus de machine learning, on a notamment conçu un service qui s'appelle SageMaker, dont je parlerai brièvement et dont Marc parlera également, qui est un service qui vous permet d'avoir le contrôle complet et même de vous concentrer entièrement sur le machine learning, mais sans gérer la moindre infrastructure. L'infrastructure d'entraînement et de prédiction est complètement managée. Et au niveau le plus bas, pour les clients qui ont envie de gérer eux-mêmes leur infrastructure, vous pourrez bien sûr créer des instances EC2, CPU, GPU, et vous pourrez y déployer toutes les librairies que vous aimez utiliser. Et on a un outil qui s'appelle la Deep Learning EMI, dont je parlerai tout à la fin, qui permet de faire ça plus facilement.

Je vais aller assez vite sur les Application Services, parce que sincèrement, aujourd'hui, avec Marc, on avait plutôt envie de parler de vrai machine learning. Donc, je vais juste les balayer, faire une petite démo, et puis après, on va rentrer un peu plus vite dans le vif du sujet. Ces services sont vraiment très faciles à aborder, et je vous donnerai les URL pour aller les tester tout à l'heure. Donc, Recognition, que j'ai déjà mentionné, est un service d'analyse d'images qui fait de la détection de scènes, de la détection de visages, de la détection de textes, la détection de célébrités, de la modération de contenus, tout un tas de fonctionnalités assez avancées, mais vraiment très simples à utiliser, juste par API. On a étendu Recognition aux flux vidéo et aux archives vidéo. Vous pouvez avoir des archives vidéo dans S3 ou des flux vidéo en temps réel que vous allez passer à travers Recognition vidéo pour faire la même chose que ce que vous faisiez avec Recognition Image, plus de la détection d'activités et éventuellement du tracking de personnes puisque évidemment vous avez le flux. On a également un service de chatbot qui s'appelle Lex qui vous permet de définir des chatbots conversationnels, soit texte, soit voix. Et puis on a quatre services texte, texte et langage disons. Un premier qui s'appelle Comprehend qui est un service qui fait du traitement du langage naturel, qui va faire de l'extraction d'entités, de l'extraction de phrases clés, de l'analyse de sentiments, ce genre de choses. Vous lui passez un document et sur quelques appels d'API très simples, vous allez pouvoir extraire de l'information de ces textes. On a un service de traduction qui s'appelle Translate, qui supporte aujourd'hui 12 langages, avec six autres dans le pipe, et probablement encore plus, mais six au moins déjà confirmés. Transcribe, qui est un service de speech-to-text, donc vous lui passez un fichier son, alors lui n'est pas temps réel, et en mode batch, il va vous générer en quelques minutes une transcription de ce texte, avec la ponctuation, avec les time-codes, etc. Alors lui, il supporte l'anglais et l'espagnol aujourd'hui. Et enfin, on a Poly, qui est le service inverse de text-to-speech. Là, vous passez à une chaîne de caractères et vous générez un clip que vous pouvez jouer en temps réel, parce que le service est rapide, donc vous pouvez vous en servir de manière transactionnelle, ou alors vous archivez le fichier son pour le jouer plus tard. Aujourd'hui, on a 25 langues et 57 voix, parce qu'il y a plusieurs voix par langue, hommes, femmes, etc. Tous ces services, vous pouvez les tester gratuitement dans le cadre de ce qu'on appelle le Free Tier, en bon français. Allez sur aws.amazon.com.fr pour plus de détails. Le Free Tier, c'est simplement une façon pour les nouveaux clients d'AWS, les gens qui ont un compte il y a moins de 12 mois, d'avoir un grand nombre de services gratuitement dans les limites d'un certain usage. Ça concerne plein de services et ça concerne tous ces services d'IA. Vous pouvez littéralement les tester gratuitement et vous faire un avis sur leur qualité.

Très vite, quelques exemples de startups qui utilisent ces services. La première dont je voulais parler, c'est une startup américaine qui s'appelle Marinus Analytics, qui développe différents outils, mais en particulier ici, un outil destiné aux forces de l'ordre pour retrouver automatiquement sur Internet des personnes disparues, et spécifiquement des enfants. Donc ils utilisent Recognition pour faire ça. Ils vont indexer les visages des personnes disparues, ils vont créer ce qu'on appelle une « face collection », une collection de visages au sein de leur compte AWS. Donc une collection de modèles mathématiques représentant ces visages. Et ensuite, automatiquement, leur outil va crawler les pires endroits du web ou crawler des disques durs et des collections d'images qui pourraient avoir été saisies dans le cadre d'enquêtes, etc. Et on va essayer d'aller matcher les visages des personnes qu'on voit dans ces images, dans ces photos ou dans ces vidéos, avec les visages des personnes disparues. Ils racontent assez bien le lancement de ce projet-là dans leur blog post, et ils disent qu'en fait, la première semaine de mise en production, dès la première semaine, ils ont commencé à trouver des gens, à les localiser, et à envoyer les informations aux forces de l'ordre pour aller les chercher. Donc voilà un cas d'usage où on voit, enfin, évidemment, l'histoire est belle, mais on voit aussi que Recognition est considéré suffisamment fiable en reconnaissance faciale pour déclencher ce genre d'action. Il ne s'agit pas de mobiliser des forces de l'ordre pour un truc qui est faux 9 fois sur 10. La précision est suffisante pour qu'on engage des actions fortes derrière.

De manière plus légère, si vous avez regardé le mariage royal il y a quelques mois, maintenant sur Sky News, vous auriez vu en temps réel une détection de célébrités sur le flux vidéo. Et en fait, ça c'est l'utilisation de Recognition vidéo et spécifiquement de la fonctionnalité de détection de célébrités où sur un flux temps réel on est capable d'aller détecter un très grand nombre de célébrités. Au Royaume-Uni, on se doute que tout le monde connaît les Beckham, j'imagine, ou Elton John, je pense qu'ils sont assez reconnaissables. Vous allez me dire, ce n'est pas très utile d'avoir Recognition pour faire ça. Par contre, le prince du Lesotho, si quelqu'un peut me citer son nom comme ça, sans regarder le slide, félicitations, pour ma part, j'ignorais qu'il y avait une famille royale au Lesotho. Ça donne une idée de la profondeur de la base. Il y a vraiment beaucoup de célébrités, d'acteurs, de politiciens, etc. dans cette base.

Un petit exemple sur Poly, donc Poly Text-to-Speech, ici utilisé par Duolingo, que vous connaissez certainement. Duolingo, c'est une application pour apprendre les langues étrangères. Et vous en avez certainement fait l'expérience. Lorsqu'on entend une voix de très bonne qualité avec un très bon accent, on apprend mieux. Souvenez-vous de votre prof d'anglais ou d'allemand au collège qui avait un accent pitoyable, ça ne vous aidait pas à apprendre. Ce n'est pas forcément pour ça que je n'étais pas très bon en allemand, mais maintenant je le justifie comme ça. Et donc, Duolingo s'est dit qu'on allait maximiser la qualité des voix que les étudiants entendent. Ils ont fait des A-B tests entre Poly et six autres fournisseurs de voix, et ils ont mesuré à quelle vitesse les étudiants apprenaient. Et dans les six cas, c'est avec Poly qu'ils apprenaient le mieux. Et donc maintenant, pour toutes les voix que Poly supporte, c'est donc Poly que Duolingo utilise. Donc si vous apprenez le russe ou l'italien avec Duolingo, c'est certainement Poly que vous entendez.

On va faire une petite démo rapide, et ensuite on parlera brièvement de SageMaker et je passerai la parole à Marc. Un premier exemple très simple avec Recognition, on va prendre une image, c'est un peu la saison, c'est une démo qui marche bien en Allemagne ça. Donc octobre, une image assez riche, beaucoup de visages, beaucoup de contexte, et puis un effet, je ne suis pas un spécialiste de photographie, mais manifestement il y a un effet déformant sur la photo. Bon, on va dire, ok, essayons de détecter un peu ce qui se passe. Donc on va utiliser Recognition. Je vais juste essayer de me souvenir... Ou alors je vais tricher, je vais faire ça, oui. Je vais rappeler ma commande tout à l'heure. Je vais copier mon image dans un bucket S3, et puis je vais demander à Recognition de l'appeler. Alors, il y a un peu de son, je pense que vous allez l'entendre. Ok, donc là, on extrait des informations de Recognition, on les passe à Poly pour qu'ils nous disent qu'il a détecté 22 visages et les principaux labels sont « human, people, person », etc. Bon, et puis ensuite on voit qu'effectivement on a détecté 22 visages et pour chacun on affiche le sexe, la transgénération, d'âge et puis les pourcentages, les probabilités d'émotion. Il y a beaucoup plus d'informations qui sont retournées par Recognition que ça, mais je me suis limité à ça. Et si on utilise également les informations que Recognition retourne sur la position des visages, etc., et puis qu'on trace des carrés autour des têtes, ça donne ça. Donc on a effectivement détecté 22 visages. On a détecté des visages très facile, évidemment. Ceux-là, ils sont de face, ils ne sont pas cachés. On a détecté celui-là, qui est quand même plus dur. On a détecté ces deux en bas, qui ne sont pas très simples. On a détecté celui-là, qui est à moitié caché. Celui-là, qui a le nez dans sa chope. Celui-là aussi, assez difficile. Celui-là, qui est trois quarts d'eau. Je pense qu'on ne détecte pas celui-là et celui-là, à mon avis, parce que l'orientation... Si vous connaissez des gens qui marchent comme ça dans la rue, je pense que le training set n'est pas forcément fait pour détecter des gens qui ont la tête à 90 degrés. Mais vous voyez, on détecte en une fraction de seconde, on détecte des visages et on pourrait regarder les transgressions d'âge et les sexes, ils sont corrects. Donc ça, c'est littéralement un appel d'API Recognition.

Ce qui est rigolo, c'est d'essayer de combiner les trucs. Alors prenons une image. C'est un panneau publicitaire pour la petite histoire, c'est un vrai panneau publicitaire, j'ai juste masqué évidemment le nom du magasin, qui était à Las Vegas. Et pour la petite histoire, les gens qui habitent dans ce quartier ont demandé à ce que le panneau soit enlevé parce que ça donnait une mauvaise image du quartier. Si vous êtes déjà allé à Las Vegas, des gens qui se plaignent de l'abus d'alcool à Las Vegas, je trouve ça assez comique. Ça n'a rien à voir avec ma présentation. Je trouve que l'anecdote est intéressante. Donc je me dis, tiens, et si j'inventais une appli, je pourrais avoir ça sur une caméra dans ma voiture, qui lit les panneaux et qui me les traduit, qui détecte le texte et qui me les traduit. Alors, faisons ça. Et finalement, avec les différents services dont j'ai parlé, ce n'est pas très compliqué. Parce que je vais utiliser Recognition pour détecter le texte, dans l'image. Je vais utiliser Comprehend pour détecter le texte pour détecter le langage. Ici, je triche un peu, car je sais que c'est de l'anglais, donc je sélectionne automatiquement la voix anglaise, mais on pourrait prendre la voix qui correspond à la langue qu'on a détectée. Ensuite, je vais le traduire avec Translate, en espagnol, portugais, français, et puis on a encore... Qu'est-ce qu'on a rajouté ? Allemand, russe, japonais, je crois. Il y a d'autres langues. Donc on va les traduire et puis on va utiliser Poly pour les prononcer. Et donc ça, ça fait... Voilà, ça fait 40 lignes de code. Et donc si on essaye ça... Ah oui, il y avait l'allemand, forcément. Ah ! Il y a un copier-coller un peu rapide. Le dernier, c'est évidemment du japonais. Eh oui, parce que tout le monde avait vu la chaîne de caractère qui est là, mais il y a celle-là aussi. Alors, il commet, si on veut être perfectionniste, il commet une toute petite erreur, parce que c'est Bombay, Saphir, si vous connaissez le produit. Donc, il commet une petite erreur, mais on peut lui pardonner, parce que l'image, ce n'est pas super haute résolution. Il arrive quand même à lire. Il arrive à lire les pixels, il n'est pas loin. Ce n'est pas mal, la précision est assez bonne. Tout ça en temps réel. Maintenant, imaginez que vous ayez ça à construire, sans utiliser ces services-là, bon, là, il y a du deep learning et du machine learning assez sévère à faire. Avec ces services-là, vous vous contentez d'appeler quelques API et puis c'est plié.

Justement, si vous avez besoin d'entraîner des modèles, il y a pas mal de murs à franchir. Si vous devez travailler avec votre dataset et que vous voulez déployer votre professionnel, il va falloir franchir tous ces murs-là. Et ça, pour la plupart des développeurs, qui ne sont pas des spécialistes de machine learning, c'est particulièrement difficile. Souvent, les gens n'y vont pas, ils abandonnent. Ils sont découragés par la difficulté de mise en œuvre du machine learning, même au-delà de la complexité du machine learning. Tous les aspects, infrastructures qui vont avec peuvent être décourageants. L'objectif d'un service comme SageMaker, c'est précisément de résoudre ça. C'est précisément de laisser les développeurs se concentrer sur la partie machine learning et d'ignorer toute considération d'infrastructure. L'infrastructure est complètement managée, Marc va en parler, et donc vous vous concentrez uniquement sur votre machine learning. Donc on a un ensemble de modules, je ne rentre pas trop dans les détails, les éléments importants, c'est évidemment des instances qu'on appelle les notebook instances, qui sont des instances préinstallées avec Jupyter, avec tous les outils de machine learning traditionnels. Une collection d'algos sur étagère, on a 14 algos sur étagère, qui permettent d'implémenter, de résoudre les grands problèmes de machine learning, la classification, le clustering, la régression linéaire, la personnalisation, recommandation, etc. Donc ces algos-là, vous les prenez, le code est déjà là, vous positionnez quelques paramètres, vous indiquez où sont vos données, et boum. Si vous voulez faire du deep learning, vous pouvez travailler avec des environnements également préexistants pour MXNet, Chainer, TensorFlow, PyTorch. Donc on a déjà ces environnements disponibles. Et puis si vous voulez complètement autre, si vous avez une librairie custom en C++ pour faire du machine learning, vous pourrez aussi l'emmener, vous devez l'emballer dans un container Docker que SageMaker va utiliser, etc. Donc vous pouvez, à différents niveaux, travailler avec des algos built-in, des environnements préexistants ou des environnements complètement custom. Et puis ensuite, vous pouvez entraîner votre modèle, évidemment, en une ligne de code, juste en indiquant combien d'instances vous voulez pour l'entraînement et de quel type. SageMaker va tout gérer. Vous pouvez faire de l'optimisation d'hyperparamètres, donc littéralement utiliser du machine learning pour trouver automatiquement les bons paramètres pour votre job, au lieu de chercher dans le désert le plus souvent. Et puis enfin, vous pouvez déployer vos modèles tout aussi simplement en une ligne, soit en mode HTTPS, avec de l'autoscaling pour piloter automatiquement la taille de l'infrastructure de prédiction, soit en mode batch, si vous voulez faire de la prédiction en mode batch, on a aussi cette possibilité-là.

On a pas mal de clients, on a des grosses boîtes, GE, Intuit, Dow Jones, etc. On vient de signer récemment avec la Formule 1. Je pense que tout ce que vous allez voir sur les écrans de Formule 1 à partir de la prochaine saison, ça sera probablement généré par du SageMaker et de l'infra AWS. C'est curieux de voir ce que ça va donner. Et puis beaucoup de gros acteurs du web, Hotels.com, Tinder, Cookpad, etc., qui ont évidemment énormément de données et besoin de construire pas mal de modèles. Ce qui nous amène donc à nos amis de Whatis. Donc Marc, bienvenue. Je pense qu'on peut peut-être l'applaudir. Merci. Et donc Marc va nous parler de Whatis et de leur usage de SageMaker et du machine learning en particulier. Bonjour à tous. C'est bon le micro, merci bien. Ouais, ça a l'air d'aller. Super. Donc j'ai 20 minutes pour... J'ai pris à peu près 20 minutes, oui, pour vous expliquer en fait d'où je suis parti, comment je fais accélérer ma boîte au mieux, qu'est-ce qu'on fait chez Whatis et finalement pourquoi j'ai conclu que Amazon SageMaker et Mixnet c'était des outils que je voulais utiliser que je recommande. Donc je commence avec un petit quiz assez simple qui est CEO dans cette dans cet amphi, CEO ou plutôt profil business ? Il y en a quelques-uns d'accord. Qui est CTO donc plutôt profil technique ? Ok. Deuxième question, qui a lancé déjà sa start-up ou est au démarrage ? Qui a envie de se lancer ? Qui a déjà fini sa start-up et a déjà revendu ? Non, pas encore. Très bien. Bon, tout ça, c'est pour vous expliquer qu'en fait, on a tous finalement les mêmes problématiques. Quelles sont-elles ? Vous avez une idée ? Parce qu'en général, les techs, on a des problèmes et les business en ont d'autres et c'est pas toujours les mêmes. On a deux grosses problématiques, on veut aller très très vite, genre vitesse de la lumière et on a à peu près en termes de ressources une trottinette, la fameuse trottinette que tout le monde vous explique en milieu de start-up en vous disant vous allez voir c'est le meilleur produit du monde et puis on vous dit qu'avec votre trottinette vous devez aller le plus vite possible. Donc des fois on se dit un peu bon courage, il y a des problématiques financières, techniques et donc moi je vous explique. Et puis, j'ai essayé avec ma petite trottinette de faire avancer au maximum ma boîte. Je suis commencé il y a trois ans, je vivais dans un laboratoire. Ça peut vous rappeler quelques bons souvenirs ou mauvais souvenirs d'ETP, avec des outils qui vous faisaient de bons ou mauvais souvenirs de MATLAB, par exemple. Je travaillais sur du redimensionnement spatial. Ça m'a permis de faire des articles de recherche et de faire plein de choses très sympathiques dans le milieu académique. Je me suis dit que j'avais envie de quitter ce milieu-là et de me lancer dans un super challenge qui est techniquement très dur, vu que j'ai plutôt profil technique, et en même temps qui est vraiment super cool d'un point de vue applicatif. Je me suis posé pas mal de questions. J'en ai discuté avec mes associés actuels. On en est arrivé sur un projet qui s'appelle Whatis. On dit qu'on est une deep tech startup, même si après, qu'est-ce qu'une deep tech startup ? C'est toujours attirer avec... de savoir, mais ce qu'on fait, c'est des visual search engines orientés dans le fashion. Qu'est-ce que ça veut dire ? Par exemple, vous avez une photo d'influenceur, vous aimez bien ce chapeau orange, et vous vous dites « mince, où est-ce que je pourrais l'acheter ? » Vous avez cette chemise bleue que vous aimez bien, enfin verte, dans ce cas-là, et vous aimeriez bien pouvoir l'acheter. Et donc là, il y a une problématique, c'est qu'il faut retrouver l'objet, et il y a beaucoup de problèmes techniques derrière, l'influence du background, l'arrière-plan, plusieurs objets déformables, on a des différences d'angle de vue. Pour ceux qui font un peu de neural net, ou ceux qui n'en font pas, je vous garantis, c'est loin d'être simple. Mais c'est très challenging et très intéressant. Donc, je me dis que j'ai envie de me lancer là-dedans, je regarde ce qui se passe en AI, donc je préfère parler de réseau de neurones en computer vision. 2012, AlexNet, je vous ai sorti une magnifique architecture de réseaux de neurones, améliore de 11%, un challenge qui s'appelle Imagenet, donc d'habitude au milieu de la recherche on fait plus 0,5, là quelqu'un arrive, un PhD guy, et puis il vous dit plus ou moins, en fait je suis en train de vous clore le sujet, depuis 15 ans vous êtes en train de travailler, et bon courage pour la suite. Donc petit coup de nettoyage sur les méthodes traditionnelles, et puis évidemment il n'y a pas que la classification qui a été élevée, bouleversée, pas ce genre de choses. Vous avez d'autres secteurs, d'autres cas applicatifs. À gauche, vous avez de la segmentation. Vous voyez qu'il y a une box qui m'entoure, mais il y a aussi un masque. On a un effet de couleur bleu clair sur la chaise, sur ma personne. On a un détourage malgré mes cheveux, on arrive même à faire le tour. En détection, vous retrouvez que le réseau de neurones a défini que j'étais un scientifique et un organisme vivant. Je suis assez fier de voir que de temps en temps, j'ai une tête de scientifique, en tout cas, c'est les raisons de neurones qui le disent, et je ne vais pas les contredire pour une fois. Et vous avez aussi la possibilité de refaire la modélisation 3D de personnes, tout ça à partir d'une caméra traditionnelle type RGB. Donc votre smartphone pourrait, modulo certaines choses, pourra faire ce genre de choses. Et donc la question aussi des Visual Search Engines, où est-ce qu'on en est sur ce secteur-là ? Notre photo de notre chapeau orange. Je l'ai mis dans Google Images pour essayer de le retrouver. Et voilà, donc aucun résultat s'y mis, enfin aucune image qui ressemble. Finalement, elle me dit que c'est une photographie de portrait. Puis vous voyez que, bon, vous n'avez peut-être pas la même classe, mais vous avez quand même un chapeau orange. Donc finalement, vous avez peut-être obtenu vos objectifs. Bon, moi, à titre personnel, je trouve qu'il y a quand même une petite différence sémantique entre les deux. Et je serais un peu embêté finalement de vouloir acheter le chapeau orange et de finir avec un casque de chantier orange. Donc il y a encore beaucoup de challenges et donc ça laisse la porte ouverte. Tout ça pour vous dire qu'à la fin, qu'est-ce qu'on est en train de parler ? On est en train de parler du moteur. Les réseaux de neurones, c'est la grosse machinerie du centre. Tout le monde dit c'est génial, il faut faire des li, il faut faire des neural nets, super. Mais en fait, vous avez un peu oublié tout le reste. C'est que ce moteur, il tourne, mais il faut qu'il serve à quelque chose. Est-ce que vous voulez à la fin le mettre dans une voiture, dans un camion, ou est-ce que c'est votre petite trottinette que finalement vous êtes en train de construire ? Et entre le moteur et tout le reste, il y a évidemment beaucoup de choses. Et c'est là où on arrive à la partie marrante. Quand vous commencez votre start-up en tant que CTO, vous vous posez un certain nombre de questions. Est-ce que je dois faire du cloud ou je dois mettre les choses en local ? Chose qu'on aime bien se poser en France finalement parce qu'on aime bien garder nos données en local. Je ne sais pas, c'est une sorte de fantasme. Bref, si vous voulez utiliser le cloud, quel cloud vous voulez utiliser ? On va faire des cartes graphiques, lesquelles ? Les bases de données ? Est-ce que vous faites du flash, du redis, serverless ? Finalement, comment on choisit dans tout ça ? Dans les frameworks, TensorFlow, PyTorch, Mixnet ? Est-ce que vous avez utilisé QDA7, QDA8, QDA9, QDA9.2 ? Je vous laisse suivre les mises à jour. Si vous êtes un peu tech et vous suivez ce secteur-là, vous allez voir que c'est toujours très drôle. Et puis ensuite, on a eu des présentations aujourd'hui sur ces sujets-là, mais moi, personnellement, ça a été des vrais sujets d'interrogation. Kubernetes, Docker, comment je crée une architecture, comment je suis l'intégration continue, comment je gère ma sécurité. Et toutes ces questions-là se posent en même temps que vous êtes en train de vous poser la question de faire vos réseaux de neurones. Bon, après, il y a des fausses questions type GitHub, GitLab, et puis finalement, quels sont les profils que vous avez besoin de recruter ? C'est quoi les besoins de votre boîte, combien ça coûte tout ça. Vraiment, on se rend compte que poser la question du réseau de neurones et de l'intelligence artificielle, c'est aussi la poser dans un contexte global. Donc, je me suis lancé, j'ai fait quelques expérimentations personnelles, je me suis acheté quelques cartes graphiques, j'ai installé CUDA. Pour ceux qui l'ont fait, en général, ils arrivent à ce genre de choses, vous êtes fiers, vous avez passé trois jours à installer CUDA, et puis vous le lancez, et on vous dit « CUDA, je ne sais pas où c'est ». En cas bon, c'est quand même un peu embêtant, je viens de passer trois jours de ma vie, et encore trois jours je suis gentil, à me faire mal. Après vous vous dites, j'utilise TensorFlow, ça ne reste que mon point de vue. J'utilise TensorFlow, donc là j'ai mes quatre cartes graphiques, je lance, et puis vous voyez en fait que les quatre cartes sont prises alors qu'il n'y en a qu'une seule qui travaille. Donc en fait TensorFlow, je vois ça un peu comme, vous avez un petit peu à manger, il vous prend tout le big back. Donc c'est un peu ennuyeux, surtout que ça consomme, puis ça chauffe les cartes graphiques si vous les avez en local, et puis si vous les avez dans le cloud, c'est dommage. Donc dans tout ça, et puis la documentation, j'ai commencé à danser sur ce flow 0.10, ça faisait mal aussi. Bref, les EC2, donc là c'est des services plus bas niveau, il faut faire des serveurs, faire la sécurité, faire du screen. Tout ça c'est plein d'expérimentations que j'ai faites, et qui m'ont permis de me rendre compte qu'en fait j'avais besoin de me concentrer sur la vraie problématique de ma boîte, c'est quoi le produit, c'est avec mes associés qu'on se pose cette question là, qui est prêt à payer pour ça, c'est quoi les réseaux de neurones que je vais faire et finalement comment je peux facilement mettre à jour mes back-ends. Parce que tout le reste c'est sympa mais finalement c'est du plaisir technique pour du plaisir technique, le but c'est d'avancer avec ma trottinette. Et donc on a fait deux produits chez Whatis, on a un produit XS qui s'appelle « What's Style », donc c'est un peu comme l'illustration que je vous ai montrée au démarrage. Vous avez une photo d'influenceur que vous aimez bien, vous l'avez enregistrée, vous nous l'envoyez, et on va vous retrouver les produits qui sont associés dans un catalogue de sites de e-commerce. Donc c'est une solution B2B. La deuxième, que j'aime beaucoup, vous êtes sur Netflix, vous êtes en train de regarder votre petite série tranquillement, puis on arrive proche de Noël, votre femme vous dit « Ah, j'adore ce petit sérieux, un sac à main, et vous dites, waouh, un sac à main, qu'est-ce qui ressemble plus à un sac à main qu'un autre sac à main ? Difficile. Vous mettez en pause, le sac à main est détecté, et on vous aide à le retrouver directement. Donc ça fait de la détection d'objets. Donc, qu'est-ce qu'on a fait, qu'est-ce qu'on a utilisé derrière ? On utilise Lambda et API Gateway, S3, DynamoDB. Ce sont des services que j'aime bien parce qu'ils sont simples, je trouve en tout cas à titre personnel, c'est plus simple que certaines choses et ça vous permet d'aller assez vite. Et évidemment, Amazon SageMaker et MXNet. Si je vous fais juste un petit feedback, ce que j'aime bien, c'est que MXNet avec Gluon, ce sont des frameworks efficaces. Vous n'êtes pas en train de consommer 12 GPU alors que finalement vous n'en avez besoin que de la moitié d'un. La documentation est très claire. Si vous n'avez jamais eu l'occasion de vous mettre aux réseaux de neurones, c'est un bon début. Et il y a des modèles qui sont à l'état de l'art. Vous n'avez pas non plus les modèles d'il y a 15 ans, type AlexNet, vous avez des choses qui sont récentes. Vraiment, je vous recommande d'aller faire un tour sur le site de MXNet et d'aller regarder le framework Gluon.

Après, vous avez Amazon SageMaker. Ce que j'aime beaucoup, ce sont les GetStartNotebook. Il y a une petite zone, il y a écrit un GetStartNotebook. Vous cliquez dessus et vous avez directement des bouts de code prêts à exécuter, vous avez juste à faire run, run, run et ça marche super bien. L'intérêt, c'est que vous n'avez plus qu'à vous concentrer sur les réseaux de neurones et pas sur toute la machinerie qu'il y avait avant, et les coûts sont réduits. On se dit que c'est un peu plus cher, par contre vous y gagnez trois semaines voire trois mois. Donc ça c'est quand même un coût, il faut aussi intégrer. Là, peut-être l'inconvénient qu'il y a, c'est que la communauté est aujourd'hui assez réduite. On est une cinquantaine ou une centaine. J'espère qu'à la fin de cette journée, on aura 50 nouvelles personnes qui vont utiliser MXNet et SageMaker.

Pour vous dire à quel point c'est simple, on a une petite base de données qu'on a faite à la maison avec des T-shirts rayés. Je suis dans mon autre. Ça, c'est des extraits de mon notebook du GetStart. Je prends mon T-shirt, je le lance et il me dit : « C'est un T-shirt rayé. » 0,999% de certitude que c'est un T-shirt rayé. Donc c'est un bon outil qui permet rapidement, en quelques lignes, avec une petite base, de déjà avoir des premiers résultats de classification. Dans notre problématique, la classification, c'est la brique élémentaire de nos problèmes, mais c'est quand même agréable d'avoir déjà un premier truc qui fonctionne. Quand on est dans le milieu de startup, on aime beaucoup se dire qu'il y a des next steps, il faut qu'on enchaîne.

Pour nos amis CEO, je vous invite fortement à aller sur ce site-là, Activate, vous allez droit à 1 000 euros de crédit, quelle que soit votre boîte. C'est quand même très pratique, parce que la question d'après, ça va être : « Ah oui, mais ça va me coûter cher d'aller jouer sur Amazon. » Non, c'est pas vrai, sauf si vous laissez tourner plein de GPU pendant trois semaines, mais ça, c'est un autre problème. Mais en tout cas, vous avez vraiment de quoi vous amuser avec 1 000 euros pendant un certain temps, et c'est juste le cas. C'est juste un clic, une inscription. Vous êtes les profils tech. Vous allez suivre Julien Simian sur Medium. Moi, ça fait un an et demi que je suis. C'est aussi comme ça que j'ai été converti, on va dire, à MXNet et SageMaker. Donc vraiment, c'est bien. Il y a beaucoup de vidéos super sympas sur YouTube, donc je vous invite fortement à le suivre. Et le Gluon MXNet, avec un get start, qui est vraiment super bien aussi. Pareil, vous avez un peu de machine learning. Si vous avez un peu oublié vos maths, ce n'est pas grave, ça vous est réexpliqué. Vous avez des petits bouts de code et vous avez de quoi vous amuser.

Après cette session, il y a un petit apéro. Ce sera l'occasion de venir nous voir et de discuter de ces sujets-là si ça vous intéresse ou si vous avez des questions supplémentaires. Si vous aimez bien notre projet chez Whatis, je vous invite à nous contacter. Moi, je suis le CTO et cofondateur de la startup. Là, il y a moi en train de faire le guignol avec un skeleton. Et on est incubé avec l'IMT Starter et en Normandie en même temps. Donc à la fin, si vous adorez cette chemise en nuage et que vous aimeriez bien l'acheter, je vous invite à aller voir sur le site de ASOS où elle n'y est pas encore. Mais voilà, si c'est des problématiques qui vous intéressent, n'hésitez pas aussi à venir nous voir, que vous soyez profil technique ou non, business, vous avez envie d'en savoir plus, voilà, je vous invite à venir nous voir. Merci beaucoup. Merci beaucoup Marc, au plaisir.

Il nous reste quelques minutes, on est peut-être un peu en retard mais c'est la dernière session de la journée, alors on va se permettre. Et puis le contenu est j'espère intéressant. Donc on a bien vu avec Marc et Whatis l'utilisation de stage management, donc d'une couche intermédiaire où on utilise des librairies de deep learning comme MXNet, Gluon, mais on pourrait utiliser TensorFlow et les autres. Et on ne s'occupe pas de la plomberie, on ne s'occupe pas de l'infrastructure sous-jacente, on ne se bat pas avec les drivers Nvidia, effectivement, je confirme. Et on se concentre sur son boulot et on essaie de faire avancer sa trottinette et sa startup.

Néanmoins, pour certains clients, quelles qu'en soient les raisons, il est important de contrôler exactement l'infrastructure et d'avoir vraiment la main, de créer des instances EC2, d'avoir un accès à ces sages et de faire tout ce qu'on peut faire avec un vrai serveur. Il y a deux familles d'instances EC2 que je peux vous recommander pour ce genre de sport. La première, c'est la famille C5. Si vous êtes familier ou pas des noms d'instances EC2, donc vous savez que C veut dire compute, donc des instances qui vont avoir les dernières générations de processeurs Intel. Ici on a l'archi Skylake, et 5 tout simplement c'est le numéro de génération. Donc il y a eu C1, C2, C3, etc. Récemment on a introduit une variante qui s'appelle C5D, qui a du stockage local, qui a du SSD, enfin c'est du NVMe pour être précis, donc du SSD dernière génération ultra rapide, ce qui peut être un atout quand on veut entraîner des gros modèles, d'avoir énormément d'Io local sans passer par de l'accès réseau. Donc si vraiment vous avez ce genre de contraintes, c'est plutôt C5D.

Cette génération C5, j'ai dit, elle a l'archi Skylake, qui contient plus de tout, comme d'habitude avec Intel, mais surtout elle contient ce nouveau jeu d'instructions avec 512 bits qui permet de faire de l'arithmétique 512 bits et lorsqu'on fait du machine learning on passe son temps à faire des multiplications de matrices, des additions de matrices, etc. Et le fait de pouvoir le faire sur des mots de 512 bits ça permet d'aller beaucoup plus vite et sans même chercher à optimiser on a une accélération déjà fois deux. Donc C5 pourrait, pour des datasets de petite et de moyenne taille, l'entraînement fonctionne bien. Et puis bien sûr pour la prédiction, généralement on n'a pas besoin d'avoir une instance GPU hyper puissante pour faire de la prédiction. Il vaut mieux s'appuyer sur une ou plusieurs instances C5 qui auront un rapport prix-performance bien plus intéressant.

Néanmoins, lorsqu'on a des très gros datasets, effectivement des très gros datasets non structurés, images, vidéos, voix, etc., on n'échappe pas aux instances GPU. Et donc là, on a une famille qui s'appelle P3, qui contient le dernier GPU Nvidia. Vous pouvez en avoir de 1 à 8 dans le même serveur. Et quand vous en avez 8, vous arrivez à un pétaflop de performance. Quand j'étais jeune au siècle dernier, un pétaflop, c'était du super ordinateur dans les centres de calcul militaire, et encore, je crois qu'on n'y était même pas. Donc c'est vraiment une puissance de calcul phénoménale, et ça c'est juste un serveur. Donc si vraiment vous voulez entraîner sur des datasets gigantesques, vous pouvez faire du training distribué, mettre autant d'instances P3 que nécessaire. SageMaker, SageMaker fera ça très bien pour vous aussi, si vous voulez utiliser les P3 dans SageMaker, et accélérer vos trainings. Bon, attention effectivement aux prix, elles sont évidemment à l'état de l'art, elles sont plus chères, donc comme l'a dit Marc, il ne s'agit pas de les laisser allumées tout le week-end pour rien, ce qui avec SageMaker ne peut pas se produire, parce que comme l'infrastructure est complètement managée, dès que le job de training est terminé, SageMaker éteint les instances. C'est quand même un gros avantage, c'est que vous ne laissez jamais tourner des instances de training pour rien.

J'ai mentionné tout à l'heure la Deep Learning AMI, donc Amazon Machine Image, c'est l'image qui sert à construire la machine virtuelle. Et donc là aussi, même si vous voulez faire les choses à la main, on vous évite gentiment d'avoir à installer toutes les librairies, d'avoir à maintenir les drivers Nvidia. Donc on a construit cette AMI que vous pouvez utiliser pour démarrer des serveurs. Son utilisation est gratuite, vous ne payez que pour l'instance sous-jacente. Donc je vous incite, si vraiment vous voulez gérer des instances EC2 pour faire du machine learning et du deep learning, très bien, faites-vous, mais par pitié, essayez d'utiliser cette AMI qui va au moins vous décharger de l'installation et de la mise à jour de tous ces packages qui peuvent prendre un certain temps à configurer et installer correctement. D'autant plus que les versions qui sont installées sur cette AMI sont des versions qui sont optimisées.

C'est-à-dire que ce n'est pas aussi simple que PIP install TensorFlow. Là, vous avez un bench d'entraînement de l'application classifieurs d'images. Donc ResNet, VGG, etc. Donc on a pris quatre modèles, le même dataset, et puis on a regardé à quelle vitesse, à combien d'images par seconde, est-ce qu'ils apprenaient sur une instance C5 avec le Vanilla TensorFlow, et à quelle vitesse ils apprenaient avec le TensorFlow qui est dans la Deep Learning AMI, qui contient des optimisations spécifiques pour l'archi Skylake, ainsi que des optimisations faites par AWS. On a des équipes qui ne font que ça, qui ne font que gérer, que optimiser la perf des librairies de deep learning sur AWS. Donc vous voyez qu'à la louche, on a un speed-up, une accélération de 5 à 6, entre le TensorFlow de base et le TensorFlow optimisé de la Deep Learning AMI. Ce qui veut dire que si vous avez un entraînement qui dure 7 heures, ou 6 heures, et bien là maintenant, en utilisant toujours la même instance, mais en utilisant la Deep Learning AMI, vous pouvez descendre ce training à une heure. Ce qui veut dire que dans la même journée, vous pouvez peut-être entraîner 6 fois, 7 fois plus, essayer différentes combinaisons d'hyperparamètres, combinaisons de réseaux et aller beaucoup plus vite et itérer plus vite pour atteindre le modèle idéal.

On a plein de clients, c'est vrai que la communauté MXNet elle n'est pas énorme, elle grossit, on la fait émerger en fait, c'est-à-dire qu'elle existe mais elle ne parle pas forcément beaucoup de ce qu'elle fait donc on est en train de travailler beaucoup aussi nous sur nos blogs pour parler de nos clients et de leur utilisation de MXNet. Ça, c'est une référence qui est sortie déjà il y a quelques temps, une société sino-américaine qui s'appelle Too Simple, qui fait des systèmes de conduite autonomes et donc qui entraîne des datasets assez gigantesques sur un MXNet et sur des instances GPU. Là, vous voyez, ils sont capables en temps réel de faire de la détection, de faire de la segmentation, des choses comme ça. Un exemple parmi d'autres.

Le dernier truc dont je voulais parler, c'est qu'évidemment, tout ça c'est sympa, mais nous, on est des développeurs, on aime bien les gadgets. Et donc, l'année dernière à re:Invent, on a lancé cette caméra qui s'appelle DeepLens, que je ne vais pas amener aujourd'hui, puisqu'on a déjà plus de temps. Donc c'est une petite caméra, une collaboration avec Intel, et le but du jeu ici, c'est de déployer sur cette caméra un modèle de deep learning et une fonction lambda, donc un petit bout de code qui va capturer le flux vidéo, le prédire localement avec le modèle de deep learning et puis le modifier, donc de déployer tout ça sur la caméra et de le faire tourner de manière autonome. Donc la caméra est capable toute seule de faire fonctionner de faire de la prédiction vidéo en temps réel sur la base d'un modèle qui aura probablement été entraîné dans le cloud, pourquoi pas dans SageMaker, et d'une fonction lambda qui lui sert à faire la prédiction. Donc on a un ensemble de projets prédéfinis que vous pouvez installer littéralement en deux minutes, configurer la caméra, déployer un projet de test, vraiment en moins de dix minutes, et commencer à jouer. Commencer à jouer avec cela, et puis ensuite, pourquoi pas, commencer à entraîner vos propres modèles et construire vos propres applications.

On a pas mal de projets communautaires qui ont été faits sur DeepLens, vous les trouverez sur le site d'AWS. Il y a eu des hackathons qui ont été organisés, on a pas mal de choses qui ont été faites. Alors voilà un exemple de ce qu'on peut faire avec ça. Ça c'est le projet de détection d'objets qui peut détecter 20 classes. Alors il y a les people, les gens, et puis il y a 19 objets à proprement parler. Donc là ce que vous voyez en fait c'est une photo de ma télé. Donc il y a DeepLens face à nous qui reconnaît la chaise, le canapé, le laptop, mon fils et moi. Le petit derrière ne l'attrape pas car son visage est un peu masqué. Et donc ça, c'est fait en temps réel par la caméra toute seule sans aucune intervention du cloud. C'est vraiment une caméra Deep Learning qui vous permet d'expérimenter, de vous former, d'éduquer les gens et de comprendre ce qu'on peut faire avec du Deep Learning.

Donc, si vous voulez aller un peu plus loin et découvrir davantage ces services, il y a une URL facile à retenir de haut niveau qui s'appelle ml.aws, où vous trouverez des liens vers tous les services dont on a parlé dans cette session, avec beaucoup de détails, des témoignages clients, etc. On a un blog machine learning qui contient pas mal d'articles plus techniques, du code, des architectures, de références, etc. Bon, effectivement, mon blog sur Medium, merci beaucoup Marc, tu vas me faire rougir, merci de l'avoir mentionné. Et puis sur YouTube maintenant pas mal de vidéos d'événements AWS ou d'autres conférences développeurs qui m'ont invité. Donc pas mal de vidéos sur tous ces services, le Deep Learning, SageMaker, etc. Donc si vous voulez creuser, aller beaucoup plus loin techniquement que ce qu'on a fait aujourd'hui, vous pouvez le faire. Et puis si vous voulez rester en contact, c'est suivre notre actualité sur ces sujets, ou me poser des questions, ou m'envoyer du feedback, ce que j'apprécie beaucoup, sur les services. Si vous voulez gueuler parce qu'un truc ne marche pas, Jules Simon sur Twitter semble être la bonne façon de me joindre, où que je sois dans le monde. Merci beaucoup, merci encore à Marc d'être venu et d'avoir pris sur son temps. Merci beaucoup.


        </div>
        
        <div class="tags">
            <h2>Tags</h2>
            <span class="tag">Machine Learning</span><span class="tag">AWS</span><span class="tag">SageMaker</span><span class="tag">Deep Learning</span><span class="tag">Computer Vision</span>
        </div>
        
        <div class="links">
            <a href="https://www.julien.org/youtube.html" class="link">Julien.org - Youtube</a>
            <a href="https://youtube.com/@juliensimon.fr" class="link youtube">Julien's YouTube channel</a>
        </div>
    </div>
</body>
</html>
<!DOCTYPE html>

<html lang="en">
<head>
<meta content="Training Transformers with AWS Trainium and the Hugging Face Neuron AMI - In this video, I show you how to accelerate Transformer training with AWS Trainium, a new custom chip designed by AWS, and the brand new Hugging Face Neuron Ama..." name="description"/><meta content="Training Transformers with AWS Trainium and the Hugging Face Neuron AMI - Julien Simon" property="og:title"/><meta content="Training Transformers with AWS Trainium and the Hugging Face Neuron AMI - In this video, I show you how to accelerate Transformer training with AWS Trainium, a new custom chip designed by AWS, and the brand new Hugging Face Neuron Ama..." property="og:description"/><meta content="https://www.julien.org/youtube/2023/20230223_Training_Transformers_with_AWS_Trainium_and_the_Hugging_Face_Neuron_AMI.html" property="og:url"/><meta content="video" property="og:type"/><meta content="summary_large_image" name="twitter:card"/><meta content="Training Transformers with AWS Trainium and the Hugging Face Neuron AMI - Julien Simon" name="twitter:title"/><meta content="Training Transformers with AWS Trainium and the Hugging Face Neuron AMI - In this video, I show you how to accelerate Transformer training with AWS Trainium, a new custom chip designed by AWS, and the brand new Hugging Face Neuron Ama..." name="twitter:description"/><link href="https://www.julien.org/youtube/2023/20230223_Training_Transformers_with_AWS_Trainium_and_the_Hugging_Face_Neuron_AMI.html" rel="canonical"/><meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<title>Training Transformers with AWS Trainium and the Hugging Face Neuron AMI - Julien Simon</title>
<style>
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
            line-height: 1.6;
            margin: 0;
            padding: 20px;
            background-color: #f5f5f5;
            color: #333;
        }
        .container {
            max-width: 800px;
            margin: 0 auto;
            background: white;
            padding: 30px;
            border-radius: 10px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
        }
        h1 {
            color: #2c3e50;
            margin-bottom: 10px;
            font-size: 2.2em;
        }
        .date {
            color: #7f8c8d;
            font-size: 1.1em;
            margin-bottom: 30px;
            font-weight: 500;
        }
        .video-container {
            position: relative;
            width: 100%;
            height: 0;
            padding-bottom: 56.25%; /* 16:9 aspect ratio */
            margin-bottom: 30px;
        }
        .video-container iframe {
            position: absolute;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
            border: none;
            border-radius: 8px;
        }
        .description {
            background: #f8f9fa;
            padding: 20px;
            border-radius: 8px;
            margin-bottom: 30px;
            white-space: pre-wrap;
            font-size: 1em;
        }
        .description a {
            color: #3498db;
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s;
        }
        .description a:hover {
            color: #2980b9;
            text-decoration: underline;
        }
        .transcript {
            background: #f8f9fa;
            padding: 20px;
            border-radius: 8px;
            margin-bottom: 30px;
            white-space: pre-wrap;
            font-size: 1em;
        }
        .transcript h2 {
            color: #2c3e50;
            margin-top: 0;
            margin-bottom: 15px;
            font-size: 1.5em;
        }
        .tags {
            margin-bottom: 30px;
        }
        .tags h2 {
            color: #2c3e50;
            margin-bottom: 15px;
            font-size: 1.5em;
        }
        .tag {
            display: inline-block;
            background: #3498db;
            color: white;
            padding: 6px 12px;
            margin: 4px;
            border-radius: 20px;
            font-size: 0.9em;
            font-weight: 500;
        }
        .links {
            display: flex;
            gap: 20px;
            flex-wrap: wrap;
        }
        .link {
            display: inline-block;
            padding: 12px 24px;
            background: #3498db;
            color: white;
            text-decoration: none;
            border-radius: 6px;
            font-weight: 500;
            transition: background-color 0.3s;
        }
        .link:hover {
            background: #2980b9;
        }
        .link.youtube {
            background: #e74c3c;
        }
        .link.youtube:hover {
            background: #c0392b;
        }
        @media (max-width: 600px) {
            .container {
                padding: 20px;
                margin: 10px;
            }
            h1 {
                font-size: 1.8em;
            }
            .links {
                flex-direction: column;
            }
        }
    </style>
</head>
<body>
<div class="container">
<h1>Training Transformers with AWS Trainium and the Hugging Face Neuron AMI</h1>
<div class="date">February 23, 2023</div>
<div class="video-container">
<iframe allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="" src="https://www.youtube.com/embed/0Y5E8RI_D2E">
</iframe>
</div>
<div class="description">In this video, I show you how to accelerate Transformer training with AWS Trainium, a new custom chip designed by AWS, and the brand new Hugging Face Neuron Amazon Machine Image (AMI).

First, I walk you through the setup of an Amazon EC2 trn1.32xlarge instance, equipped with 16 Trainium chips. Then, I run a natural language processing job, accelerating a BERT model to classify the Yelp review datatset on 32 Neuron cores. 

⭐️⭐️⭐️ Don't forget to subscribe to be notified of future videos ⭐️⭐️⭐️
 

- Hugging Face Neuron AMI on AWS Marketplace: <a href="https://aws.amazon.com/marketplace/pp/prodview-gr3e6yiscria2" rel="noopener noreferrer" target="_blank">https://aws.amazon.com/marketplace/pp/prodview-gr3e6yiscria2</a>
- Original Trainium video with the AWS Deep Learning AMI: <a href="https://youtu.be/HweP7OYNiIA" rel="noopener noreferrer" target="_blank">https://youtu.be/HweP7OYNiIA</a>
- Code: <a href="https://github.com/juliensimon/huggingface-demos/tree/main/trainium" rel="noopener noreferrer" target="_blank">https://github.com/juliensimon/huggingface-demos/tree/main/trainium</a>
- AWS Trainium: <a href="https://aws.amazon.com/ec2/instance-types/trn1/" rel="noopener noreferrer" target="_blank">https://aws.amazon.com/ec2/instance-types/trn1/</a>
- AWS Neuron SDK documentation: <a href="https://awsdocs-neuron.readthedocs-hosted.com/en/latest/frameworks/torch/index.html" rel="noopener noreferrer" target="_blank">https://awsdocs-neuron.readthedocs-hosted.com/en/latest/frameworks/torch/index.html</a>

Interested in hardware acceleration? Check out my other videos :
- Habana Gaudi: <a href="https://youtu.be/56fpEa1Y1F8" rel="noopener noreferrer" target="_blank">https://youtu.be/56fpEa1Y1F8</a>
- Graphcore: <a href="https://youtu.be/DgcJscPu1Vo" rel="noopener noreferrer" target="_blank">https://youtu.be/DgcJscPu1Vo</a>
- Trainium on SageMaker: <a href="https://youtu.be/pokM1r3rgIg" rel="noopener noreferrer" target="_blank">https://youtu.be/pokM1r3rgIg</a></div>
<div class="transcript">
<h2>Transcript</h2>
            Hi, everybody. This is Julien from Arcee. In a previous video, I showed you how to use AWS Trainium, a custom training accelerator designed by AWS to speed up transformer training. There was quite a bit of setup to do, particularly installing the so-called Neuron SDK to support the Trainium chip and additional dependencies. To simplify all of this, we've built a new Amazon machine image, AMI, that's freely available on the AWS Marketplace. So in this video, I'm going to show you how to launch a Trainium instance with this new AMI and how to start training your transformer immediately without any setup. OK, let's get to work.

Let's launch an instance. Just click here. Give it a name. Find our Arcee AMI in Marketplace AMIs. Yes. Okay. Select. Continue. Okay. Now we get to pick the instance type. So here we have the smaller one. Let's see if we can grab a large one, maybe. Add a key pair. Security group is fine. Storage is fine. Yeah, we can go and launch. All right, so in a minute or two, we should have an instance ready for us. So let's just wait for this and I'll be right back.

Okay, so I've connected to my instance. Let's clone the repo with my code here. Of course, I'll put all the links in the video description. Let's just go clone this thing. All right. The code is in this directory here. Okay. So I guess first things first, we can check that we see the neuron devices. Neuron LS is part of the neuron CLI. Yeah. And we see our 16 chips. Each one comes with two cores. So this is the same code I used in a previous video on Trainium. I am actually fine-tuning here, yes, BERT base on the Yelp full review data set, which is quite large, hundreds of thousands of local business reviews, and it's a pretty simple example. In the previous video, again, I'll put the link. But long story short, we just need to add this one line of code to our PyTorch training loop to support Trainium through PyTorch XLA. So very cool.

So that's the standalone version, and we have a distributed version which obviously is the one we want to run here because we have 16 chips and 32 cores, right? And this is, again, very close to vanilla PyTorch distributed training, again, through the XLA interface. Here we're just training on 10,000 samples, which is fine. I just want to highlight the fact that, hey, we can start this immediately and it's going to work. Okay. We'll run at scale in another video, but more on this later. Okay. So let's just fire this up. Oh, no, not like that. I got to use the port run command. And yes, I do want to disable tokenizer parallelism because I don't want to run tokenizer on each chip, right? Would be silly. And here I want to run 32 processes on this node. So I'll just say 32 here. And we should be good to go. No installation whatsoever, which is much better than what I had to do last time around on the deep learning AMI, which was all of this. Not complicated, but if you can do away with it, even better, right?

So this will start. This is the first run, so it's initializing all kinds of Python things, and that'll take a minute or two. Okay, I'll be right back when we start seeing output. All right, so once the model has been compiled, training can start, and we can see all the cores are happily busy training the model. Okay? Should be done in a couple of seconds. So there you go. This is really the simplest way to get started with AWS Trainium. Just grab our Arcee Neuron AMI and fire up a Trainium instance, the small one or the large one, and then you can run your code out of the box. No fuss, nothing to install. Okay, I think it's pretty cool. All right, well, that's it for this video. And of course, I'll put all the information in the description. And until next time, keep rocking.
        </div>
<div class="tags">
<h2>Tags</h2>
<span class="tag">AWS Trainium</span><span class="tag">Transformer Training</span><span class="tag">Neuron SDK</span><span class="tag">Arcee AMI</span><span class="tag">PyTorch XLA</span>
</div>
<div class="links"><a class="link" href="../../../youtube.html">← Back to YouTube Overview</a></div>
</div>
</body>
</html>
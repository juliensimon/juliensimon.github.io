<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>
<!-- Umami Analytics -->
<script defer src="https://cloud.umami.is/script.js" data-website-id="27550dad-d418-4f5d-ad1b-dab573da1020"></script>
<link rel="dns-prefetch" href="//cloud.umami.is">L Intelligence Artificielle en Java et Scala sur AWS</title>
    <style>
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
            line-height: 1.6;
            margin: 0;
            padding: 20px;
            background-color: #f5f5f5;
            color: #333;
        }
        .container {
            max-width: 800px;
            margin: 0 auto;
            background: white;
            padding: 30px;
            border-radius: 10px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
        }
        h1 {
            color: #2c3e50;
            margin-bottom: 10px;
            font-size: 2.2em;
        }
        .date {
            color: #7f8c8d;
            font-size: 1.1em;
            margin-bottom: 30px;
            font-weight: 500;
        }
        .video-container {
            position: relative;
            width: 100%;
            height: 0;
            padding-bottom: 56.25%; /* 16:9 aspect ratio */
            margin-bottom: 30px;
        }
        .video-container iframe {
            position: absolute;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
            border: none;
            border-radius: 8px;
        }
        .description {
            background: #f8f9fa;
            padding: 20px;
            border-radius: 8px;
            margin-bottom: 30px;
            white-space: pre-wrap;
            font-size: 1em;
        }
        .description a {
            color: #3498db;
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s;
        }
        .description a:hover {
            color: #2980b9;
            text-decoration: underline;
        }
        .transcript {
            background: #f8f9fa;
            padding: 20px;
            border-radius: 8px;
            margin-bottom: 30px;
            white-space: pre-wrap;
            font-size: 1em;
        }
        .transcript h2 {
            color: #2c3e50;
            margin-top: 0;
            margin-bottom: 15px;
            font-size: 1.5em;
        }
        .tags {
            margin-bottom: 30px;
        }
        .tags h2 {
            color: #2c3e50;
            margin-bottom: 15px;
            font-size: 1.5em;
        }
        .tag {
            display: inline-block;
            background: #3498db;
            color: white;
            padding: 6px 12px;
            margin: 4px;
            border-radius: 20px;
            font-size: 0.9em;
            font-weight: 500;
        }
        .links {
            display: flex;
            gap: 20px;
            flex-wrap: wrap;
        }
        .link {
            display: inline-block;
            padding: 12px 24px;
            background: #3498db;
            color: white;
            text-decoration: none;
            border-radius: 6px;
            font-weight: 500;
            transition: background-color 0.3s;
        }
        .link:hover {
            background: #2980b9;
        }
        .link.youtube {
            background: #e74c3c;
        }
        .link.youtube:hover {
            background: #c0392b;
        }
        @media (max-width: 600px) {
            .container {
                padding: 20px;
                margin: 10px;
            }
            h1 {
                font-size: 1.8em;
            }
            .links {
                flex-direction: column;
            }
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>L Intelligence Artificielle en Java et Scala sur AWS</h1>
        <div class="date">December 08, 2017</div>
        
        <div class="video-container">
            <iframe src="https://www.youtube.com/embed/d6KGG4CKfB0" 
                    allowfullscreen 
                    allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture">
            </iframe>
        </div>
        
        <div class="description">Cette session est basée presque exclusivement sur des démos en Java et Scala. Elles vous montrent comment utiliser différents services liés à l'Intelligence Artificielle et au Machine Learning : Polly, Rekognition, Amazon Machine Learning, Spark MLlib sur EMR et Apache MXNet.

✚ Retrouvez tous nos événements : <a href="https://aws.amazon.com/fr/events/" target="_blank" rel="noopener noreferrer">https://aws.amazon.com/fr/events/</a>
✚ Rendez-vous sur notre site internet : <a href="http://amzn.to/2ktrf5g" target="_blank" rel="noopener noreferrer">http://amzn.to/2ktrf5g</a>
✚ Suivez-nous sur Twitter : <a href="https://twitter.com/aws_actus" target="_blank" rel="noopener noreferrer">https://twitter.com/aws_actus</a></div>
        
        <div class="transcript">
            <h2>Transcript</h2>
            Bonjour à tous, rebonjour, toujours en direct de Las Vegas avec beaucoup de problèmes techniques, mais c'est rien. Je pense que ça va mieux se passer maintenant. On a visiblement réglé le problème. On va pouvoir continuer. Donc, dans ce second segment, on va vous donner un avant-goût des sujets dont on va parler à la fin de l'année, c'est-à-dire très bientôt. On a, vous le savez sûrement, une semaine de webinaires consacrés à l'IA, au machine learning et au deep learning du 11 au 15, où sans travers de secret, il risque d'y avoir pas mal de nouveautés à l'IA Reinvent. Donc, je trouvais intéressant juste avant de faire un petit état des lieux, de parler déjà des différents sujets disponibles aujourd'hui et puis ensuite de profiter des prochains webinaires pour rentrer dans les détails.

On va regarder un peu ce dont on va parler, on va faire un passage rapide sur ce que veut dire l'IA chez Amazon. Ensuite, je vais vous montrer comment utiliser deux services d'IA de haut niveau qui s'appellent Polly pour le text-to-speech et Rekognition pour la détection d'objets et la détection de visages. Je vous montrerai également comment utiliser Amazon Machine Learning. Je passerai assez rapidement sur la construction du modèle parce que ça mériterait sûrement un webinar complet. Je vous donnerai les grandes lignes du service et je vous montrerai comment on peut l'invoquer assez facilement. Comment on peut, à partir d'une application Java, invoquer un modèle de prédiction et obtenir des résultats. Et puis, si les dieux de MR sont avec nous, on fera une petite démo de Spark MLlib sur EMR, où on construira un détecteur de spam. Et puis je parlerai très rapidement des Mixnets et du deep learning qui seront couverts en détail, et plus qu'en détail, lors de la semaine IA.

Alors, quelques définitions avant tout pour qu'on parle bien de la même chose. L'IA, c'est un domaine qui vise à construire des applications et des systèmes qui démontrent un comportement humain. Par exemple, des systèmes qui sont capables de parler, des systèmes qui sont capables de comprendre et de traiter le langage naturel. À qui on va parler en langage naturel et qui vont déclencher des actions. Bref, où l'interaction se fait de manière simple. Des systèmes qui sont capables de mener des raisonnements complexes au-delà d'une programmation explicite. Puis des systèmes qui sont capables de montrer une intuition, de prédire, de classer, de donner des résultats probabilistes. C'est vraiment un des grands sujets de l'IA. À l'intérieur de l'IA, il y a un sous-ensemble qui s'appelle le machine learning, qui vise à apprendre aux machines à apprendre des comportements sur la base de jeux de données, sans être programmé explicitement. Vous allez prendre un dataset, vous allez le faire passer à travers un algorithme d'apprentissage, et sur la base de cet apprentissage, votre application, votre système va être capable d'effectuer des tâches complexes, de prédire des résultats sur la base de nouveaux échantillons qu'il n'a jamais vus avant, de classer, de regrouper de l'information, etc. Mais tout ça sur la base d'un dataset qui est structuré, où les features, les variables finalement, sont explicitement visibles.

Dans le cas du deep learning, qui est un sous-ensemble du machine learning, on va faire la même chose, on va faire de l'apprentissage automatique, mais cette fois on va le faire sur des données qui ne sont pas structurées, où les variables ne sont pas visibles. Par exemple, des images. Une image qui a un million de pixels, est-ce que chaque pixel est un feature ? Est-ce que chaque pixel est une variable ? Probablement pas. Est-ce qu'on a besoin d'un million de pixels pour dire que cet animal est un chat ou un chien ? Sans doute pas, il y a sans doute des pixels ou des zones de pixels, des groupes de pixels qui sont plus importants que d'autres, mais on n'arrive pas à le définir explicitement. Donc, on va utiliser une technologie qui s'appelle les réseaux de neurones, qui sera capable de trouver les zones de l'image qui sont importantes pour comprendre que c'est un chien, un chat, un lion, etc.

Amazon utilise l'IA depuis des années, aussi bien sur le site Amazon.com et ses versions internationales, tout ce qui est lié à la personnalisation du contenu, à la recommandation de contenu, etc. Vous vous doutez bien que tout ça s'appuie sur des algorithmes d'IA et de machine learning. De même, la gestion et l'automatisation des entrepôts d'Amazon s'appuient sur de la robotique. Vous avez certainement déjà vu ces petits robots oranges, enfin ils ne sont pas si petits d'ailleurs, qui sont des robots qui transportent les étagères contenant les produits jusqu'aux équipes qui sont chargées de préparer les commandes, qui se déplacent de manière autonome à l'intérieur des entrepôts. Et puis bien sûr, il y a eu des nouvelles catégories de produits, le Kindle, l'Echo, etc., qui utilisent elles aussi l'IA. Et dans le cas d'Echo, évidemment, beaucoup de deep learning pour le langage naturel. Et puis enfin, AWS, qui vous amène, j'ai envie de dire, sur étagère, des services d'IA, de deep learning. Et une fois de plus, je pense que les quelques jours à venir vont continuer à le montrer.

On a plein de clients qui font de l'IA au sens large sur AWS. J'ai parlé de FINRA dans le webinaire précédent, qui fait du machine learning à très, très grande échelle sur des données financières en utilisant l'écosystème Hadoop. On a évidemment Netflix, que tout le monde connaît, qui utilise aussi massivement de la personnalisation, de la recommandation pour vous proposer les meilleurs films, etc. On a beaucoup d'histoires à raconter. Vous trouverez tous ces détails sur le site web et les cas d'études. Si l'un d'entre eux vous intéresse particulièrement, n'hésitez pas à me contacter, à me demander des détails. Sachez que nous, on voit des clients faire de la prod sur le machine learning, sur l'IA, sur le deep learning. On n'est pas dans le prototype, on n'est pas dans l'expérimentation, on est vraiment dans l'utilisation à l'échelle de toutes ces technologies.

Donc, notre stack aujourd'hui, et je pense que ce slide va évoluer beaucoup d'ici 48 heures, se compose ainsi : on a des services de très haut niveau, donc des services on va dire SaaS, où vous vous contentez d'appeler une API sans avoir à être un expert en deep learning ou d'IA. Par exemple, on va le faire tout à l'heure avec Polly, on appelle une API qui va générer à partir d'un texte un fichier son qu'on pourra jouer. Avec Rekognition, on va passer une image et on va détecter des visages, on va détecter le contenu de l'image, etc. On a également un service dont je ne parlerai pas aujourd'hui qui s'appelle Lex, qui permet de construire des chatbots, textes ou voix. Donc, des services, on va dire des API de haut niveau qui sont basés sur du deep learning mais qui ne nécessitent aucune connaissance en deep learning. Au niveau d'en dessous, on a des services managés, donc où l'infrastructure, où vous êtes libéré de la gestion de l'infrastructure mais où vous avez plus de contrôles, plus de possibilités de customiser vos traitements, vos applications, et par exemple avec Amazon Machine Learning, dont on parlera tout à l'heure, ou avec Spark sur EMR, dont on parlera tout à l'heure aussi. Et puis d'autres services comme ECS, qui vous permettent d'exécuter des containers et d'utiliser des jobs de machine learning dans des containers, etc. Et puis encore en dessous, on a le niveau le plus bas, où là vous allez utiliser une des librairies open source comme MXNet, TensorFlow, etc., qui vont vous permettre de faire du deep learning de manière fine où vous allez pouvoir construire des modèles, les entraîner et les déployer dans vos applications. Et évidemment tout en bas, on a de l'infrastructure donc des instances CPU, des instances GPU pour faire l'entraînement des modèles, pour faire la prédiction, et on peut également via des services IoT déployer des modèles sur des équipements IoT voire même les embarquer sur mobile si on en a envie.

On va commencer par les services de haut niveau, donc commençons par Polly, qui est vraiment sans doute le plus intuitif et le plus simple à utiliser également. Polly, c'est un service qui va vous permettre à partir d'une chaîne de caractères de créer un fichier son, tout simplement ce qu'on appelle le texte to speech. Vous allez pouvoir choisir 24 langages et 50 voix, donc des voix hommes, femmes, qui ont toutes ces voix ayant des prénoms, on les verra tout à l'heure dans le code. Donc, vous passez votre chaîne de caractères, vous lui dites voilà, utilise la voix de Brian ou la voix de Jennifer, etc. Et en temps réel, on va générer un fichier son que vous pouvez soit jouer, évidemment, immédiatement dans votre application, soit vous pouvez le stocker et le rejouer plus tard. Vous pouvez utiliser Polly pour faire juste de la génération de son et puis intégrer ces sons au fil des besoins dans vos applis. Ce service, vous le connaissez sans doute déjà, si vous avez déjà vu un Amazon Echo ou utilisé un Amazon Echo, la voix d'Alexa est générée par Polly. Donc, quand vous parlez à un Echo et qu'il vous répond, le son que vous entendez, la voix que vous entendez, est générée par Polly.

On va sauter directement dans le code. Il y a deux façons d'utiliser Polly. On peut lui passer du texte simple, juste du texte. On va voir un exemple par ici. Ici, qu'est-ce que j'ai fait auparavant ? Je me suis connecté à mon client Polly et j'ai construit ma requête en lui disant voilà ma chaîne de caractère, génère-moi un MP3 et fais-le avec la voix de Brian, qui est une des voix anglaises britanniques. Il y a des voix anglaises américaines, britanniques, australiennes, il y a plein de variantes. Cette voix-là, c'est la voix britannique. Donc, ici on passe du texte simple comme on le voit, mais on peut aussi passer du texte enrichi. On peut passer du texte enrichi avec un langage de markup qui s'appelle SSML qui va permettre finalement de préciser la prononciation de certaines parties de la phrase. Par exemple, ici on veut que la date soit prononcée d'une certaine façon, on veut que le numéro de téléphone soit prononcé d'une certaine façon, etc. Donc, on peut jouer sur la façon dont le texte est lu, vous voyez les nombres, les dates, etc., les numéros de téléphone. On peut aussi faire varier la vitesse du discours, on peut faire varier le ton, donc plus grave, plus aigu, etc. On peut agir sur ces trucs-là.

Voilà, donc on va essayer. Je ne sais pas si vous allez l'entendre, sincèrement, parce que moi je vais l'entendre dans mon casque. On va essayer, je vais essayer de vous le faire écouter par mon micro. Ça devrait marcher quand même. ... Voilà, j'espère que vous l'avez entendu. Hugo, tu peux me dire si on l'a entendu ou pas ? J'ai le son au maximum sur le Mac, je sais pas si c'est passé. Sinon, ce n'est pas grave, vous pouvez reprendre ces petits exemples et les copier-coller dans la console AWS, vous pouvez l'essayer tout à l'heure et vous pourrez les jouer. Ok, donc là, je fais cet appel qui s'appelle « Synthesize Speech ». Vous avez vu qu'on avait une deuxième voix ici qui était une voix de femme. Je précise que le texte est du SSML et donc je récupère un MP3. Et puis je le joue après automatiquement avec une librairie quelconque. Et c'est suffisamment rapide et c'est suffisamment temps réel pour qu'on puisse l'intégrer dans une conversation. Une fois de plus, c'est ce que fait Alexa. Vous voyez, l'appel d'API, il est super simple. Derrière, il y a évidemment des modèles de traitement du langage naturel, des modèles d'IA, etc. Mais j'ai envie de dire, à ce stade, on s'en fiche. On veut générer du son à partir d'un texte. Et ça se fait de manière aussi simple que ça. C'est vraiment... J'aime bien le service, c'est rigolo. Alors, je vous incite, il y a des démos dans la console AWS, vous pouvez sélectionner toutes les voix. Il y a évidemment des voix françaises, il y a des voix françaises avec un accent canadien. Il y a beaucoup de voix à essayer. C'est assez marrant à tester. Voilà pour Polly. Vous voyez, c'est vraiment un service hyper simple à utiliser.

Alors, Rekognition. Comme son nom l'indique, c'est un service qui va vous permettre de comprendre du contenu visuel. Vous allez passer une image à Rekognition et il va être capable de faire différentes choses en temps réel avec un niveau d'abstraction qui est le même que Polly, c'est vraiment juste une API. On peut faire ce qu'on appelle de la détection d'objets ou de la détection de scènes, passer une image et dire qu'est-ce que tu vois dans l'image. Il va dire je vois une table, une chaise, je ne sais pas, la plage, des palmiers, ce que vous voulez, avec des scores de confiance. On peut faire de la détection de visages, donc dire, ok, trouve-moi les visages qui sont dans cette image. Donc, il va trouver l'image, il va trouver les visages, il va trouver la position des visages, etc. Vous pouvez lui demander une analyse faciale, donc il va vous dire, c'est un homme, c'est une femme, la tranche d'âge, c'est tant, l'émotion dominante, cette personne porte des lunettes, a une moustache, tout un tas d'attributs sur les visages. On va pouvoir faire des recherches, on peut construire des collections de visages. Par exemple, vous pourriez construire une collection avec les visages des employés de votre société, et puis lorsque quelqu'un arrive le matin devant la porte, avoir une caméra qui prend une photo, compare la photo de cette personne avec la collection de visages connus et puis en fonction de ça, ouvrir ou pas la porte, etc. On va pouvoir trouver des visages qui se ressemblent, on peut faire des similarités, on peut détecter les célébrités, on a indexé une très grande quantité de visages de célébrités, du monde du sport, de la musique, du cinéma, des politiciens, enfin tout un tas de gens connus et donc vous pouvez les reconnaître explicitement. Vous pouvez uploader la photo de votre acteur ou de votre chanteur préféré et le reconnaître. On peut aussi faire la modération de contenu. On a la capacité à détecter du contenu suggestif, explicite, voire pire, et vous comprendrez que je ne ferai pas de démo sur ce sujet aujourd'hui. Ce qui est très utile pour les gens qui font de la modération de contenu sur des sites web, tout ce qui peut être uploadé par des utilisateurs. Il peut y avoir tout et surtout n'importe quoi, donc c'est important d'essayer d'automatiser au maximum ces traitements. Et puis depuis à peu près une semaine, on a une nouvelle fonctionnalité qui permet de détecter du texte dans des images, donc de faire de la reconnaissance de texte dans des images. C'est sorti très récemment.

Un exemple d'utilisation, c'est un client qui a une chaîne de télé américaine qui s'appelle C-SPAN, qui est en gros la chaîne parlementaire aux États-Unis. Leur problème, c'est sur leurs années et leurs années d'archives vidéo d'être capables de retrouver les allocutions d'un speaker donné. Donc, ils veulent pouvoir dire, voilà, quand est-ce que, ici, le sénateur McConnell, quand est-ce qu'il a parlé dans les six derniers mois ? Et donc à petite échelle, c'est un problème qui est simple, mais à grande échelle avec des années de vidéos et quasiment 100 000 personnes à identifier, on voit tout de suite que c'est compliqué. Donc, ils ont créé une collection de visages avec les 100 000 speakers, et ils font passer le flux vidéo, ils extraient des images du flux vidéo, et ils les font passer à travers Rekognition pour détecter les visages des différents speakers et donc ils construisent comme ça un index qui leur permet de dire que telle personne a parlé tel jour à telle heure. Et donc ça marche tellement bien finalement qu'ils ont décidé d'indexer vraiment toutes leurs archives, toutes leurs archives historiques vont être indexées comme ça, vont être passées au travers de Rekognition pour trouver tous les gens qui ont pu parler au Parlement ou au Sénat américain.

Alors, on va faire un petit exemple. Donc, on va commencer par la détection de labels. Il faut que je vous montre l'image avant quand même. Voilà, l'image qu'on va envoyer à Rekognition. Si j'arrive à l'ouvrir, c'est intéressant. C'est amusant, c'est pas grave, ok. Oh, c'est bien, je sens que je vais avoir un nouveau Mac pour Noël. Alors... Voilà l'image. Une image assez compliquée, beaucoup de foules, beaucoup de contenu. On va envoyer cette image à Rekognition. Toujours pareil, on se connecte au client Rekognition. Et on lui dit détecte-moi les labels. Ok. Donc, cette photo est dans un bucket S3. Et détecte-moi, donne-moi 10 labels avec au moins 75% de confiance. Ok. Donc, en avant. Ok. Voilà les labels qui ont été détectés, il doit y en avoir 10, avec les très hauts niveaux de confiance. En gros, on voit des gens qui ont l'air de boire, on voit une foule de gens avec des hommes, des femmes qui boivent de l'alcool, ce qui me paraît une assez bonne définition de cette image. Et donc là aussi, bien sûr, on a des modèles de deep learning qui tournent derrière tout ça, mais vous voyez que tout ce qu'on fait ici, c'est simplement appeler une API de haut niveau et c'est tout. Voilà, et puis ensuite on récupère des résultats qu'on affiche. Donc, service super simple à utiliser, rapide et qui vous permet d'indexer comme ça des grosses collections d'images. Et ici, ce qu'on ferait sûrement, c'est qu'on récupérerait ces labels, on les insérerait dans une base quelque part, et puis après on dirait, je veux voir des... Sors-moi dans ma collection d'images des photos de gens qui boivent de l'alcool, et puis, paf, je pourrais faire une recherche comme ça, hyper rapide sur ma base d'images. Ok ?

Alors, deuxième fonctionnalité que je vais vous montrer, la détection de visage. Donc, ici, toujours pareil, on va utiliser la même image de test et cette fois on va lui dire, voilà, trouve-moi les visages. Donc, c'est tout simple, detect faces request, et puis ensuite on va afficher les informations, on va récupérer la liste des visages qui ont été détectés et puis on va afficher quelques paramètres. Donc, on va afficher la tranche d'âge et c'est tout. Alors en avant, mais il y a beaucoup, vous allez voir, on va voir qu'il y a beaucoup d'autres informations. Donc, même principe, voilà l'image est dans S3. Voilà, il est en train de détecter une tonne, on a remonté tout là-haut, il y en a un, et ben, voilà, donc il a détecté 22 visages, ok, dans cette photo, c'est pas mal. Je crois que j'avais fait le test... Non, ça, on a dit que ça ne marchait pas. Voilà, donc ça, c'est l'image de base, on va dire. Voilà, et le résultat, ça donne ça. Voilà. Donc, ça, bon, voilà. Voilà les 22 visages qui ont été détectés. Donc, vous voyez, il y en a là en bas à gauche, là, qui sont très... Voilà. C'est déformé, mais on les voit bien quand même. Là, on a quelqu'un qui est à moitié masqué, on le voit quand même. Là, on a quelqu'un qui est de loin, de dos, on arrive à le choper. On peut détecter jusqu'à 100 visages maintenant, avant c'était la limite était à 15, maintenant on est monté à 100, ce qui est quand même pas mal. Le résultat est assez sympa. Et donc, pour chaque visage, en fait, on va récupérer alors on affiche la tranche d'âge, mais voilà toutes les infos qu'on a. On a la position du visage, l'âge, est-ce que cette personne sourit, est-ce que cette personne a des lunettes, est-ce que c'est un homme ou une femme, etc. La liste de tous les attributs que vous récupérez pour chaque visage. Et puis on a la position des yeux, la position du nez, la position de la bouche. On sait précisément où est le visage. On a les sourcils. Donc, on a ça 22 fois. C'est assez détaillé. Donc, voilà pour la reconnaissance de visage.

Alors, maintenant, la comparaison de visage. Cette fois, l'idée, c'est de dire voilà une image de référence, trouve cette personne dans la deuxième image que je vais te montrer. On pourrait avoir plusieurs visages des deux côtés. On pourrait faire une comparaison de toutes les paires de visages. Alors, ici, on va regarder mes images. Donc, ça, voilà, ça c'est mon image de référence. Et ça, c'est une autre image qui est beaucoup plus grosse. Je ne sais pas si je peux dézoomer un peu là-dessus, non ? Voilà, c'est pas grave. Voilà, donc là, c'est une image qui a été prise lors de nos événements. Il y a plein de visages, il y a des gens de face, de dos, etc. Effectivement, c'est moi, voilà. Donc, on va essayer de me retrouver dans tout ce truc-là. Ok, comment on va faire ça ? Donc, cette fois, je lance comme parfait. Ok, donc je dis voilà, donc l'image 1, image 2, je récupère un client Rekognition. Alors là, je charge les images localement. On a aussi la possibilité, on peut passer les images dans l'appel d'API. On n'est pas obligé de les mettre dans S3. Donc, là, on charge les trucs et puis on lui dit compare faces, la source image, la target image, et puis trouve-nous des gens qui ressemblent. Alors, on va exécuter ça. Alors, là, c'est un peu plus long parce qu'il faut que j'upload les images, en particulier la deuxième qui est un peu plus grosse. A priori, c'est quand même une meilleure idée d'avoir les images dans S3. Rekognition peut y accéder beaucoup plus rapidement. Mais bon, il y a des cas d'usage où vous êtes obligé d'uploader. Alors, c'est vrai qu'ici... s'il aurait fallu que je la redimensionne. Alors, là, il me dit qu'il a trouvé une correspondance. Il y en a trois qui ne correspondaient pas, mais il y en a une quatrième qui marche. Et en fait, là, il me donne la position relative de visage. Donc, la tête qui correspond à l'image de référence, elle est en gros à 60% en largeur de l'image et 51% en hauteur. Donc, si on regarde cette image là, voilà, donc on est à 60% quelque part par là et si je descends à peu près à mi-hauteur, je suis allé un peu loin, si je descends à mi-hauteur, je suis là, ok, d'accord, donc voyez, il est capable de me retrouver là dedans, alors que bon, là, déjà j'ai l'air idiot, ça c'est comme d'habitude, j'ai la tête un peu baissée, etc. Et effectivement, il y arrive avec un niveau de confiance qui est élevé. Donc, il est vraiment capable de retrouver ma tête. Mais il a aussi évalué sans doute ces deux-là, elle, etc. Et tout ça, c'est un appel d'API à chaque fois, et on n'a pas besoin d'avoir trop... Voilà, ces services de haut niveau basés sur du deep learning, basés sur des modèles, mais simples à utiliser et suffisamment rapides pour être intégrés directement dans les applications et avoir du temps réel avec des latences très correctes. Ici, je suis dans des conditions hostiles pour faire ce test.

Alors, parlons rapidement, enfin, parlons maintenant d'Amazon Machine Learning, donc on va descendre d'une couche. Donc, Amazon Machine Learning, c'est un service où l'infrastructure va être gérée, où on va vous permettre de manière super simple, le mot clé c'est vraiment la simplicité, on va vous permettre de créer des modèles, de les entraîner et de les déployer pour faire des prédictions, et c'est vraiment très volontairement simple. Alors, certains disent simpliste, bon, on peut en discuter, mais c'est vrai que ce service a été conçu pour des cas d'usage simples, typiques. Moi, j'appelle ça l'autoroute, c'est-à-dire que vous avez besoin d'ingérer des données qui sont dans S3 ou dans des fichiers CSV, etc., il y a des milliers de lignes, des dizaines de colonnes, et vous voulez faire une prédiction sur un attribut particulier, il n'y a pas de complexité particulière, c'est juste que vous voulez le faire vite, bien, simplement, souvent, etc. Vous êtes sur l'autoroute, vous n'avez pas un cas d'usage tordu, Amazon Machine Learning va bien fonctionner. C'est un service qui est utilisé en interne depuis longtemps par Amazon et qui a été, on va dire, professionnellement mis à disposition de tous. Donc, aujourd'hui avec Amazon Machine Learning, vous pouvez faire des modèles de régression, donc régression linéaire, donc prédire des valeurs numériques, régression logistique, donc vous pouvez prédire une catégorie, faire de la classification, soit de la classification binaire, donc deux classes, soit de la classification multiclasse, avec plus de deux classes. Et l'un des avantages de ce service, c'est qu'une fois que le modèle est entraîné, vous allez pouvoir le déployer en quelques secondes derrière une API que vous allez invoquer. C'est ce qu'on va faire. Je vais peut-être aller voir directement, je vais vous montrer rapidement la console Amazon ML. Vous montrer l'analyse, heureusement qu'il y a ce petit modèle, ce petit moteur de recherche, c'était bien indispensable. Donc, ici, j'ai ingéré un jeu de données qui est un jeu de données public, qui est un jeu de données financières que je dois avoir là, sauf erreur. Sinon, c'est pas grave, je vous le montrerai ailleurs. Voilà. Donc, c'est un jeu de données qui est pas très gros, qui a 40 000 lignes. Donc, c'est des données sur des données bancaires, ça ressemble à ça. C'est un modèle qui permet de prédire si un client est susceptible d'être intéressé par une offre de prêt ou pas. Si vous voulez plus de détails sur ce dataset, contactez-moi, je vous le donnerai, mais il est public. Donc, on voit qu'on a une vingtaine de colonnes, on a du texte, donc du texte catégorie, vous voyez, on a le métier, on a le statut marital, on a d'autres informations comme ça. Et en gros, sur la base de ça, on a le dernier attribut ici qui est 0 ou 1 pour dire, voilà, c'est des données historiques. Donc, cette personne-là, si elle a un 0, c'est que non, elle n'a pas accepté l'offre. Si c'est un 1, elle a accepté l'offre. Donc, on veut faire cette prédiction-là en disant sur la base de tous ces attributs, est-ce que tel client va être susceptible d'être intéressé ou pas par une offre de produits financiers. Donc, là, on est sur le dataset d'entraînement, donc on a cette valeur-là, mais évidemment, le pull du jeu après, une fois que l'entraînement est fait, ça va être de fournir un échantillon qui aura tous ces attributs-là, et juste de prédire la valeur ici. C'est un dataset assez simple. Comment ça se passe ? Oui, je crois que je l'ai entraîné en Irlande. J'ai ingéré ces données. Les données, vous pouvez les ingérer soit dans la console directement, soit à partir d'S3, soit à partir de RDS ou Redshift, et donc dans le cas de RDS ou Redshift, en fait, vous pouvez écrire une requête SQL qui va aller extraire le dataset, donc c'est assez intéressant. Si vos données sont déjà propres à plat dans des fichiers CSV, très bien, sinon si elles sont en base, vous pouvez écrire une requête SQL et construire le dataset comme ça, c'est ce qu'on appelle une data source. Donc, moi ici, j'ai fourni ma data source à partir de S3, c'est le fichier que je viens de vous montrer, il est splitté automatiquement en deux, une partie qui va servir pour le training, l'apprentissage, une partie qui va servir pour la validation. Vous savez, en machine learning, on coupe toujours en deux, on entraîne sur une grosse partie et on valide sur une petite partie. L'intérêt, c'est de tester le score, la performance du modèle avec des échantillons qui n'ont pas servi à l'entraînement. Donc, on donne de nouveaux échantillons au modèle et on regarde quel score il fait. Donc, j'ai créé mon modèle de classification ensuite. On va essayer d'ouvrir ça ainsi que l'évaluation. Donc, ici, je lui ai dit voilà les différents attributs du modèle et moi ce que je veux, c'est que tu me prédises ce dernier attribut. Donc, je veux que tu me dises cette valeur binaire, finalement, quelle est la probabilité entre 0 et 1 que les clients soient intéressés. Donc, classification binaire, vous voyez, l'entraînement a duré 4 minutes, c'est pas un très gros dataset. J'ai ensuite fait une évaluation de ce modèle, je vois que, alors, c'est des métriques qui sont un peu plus machine learning, si vous n'êtes pas familier, ça ne vous parlera pas. Mais en gros, je peux voir la performance de mon modèle. Je peux voir ce qu'on appelle l'area under curve, qui est une métrique importante pour s'assurer qu'effectivement le modèle a bien appris. Et puis, je peux voir le nombre de faux positifs, le nombre de faux négatifs, etc. Et je peux d'ailleurs optimiser mon modèle pour ces paramètres. Donc, si je dis par exemple, là, on voit que j'ai 268 faux positifs, si je décide que non, les faux positifs, c'est embêtant, j'en veux le moins possible, je peux optimiser, je peux reparamétrer mon modèle en disant voilà, je veux un modèle qui minimise les faux positifs, parce que je n'ai pas envie d'avoir des faux positifs, c'est des gens que je vais appeler pour rien, ça me fait perdre du temps. Donc, c'est un compromis, je peux ensuite régler mon modèle en disant, ok, tu as appris, mais maintenant, pour la prédiction, tu minimises les faux positifs, ou au contraire, tu minimises les faux négatifs. Donc, on peut comme ça régler avec le modèle, voir ces différentes métriques dans lesquelles je ne rentre pas, parce qu'on n'a pas le temps de faire trop de machine learning ici. C'est juste pour vous montrer qu'une fois qu'on a uploadé les données, une fois qu'on a créé le modèle, et tout ça se fait vraiment clic clic clic, il n'y a aucune complexité, on a ces informations à disposition. Et ensuite, ce que j'ai fait, c'est que j'ai publié mon modèle derrière un endpoint. Donc, là, je l'ai fait, mais si je ne l'avais pas fait, il y a un bouton qui dit « Enable real-time predictions ». Vous cliquez dessus et vous avez un endpoint HTTP que vous pouvez invoquer sur lequel vous pouvez demander de nouvelles prédictions au modèle. Et c'est ce que j'ai fait là. Donc, on va basculer dans ça. Ok, donc ici, ce que je fais, c'est que je vais me connecter, je récupère un client machine learning, je vais lister les modèles disponibles, alors ici, je pense qu'il n'y en a qu'un. Et quand j'ai trouvé mon modèle, comme on peut automatiser et faire ça à l'échelle, on peut avoir 100 modèles, 200 modèles, c'est facile de générer des modèles avec les API. Je vais afficher, je vais imprimer quelques informations sur mon modèle, ok, et puis ensuite, je vais faire une prédiction, donc je crée une requête qui va être destinée à ce modèle là et à ce endpoint là, ok, là, je construis l'échantillon de données, donc c'est l'équivalent de la ligne dans le fichier que vous avez vu tout à l'heure, où il y a les 20 ou 21 attributs, ok, et ce que je veux maintenant, c'est la probabilité que cette personne là soit intéressée ou pas, donc je fais la prédiction, ok, et je vais afficher, c'est tout en bas un petit peu, voilà, et je vais afficher le résultat de la prédiction. Donc, là, j'ai uploadé des données, j'ai entraîné un modèle, je l'ai optimisé un petit peu, je l'ai déployé derrière mon API, et je peux l'invoquer. Tout ça sans gérer la moindre infrastructure. On va exécuter ça. Donc, on va voir les informations sur le modèle. Ok. Donc, je vois les infos sur le modèle, je vois l'emplacement de ma data source, je vois que c'est un classifieur binaire, etc. Et puis, là, je vois ma prédiction, donc là, ma prédiction brute c'est 0,02, donc 2%, c'est très bas. Donc, avec le seuil que j'ai défini dans mon modèle, bon, évidemment, ce 0,02 ça devient 0, et donc ma prédiction est négative pour cet échantillon là. Ok ? Le SLA sur Amazon Machine Learning, c'est quand vous faites une prédiction depuis C2, à l'intérieur d'AWS, vous avez une réponse en moins de 100 millisecondes. Ici, je passe par le wifi de l'hôtel, internet jusqu'à l'Irlande, aller-retour, donc on est sur des délais énormes. Mais une fois de plus, à l'intérieur de l'infra AWS, c'est moins de 100 millisecondes. Donc voilà un exemple d'invocation de modèles sur Amazon Machine Learning, sans gérer d'infrastructures et sans trop se battre sur la construction du modèle. Cet exemple est en fait le tutoriel Amazon Machine Learning qui est dans la doc, donc je vous incite à le refaire si vous êtes intéressé. Vous allez récupérer les données, les uploader, construire le modèle, etc. Puis ensuite vous pouvez prendre ce code et faire des prédictions. C'est le tuto officiel qui est dans la doc d'Amazon ML.

On avance. Maintenant, jetons un œil à Spark et à Spark ML. On a vu un peu EMR tout à l'heure, donc je rappelle juste, EMR c'est l'environnement managé, c'est la capacité à démarrer un cluster en quelques minutes, à le redimensionner, etc. Vous choisissez entre une distribution open source, qui est en gros la distribution Apache, ou MapR, qui est un fournisseur de distribution avec des fonctionnalités supplémentaires. Et puis vous avez tous les avantages habituels, l'élasticité, la sécurité qui est gérée, le paiement à l'heure, et je crois que maintenant on est sur EMR, on est même au paiement à la seconde, depuis qu'on est passé à la seconde sur EC2. Je suis à peu près sûr que ça s'applique également à EMR. Vous pouvez utiliser des instances spot, donc des instances que vous allez acheter aux enchères et qui vont être, c'est toujours difficile de donner des ordres de grandeur, mais vous pouvez arriver à des économies de l'ordre de 70-80%. Ce qui est intéressant dans le cas précis d'EMR, c'est que entre un cluster de 10 nœuds qui va vous donner la réponse en 10 heures et un cluster de 100 nœuds qui va vous donner la réponse en une heure, a priori on préfère le deuxième. Surtout que si vous utilisez des instances spot, il est probable que le cluster de 100 nœuds vous coûte à peu près le même prix que le cluster de 10 nœuds avec le tarif à la demande. Donc on a vraiment beaucoup de clients qui utilisent massivement spot sur EMR pour se créer des très gros clusters à tarifs réduits et avec des résultats qui arrivent beaucoup plus vite. Puis ensuite ils éteignent le cluster et ils recommencent. Un autre avantage d'Amazon EMR, c'est que ce n'est pas juste EMR, c'est pas juste Hadoop enrobé par AWS, c'est aussi l'intégration avec les back-ends. C'est-à-dire que vos données, bien sûr, peuvent être dans HDFS. Vous pouvez copier vos données sur le HDFS du cluster et travailler avec vos données localement. Mais dans la plupart des cas, ces données vont déjà vivre quelque part dans AWS et on a un ensemble de connecteurs qui vont vous permettre d'aller chercher ces données là où elles sont, sans avoir à les copier sur le cluster, ce qui est un gain de temps, et sans avoir à dimensionner le cluster de manière telle qu'il ait suffisamment de stockage. Vous savez que chez AWS, on aime bien décorréler compute et storage, donc le problème d'Hadoop, c'est que les deux sont corrélés, que parfois on est obligé d'ajouter des nœuds pour avoir davantage de stockage, donc ce n'est pas hyper efficace. En faisant ainsi, en ayant un cluster qui ne fait que du calcul dimensionné en fonction de votre job, et en ayant les données dans des back-ends comme DynamoDB, S3, etc., vous pouvez décorréler les deux. Donc on a un connecteur pour DynamoDB, on a un connecteur pour RDS, on a un connecteur pour Elasticsearch, on a un connecteur pour S3, qui est évidemment la source favorite pour les données, surtout des données non structurées que vous allez transformer avec Hadoop. Vous pouvez avoir des centaines de teraoctets dans S3 ou des pétaoctets dans S3 que vous allez cruncher avec EMR. On a des connecteurs pour faire du streaming vers Kinesis, donc vraiment tous les back-ends favoris ont un connecteur, y compris Redshift.

On va faire un petit exemple avec Spark. Donc, comment est-ce qu'on crée un job Spark sur EMR ? On peut utiliser ce qu'on appelle les steps, c'est-à-dire qu'on peut, quand on crée un cluster EMR, définir des étapes qui vont être lancées automatiquement dès que le cluster est prêt à travailler. Donc, on peut dire, dès que le cluster est prêt, tu vas chercher ça dans S3, tu lances ce job Scala, etc. Donc, on peut, comme ça, automatiser complètement le lancement des jobs au lancement du cluster. On peut évidemment le faire avec Lambda, on peut tout faire avec Lambda, donc pourquoi pas déclencher via Lambda des exécutions de JobSpark sur la base d'événements quelconques dans votre infra. Et puis on peut aussi s'interfacer avec les services comme Glue ou Data Pipeline pour gérer et ordonnancer des transactions de données, y compris des régions différentes, vers un cluster EMR. Ou alors, après, vous faites comme vous voulez, c'est-à-dire que vous utilisez des outils open source comme Airflow, Luigi ou toute autre forme d'ordonnanceur sur EC2 pour exécuter vos jobs. Et puis on a aussi, c'est bon à savoir, on a Zeppelin qui est un notebook installé sur EMR donc qui vous permet de définir des notebooks Spark et de les exécuter. On va essayer de faire un petit exemple, on va retourner dans EMR, on va se connecter. On est connecté sur mon cluster EMR et donc on va faire du Spark. On va faire un classifieur de spam. On va déjà importer tout un tas de trucs. Voilà, ça c'est tous les algos dont on va avoir besoin. Voilà notre dataset, donc je l'ai copié dans S3, mais je peux vous le montrer là. Donc j'ai des messages, chaque message fait une ligne. Donc ça c'est chaque message ici, c'est des messages de spam divers et variés. Je ne sais pas combien il y en a, on va les compter. 747. Et là on a des messages, on va dire, normaux. En tout cas des messages qui ne sont pas considérés comme du spam. Même s'ils contiennent des âneries et du langage SMS, ce sont des vrais messages. Et là on en a 4800. Ce qu'on veut c'est construire un modèle qui va être capable de dire si c'est bien ou pas. Donc on va définir notre dataset, voilà, ok, donc le spam et le ham, comme on dit. Ensuite on va créer un modèle, donc on va prendre, en fait on va créer 1000 features, on va créer un modèle à partir de ces datasets. On va extraire les 1000 mots qui sont les plus discriminants pour dire que c'est du spam ou pas. Voilà. Si vous n'êtes pas familier de ce cas-là, je pense qu'à ce stade vous êtes horrifié. Voilà, pas de soucis, je vais expliquer les grandes lignes et puis si vous voulez creuser, vous n'aurez pas de mal à le faire. Ensuite, je vais étiqueter les exemples qui sont des exemples de spam avec le label 1, donc 1 ça veut dire c'est positif, c'est du spam. Les features qui correspondent, donc les mots les plus fréquents qui correspondent à des vrais messages, on va les étiqueter à zéro. Pour l'instant tout va bien. Ensuite on va mélanger les deux, puisque là on avait deux datasets séparés, donc on les fusionne et puis on les split en 80-20, un peu comme on a fait tout à l'heure, donc on va en prendre 80% pour faire le training et puis 20% pour faire la validation. Ok, ça c'est simple. Et maintenant on va faire un premier modèle. Donc on va utiliser un algorithme qui s'appelle SVM, Support Vector Machines, qui est une technique de machine learning pour faire de la classification. Donc on va entraîner un modèle pendant 100 itérations sur ce dataset-là, sur le dataset de training. En espérant que ce ne soit pas trop lent. Voilà, donc il a entraîné, donc pendant 100 itérations, il a appris à reconnaître les features positifs, donc les features spam versus les features non-spam. Je répète, les features étant, on va dire, les mêmes mots les plus fréquents, les plus fréquemment associés à du spam ou pas. Donc maintenant on va lui demander de prédire, donc on va prendre notre dataset, on va le passer à travers ce modèle, et on va mesurer la précision, c'est-à-dire le nombre de prédictions correctes. Cette fois j'utilise le test data, et donc pour chaque échantillon du dataset de test je fais une prédiction, je regarde quel est le score prédit et puis je le compare avec le vrai score. Voilà, donc là il a une précision de 91,9%, ce qui est pas mal. Donc là il a pris 20% des... combien on avait dit qu'on en avait ? 5000 et quelques, un truc comme ça, c'est ça ? On en avait 5500, donc là il a mis l'échantillon du spam et du non-spam, puis on les passe à travers le modèle et puis on voit la précision. Donc vous voyez, ça marche pas mal. Il y a plein d'algos possibles, on va pas tous les faire. Après c'est juste des variations, j'ai essayé différents algos, donc on fait la même chose à chaque fois, on va peut-être juste faire un classifieur bayésien, qui est un grand classique, donc on va l'entraîner, donc voilà, ça va vite, et on va prédire, donc même chose, on fait la même chose c'est juste avec un autre algo, un autre modèle. Là j'ai une précision qui est meilleure, 96,44%. Vous voyez, c'est intéressant de tester différents algos parce que tous n'ont pas forcément les mêmes performances. Et maintenant on va faire un test, donc j'ai inventé un message, je pense qu'on reçoit tous ça par mail, je tiens à préciser que j'ai absolument rien contre les braves habitants du Nigéria, c'est juste que voilà, il y a des gens là-bas qui aiment bien nous spammer. Et donc un message qui est manifestement du spam et puis un qui est manifestement pas du spam. Ok ? Donc je les ai définis. Voilà. Ok, donc vous voyez, on construit les... Voilà, on voit les features. On voit les... parmi les mille mots, les mille mots les plus fréquents, donc on voit le nombre, on voit l'identifiant des mots qui sont présents dans le message et puis est-ce que ces mots là ils sont associés à du spam ou pas, et puis on va prédire. Donc on va prédire l'exemple positif, voilà, donc là celui-là il est prédit à 1, donc le Nigéria manifestement c'est du spam, l'autre il est prédit à 0, donc manifestement c'est pas du spam. Le premier message est considéré comme du spam, le deuxième n'est pas considéré comme du spam. Voilà un petit exemple de ce cas-là pour changer, avec cette librairie qui s'appelle Spark ML, qui a plein d'algos sur étagère, on les a vus là, il y a plein d'algos sur étagère pour faire ici de la classification, mais il y a plein d'autres problèmes qui sont adressables avec cette librairie. Et puis on le fait sur un cluster Spark, alors évidemment là on a un petit dataset, mais imaginez que vous êtes hébergeur et vous avez des dizaines et des dizaines de millions de mails à classer par jour, ils sont dans S3 et vous allez les traiter comme ça à partir d'un dataset dans S3 comme j'ai fait là. Et puis vous allez pouvoir les classifier. Et puis si vous avez besoin de mettre 50 nœuds pour faire ça, parce que c'est un gros dataset, vous faites ça. Donc la combinaison de MR, la scalabilité de MR, l'intégration avec S3 et les autres back-ends, et puis l'utilisation d'outils open source comme ça, ça permet d'avoir des solutions, à mon avis, assez efficaces. C'est sympa, c'est flexible.

On va faire un petit mot sur MXNet. Hugo, combien il reste de temps, s'il te plaît ? Si tu peux me confirmer, parce que j'ai pas l'horloge sous le nez. D'accord, on est en retard. Ok, merci. Comme d'hab, quoi. Mais avec le décalage horaire, en fait, je suis dans les temps, non ? Ça ne compte pas. Donc, MXNet, on en parlera en détail pendant la semaine de l'IA. Très vite, qu'est-ce que c'est qu'MXNet ? C'est une librairie open source qui vous permet de définir, d'entraîner et de réutiliser des modèles de deep learning, des réseaux de neurones avec toutes sortes d'architectures. On rentrera dans les détails d'ici quelques semaines. Elle est la librairie préférée par AWS pour les raisons qui sont indiquées ici, parce qu'elle supporte beaucoup plus de langages que les librairies concurrentes. Elle fonctionne bien dans des environnements cloud et dans des environnements IoT. Elle est scalable, on peut utiliser des dizaines voire des centaines de GPU pour faire du training sur des gros datasets. On peut faire du training distributed sur plusieurs instances, etc. Tout ça est assez facile à mettre en œuvre et c'est important pour nos clients. Et puis en plus le projet maintenant fait partie d'Apache, donc il est indépendant, c'est pas un projet AWS, c'est un projet qui appartient à Apache donc à la communauté Apache et c'est la meilleure garantie d'indépendance. On peut faire de l'entraînement, mais comme on est en retard, je ne vais pas le faire aujourd'hui et puis de toute façon on va rentrer beaucoup dans les détails lors de la semaine de l'IA. Je vous montrerai comment entraîner des modèles pour faire de la reconnaissance d'image, etc. Ici, je vous montre un petit exemple en Scala, on peut faire aussi du Scala pour faire du training, en tout cas il y a une API Scala dans MXNet, il y a une API C++, il y a une API Python, enfin il y a tout un tas de langages. Donc si vous êtes curieux, allez voir les liens qui sont là. Mais voilà, on va passer au moins une journée pendant la semaine de l'IA spécifiquement sur MXNet. Donc on rentrera énormément dans les détails. Il y a un feature récent qui permet depuis une semaine d'utiliser des instances, on avait déjà les instances GPU dans EMR, on avait déjà les G2, mais les P2 et les P3 maintenant sont supportés avec les nouvelles générations de GPU, et la distribution EMR inclut MXNet, ce qui est intéressant parce que ça veut dire que si vous voulez faire du Spark, pour faire par exemple de l'ETL, de la transformation de données, et qu'ensuite sur la base de ces données transformées, vous voulez faire un apprentissage de deep learning, maintenant vous pouvez le faire sur la même infrastructure. En fait, vous pouvez créer un cluster EMR, faire l'ETL avec Spark sur le cluster et ensuite faire le training avec MXNet sur ce cluster qui peut être composé d'instances GPU. Donc je trouve que c'est une super idée parce qu'avant c'est vrai qu'on avait tendance à avoir deux infrastructures différentes, une pour l'ETL, une pour le deep learning, et maintenant on peut faire les deux. Donc c'est plutôt intéressant. On a un dernier truc qui est expérimental, mais qui peut intéresser certains d'entre vous, qui est dans MXNet, la capacité à démarrer des Jobs Spark. Donc en fait, vous pourriez même piloter l'ensemble à partir d'MXNet. Donc avoir votre programme MXNet qui lance les Jobs Spark d'ETL sur le cluster, et puis qui sur la base des résultats ensuite lance le training. C'est des cas d'usage un peu avancés, mais ça peut intéresser certains d'entre vous. Voilà, donc une fois de plus, c'est la tête de la stack à l'instant T. Dans les 48 heures, ça va beaucoup, 72 heures, ça va changer. Voilà plein de ressources pour commencer à découvrir le sujet. Le repo GitHub sur lequel j'ai mis le code. Donc la semaine de l'IA dont j'ai parlé, du 11 au 15 décembre. On est en train de battre le record d'inscription, alors j'espère qu'on battra aussi le record de présence. Il va y avoir un monde fou. Si vous êtes déjà inscrit, merci beaucoup. Invitez vos potes. Plus il y a de fous, plus on rit. Plus il y aura de questions et plus ça sera intéressant pour tout le monde. N'hésitez pas à partager ce lien avec vos collègues ou vos amis qui ont un intérêt pour l'IA, le machine learning, le deep learning. Ça va vraiment être une semaine assez sympa et puis on aura toutes les nouveautés de reInvent que vous pouvez suivre en live donc les keynotes commencent, il y en a une ce soir, mais les grosses keynotes auront lieu mercredi matin et jeudi matin, donc même avec le décalage horaire pour la France, vous pouvez les regarder en fin d'après-midi sur le live et puis si vous n'avez pas le temps, pas de soucis, vous pourrez les revoir sur YouTube dans la foulée. Pour voir toutes les annonces, vous verrez que sur l'IA, il risque d'y avoir des choses intéressantes. Voilà, merci beaucoup. Je suis content que ce deuxième webinaire se soit passé de manière plus satisfaisante. Je suis désolé que le premier a été compliqué. On va voir comment on peut régler ça. Est-ce qu'on a encore un peu de temps pour les questions, Hugo ? Ou est-ce qu'on est trop en retard ? Ah oui, et puis le sondage, pardon. N'oubliez pas le sondage comme d'habitude, mais vous avez l'habitude maintenant du sondage. Alors, les questions, les questions. Non. Alors une question de Jérôme. Nos datas sont essentiellement sur Elasticsearch et S3. Quel workflow peut-on imaginer pour mettre à profit EMR ? Il y a des connecteurs pour les deux, comme je l'ai indiqué. En toute modestie, c'est difficile de vous donner des conseils intelligents en ne sachant pas précisément ce que vous avez dans Elasticsearch et ce que vous avez dans S3. Mais bon, dans l'hypothèse où vous avez besoin de faire de l'ETL sur ces données, de les réconcilier, j'imagine que vous allez faire, dans Elasticsearch certainement, vous avez des logs et vous allez faire de l'agrégation ou du filtrage spécifique. Les logs et puis dans S3 on a plutôt des données plus métiers, j'imagine, et vous avez envie de préparer les deux pour faire du Spark ou du MapReduce. Moi ce que je ferai, c'est que je laisserai les données là où elles sont, en plus dans Elasticsearch généralement on a tendance à empiler donc il y a sûrement beaucoup de choses et S3 aussi, donc je laisserai les données là où elles sont, j'utiliserai les connecteurs pour que mon job Spark aille les chercher, un peu comme je suis allé chercher mes fichiers dans S3 avec mon programme Scala. Puis ensuite je ferai mon ETL, je ferai mon filtrage, je ferai mon agrégation, je ferai mes jointures, mais bon on se comprend. Et puis ensuite à partir de ça, vous déclenchez vos jobs. Soit vous voulez juste faire de l'ETL et puis après remettre les données, par exemple dans Redshift pour faire de l'analytics, soit vous voulez vraiment faire du machine learning et à ce moment-là vous êtes sur EMR et vous pouvez lancer votre job. Donc c'est ce qu'on voit chez les clients, c'est-à-dire qu'ils ont des données éparpillées dans les différents endroits parce que comme je l'ai dit dans le premier webinaire, on a plein de sources possibles et plein de types de services possibles pour le backend, l'analytics, etc. Donc c'est très bien, utilisez-les tous. Et puis c'est à charge à nous dans EMR de vous mettre les connecteurs pour que vous alliez chercher vos données au bon endroit et que vous puissiez les traiter, les agréger et ensuite republier les résultats vers RDS ou Redshift, qui sont souvent des destinations utilisées pour ça, et puis éteindre le cluster et recommencer la fois d'après, etc. Le fait de ne pas laisser les données sur le cluster, de ne pas les copier sur le cluster, ça permet aussi d'éteindre les clusters sans vergogne, de se dire, mon cluster, c'est juste un crunch, il ne fait que cruncher les données. Donc mes données sont dans les back-ends, je démarre un cluster EMR quand j'ai besoin de cruncher et je prends les résultats, je les remets dans un autre back-end et puis j'éteins le cluster. Voilà, c'est ça, c'est vraiment le pattern qu'on voit partout.

Une question de Fred qui veut compter des vélos qui passent devant la caméra avec Recognition. Recognition à l'instant T ne travaille que sur des images fixes, ce qui n'a pas empêché Syspan de traiter des vidéos. Donc on peut extraire, on peut prendre une image par seconde ou cinq images par seconde, et puis faire de la reconnaissance sur les photos. Donc à l'instant T sur la base des services existants, je fais. On en reparle dans 48 heures. Une question de Philippe. Est-ce qu'on trouve sur la Marketplace des produits basés sur des briques que je viens de présenter pour accéder à des services d'IA en mode PaaS et IaaS ? Alors, je n'en doute pas. Le truc c'est qu'il y a plus de 3500 produits sur la Marketplace. Donc oui, je suis convaincu qu'il y a des solutions sur la Marketplace qui vont utiliser EMR, qui vont utiliser Spark, qui vont utiliser peut-être même Amazon Machine Learning. On a des partenaires qui s'appuient sur ces services-là pour construire leurs propres offres. Après, il faut fouiller, ça change tous les jours. Donc oui, je ne doute pas qu'il y en ait. C'est une bonne question. Alors une question de Naja, le fonctionnement de Recognition suppose que la banque d'images soit à proximité des instances ? Alors oui, a priori dans S3, le bon endroit c'est S3, maintenant on a vu aussi qu'on peut inliner les images dans la requête. Est-ce que le service fonctionne sur des vidéos en streaming temps réel ? J'ai déjà répondu. Aujourd'hui, Recognition marche sur des images. C'est le scénario de Syspan, on est plutôt sur de l'indexation de contenu archivé. Maintenant, pour ceux qui étaient au Summit précédemment, de Paris cet été, il y a un de nos partenaires qui s'appelle Corexpert à Lyon qui avait fait une démo justement de Recognition sur un flux vidéo, donc où il faisait de l'échantillonnage, il filmait la salle et il comptait le nombre de gens, il détectait les émotions, etc. Donc on arrive à le faire, voilà, maintenant l'idéal serait d'avoir un service qui sait faire ça sur de la vidéo. Mais pour l'instant, ça n'est pas le cas de Recognition. Est-ce qu'on a encore quelques questions, Hugo ? Ou est-ce qu'on stoppe là ? Tu me dis. Ok, on est très très très en retard. J'ai quelques événements prévus à re-invent dans peu de temps. Donc il va falloir que je me sauve. Merci beaucoup, je vous donne rendez-vous le mois prochain, c'est même dans 15 jours, pour la semaine IA, du 11 au 15 décembre. Deux webinaires par jour. Machine Learning, Machine Learning as a Service. On reviendra sur EMR, on reviendra sur Spark. On parlera de toutes les nouveautés de reInvent. On parlera de MXNet, deep learning, etc. Donc si ces sujets vous intéressent, c'est un rendez-vous inratable. Voilà, écoutez, merci beaucoup, je vous souhaite une bonne soirée et pour moi la journée continue. Voilà, merci beaucoup, à bientôt.


        </div>
        
        <div class="tags">
            <h2>Tags</h2>
            <span class="tag">MachineLearning</span><span class="tag">DeepLearning</span><span class="tag">AWSservices</span><span class="tag">ArtificialIntelligence</span><span class="tag">PollyRekognitionDemo</span>
        </div>
        
        <div class="links">
            <a href="https://www.julien.org/youtube.html" class="link">Julien.org - Youtube</a>
            <a href="https://youtube.com/@juliensimon.fr" class="link youtube">Julien's YouTube channel</a>
        </div>
    </div>
</body>
</html>
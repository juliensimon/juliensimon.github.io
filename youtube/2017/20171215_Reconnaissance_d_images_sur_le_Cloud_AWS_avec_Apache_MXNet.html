<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Reconnaissance d images sur le Cloud AWS avec Apache MXNet</title>
    <style>
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
            line-height: 1.6;
            margin: 0;
            padding: 20px;
            background-color: #f5f5f5;
            color: #333;
        }
        .container {
            max-width: 800px;
            margin: 0 auto;
            background: white;
            padding: 30px;
            border-radius: 10px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
        }
        h1 {
            color: #2c3e50;
            margin-bottom: 10px;
            font-size: 2.2em;
        }
        .date {
            color: #7f8c8d;
            font-size: 1.1em;
            margin-bottom: 30px;
            font-weight: 500;
        }
        .video-container {
            position: relative;
            width: 100%;
            height: 0;
            padding-bottom: 56.25%; /* 16:9 aspect ratio */
            margin-bottom: 30px;
        }
        .video-container iframe {
            position: absolute;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
            border: none;
            border-radius: 8px;
        }
        .description {
            background: #f8f9fa;
            padding: 20px;
            border-radius: 8px;
            margin-bottom: 30px;
            white-space: pre-wrap;
            font-size: 1em;
        }
        .description a {
            color: #3498db;
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s;
        }
        .description a:hover {
            color: #2980b9;
            text-decoration: underline;
        }
        .transcript {
            background: #f8f9fa;
            padding: 20px;
            border-radius: 8px;
            margin-bottom: 30px;
            white-space: pre-wrap;
            font-size: 1em;
        }
        .transcript h2 {
            color: #2c3e50;
            margin-top: 0;
            margin-bottom: 15px;
            font-size: 1.5em;
        }
        .tags {
            margin-bottom: 30px;
        }
        .tags h2 {
            color: #2c3e50;
            margin-bottom: 15px;
            font-size: 1.5em;
        }
        .tag {
            display: inline-block;
            background: #3498db;
            color: white;
            padding: 6px 12px;
            margin: 4px;
            border-radius: 20px;
            font-size: 0.9em;
            font-weight: 500;
        }
        .links {
            display: flex;
            gap: 20px;
            flex-wrap: wrap;
        }
        .link {
            display: inline-block;
            padding: 12px 24px;
            background: #3498db;
            color: white;
            text-decoration: none;
            border-radius: 6px;
            font-weight: 500;
            transition: background-color 0.3s;
        }
        .link:hover {
            background: #2980b9;
        }
        .link.youtube {
            background: #e74c3c;
        }
        .link.youtube:hover {
            background: #c0392b;
        }
        @media (max-width: 600px) {
            .container {
                padding: 20px;
                margin: 10px;
            }
            h1 {
                font-size: 1.8em;
            }
            .links {
                flex-direction: column;
            }
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>Reconnaissance d images sur le Cloud AWS avec Apache MXNet</h1>
        <div class="date">December 15, 2017</div>
        
        <div class="video-container">
            <iframe src="https://www.youtube.com/embed/RgORMZKEmJI" 
                    allowfullscreen 
                    allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture">
            </iframe>
        </div>
        
        <div class="description">Slides : <a href="http://chilp.it/69bf238" target="_blank" rel="noopener noreferrer">http://chilp.it/69bf238</a>

Cette session s’inscrit dans la continuité du jour 3 et se concentrera sur la reconnaissance d’image. Nous y voyons comment entrainer des modèles sur le jeu de données ImageNet, qui compte plus d’un 1,2 million d’images dans 10,000 catégories. Ce faisant, nous nous intéressons aussi bien à la performance des modèles qu’à la performance de l’infrastructure nécessaire à son entraînement.

✚ Retrouvez tous nos événements : <a href="https://aws.amazon.com/fr/events/" target="_blank" rel="noopener noreferrer">https://aws.amazon.com/fr/events/</a>
✚ Rendez-vous sur notre site internet : <a href="http://amzn.to/2ktrf5g" target="_blank" rel="noopener noreferrer">http://amzn.to/2ktrf5g</a>
✚ Suivez-nous sur Twitter : <a href="https://twitter.com/aws_actus" target="_blank" rel="noopener noreferrer">https://twitter.com/aws_actus</a></div>
        
        <div class="transcript">
            <h2>Transcript</h2>
            Au programme cet après-midi, un premier webinar pendant lequel nous allons nous concentrer sur le traitement d'images avec le Deep Learning. Nous allons étudier différents problèmes, comme la classification et la génération d'images. Vous verrez que c'est une technologie assez étonnante. Pour cela, nous utiliserons différentes librairies, notamment MXNet et Keras.

Dans le deuxième webinar, vous verrez que je suis accompagné de mes gadgets ici. Je ne sais pas si vous les avez à l'écran. Pas encore, peut-être plus tard. J'ai mon robot Raspberry Pi, un Amazon Echo et surtout une DeepLens, cette fameuse caméra deep learning qui a été lancée à re:Invent. Il est bien probable que ce soit la première fois qu'on en parle en vidéo, car je n'ai pas vu beaucoup de vidéos officielles sur le sujet pour l'instant. Donc, c'est votre jour de chance. Très bien, on va attaquer directement le premier webinar.

Nous allons commencer par un tout petit rappel sur un service qui est sorti l'année dernière à re:Invent, qui s'appelle Amazon Rekognition. C'est un service qui fait de la reconnaissance d'images. Nous allons voir dans quel cas on peut utiliser Rekognition et dans quel cas il faudra construire une solution spécialisée, par exemple avec MXNet. Ensuite, je vais vous donner quelques exemples d'applications de traitement d'images, vous montrer les grandes classes de problèmes qu'on peut traiter en deep learning. Je vais le faire en vous montrant des projets GitHub, des projets open source, qui ont implémenté des réseaux à l'état de l'art. Vous verrez qu'il y a des choses assez spectaculaires.

Ensuite, nous jetterons un œil plus attentif à une opération dont nous avons parlé hier et qui est au cœur des réseaux de neurones lorsqu'on parle de traitement d'image : l'opération de convolution. Nous avons parlé hier de convolutional neural networks, qui sont basés sur cette opération de convolution, une opération mathématique. Nous allons essayer de regarder un peu plus en détail ce qu'elle signifie et comment elle fonctionne. Bien sûr, nous ferons ensuite des démos. Nous utiliserons des modèles préentraînés, ferons du fine tuning, c'est-à-dire réentraîner un modèle préentraîné pour le spécialiser, et quelques autres choses. Nous verrons tout cela en fonction du temps qui me reste.

Un petit rappel rapide sur Rekognition. C'est un service qui a été lancé à re:Invent 2016 et qui permet de faire différentes choses, comme la détection d'objets, de scènes, la détection de visages et leurs attributs, la reconnaissance de célébrités, la comparaison de visages, la modération de contenu, et depuis peu, la reconnaissance de texte dans les images. Depuis re:Invent, on peut même faire de l'analyse vidéo, mais je n'en parlerai pas aujourd'hui, seulement demain matin. Ce service est un service de haut niveau pour lequel il suffit d'invoquer une API, comme une API de détection de scènes, une API de détection d'objets, une API de comparaison de visages, etc.

Faisons un petit exemple. Ici, j'ai des images et un petit script en Python. Par exemple, pour la détection, je vais vous montrer l'API en question, qui est vraiment très simple. En Python, si on veut faire une détection de visage avec Rekognition, on va mettre une image dans un bucket S3 ou la passer en ligne, puis on va appeler « detect_faces ». On peut faire de la comparaison de visages, de la détection de labels, etc. Allons-y, faisons un petit exemple. On va faire une détection. Je vais indiquer le bucket dans lequel mon image est stockée. Voilà, et très rapidement, j'ai une série de labels avec un score de confiance. Dans cette image, il y a manifestement des gens qui boivent de l'alcool, et il y a des hommes et des femmes. Très bien, qu'est-ce qu'il y a dans cette image ? Effectivement, c'est Oktoberfest, c'est une assez bonne description. La reconnaissance de scène a été faite de manière satisfaisante, et j'ai demandé une détection de visage, donc on détecte ici 22 visages. Pour chacun des visages, je vais avoir une série d'attributs, comme le sexe, l'âge, une détection d'émotions. Ici, je n'ai imprimé à l'écran que quelques paramètres, mais il y en a beaucoup plus. Donc, voilà un exemple très rapide de Rekognition. On prend une image, on la passe à une API, et le traitement est fait. On peut faire de la détection d'images, de la modération, de la détection de visages, de la reconnaissance de célébrités, etc.

Si le problème business que vous essayez de résoudre correspond bien à l'un de ces cas, ma recommandation est de ne pas chercher plus loin. Il suffit d'utiliser cette API, et vous détecterez les objets, les personnes, etc. Il n'est pas nécessaire d'être un expert en deep learning pour utiliser ce service. Dans certains cas, ces services ne vous conviendront pas. Par exemple, si vous devez détecter des objets particuliers qui ne sont pas des objets de la vie de tous les jours, comme des pièces mécaniques ou des images médicales, vous devrez vous orienter vers une solution sur mesure que vous pouvez faire avec MXNet ou une autre librairie. Mais gardez en tête que vous avez ces services et ces possibilités sur étagère, et si elles vous conviennent, c'est vraiment la façon la plus simple et la plus instantanée d'obtenir une réponse à votre problème.

Si vous êtes amené à construire quelque chose, nous avons beaucoup parlé d'AmixNet hier, mais au cas où quelques personnes ne nous auraient pas rejoints, je rappelle qu'AmixNet est une librairie open source pour le deep learning, qui est la librairie de prédilection d'AWS. Elle nous permet de construire tout type de réseau, toute architecture de réseau. Nous allons en voir différentes tout à l'heure. Elle supporte différentes API, que ce soit en Python, C++, R, etc., donc un choix de langage assez large. Elle fonctionne bien dans des environnements cloud, mais aussi dans des environnements IoT. Nous verrons cela plus tard avec le Raspberry Pi. Elle scale de manière très efficace si vous avez de gros datasets qui nécessitent des durées d'entraînement particulièrement longues. Vous pourrez ajouter des GPU supplémentaires qui seront pris en compte par MXNet, et vous aurez un scaling quasi linéaire. Si vous doublez le nombre de GPU, le temps d'entraînement sera quasiment divisé par deux. Nous pouvons scaler très haut, jusqu'à des centaines de GPU. Si vous avez des datasets énormes, MXNet peut vous aider à réduire les temps d'entraînement. MXNet fait partie du bloc, ce qui est une garantie d'indépendance pour tous ses utilisateurs et développeurs. Personne n'a la mainmise sur MXNet.

Voici quelques exemples d'applications que l'on peut réaliser lorsqu'on parle de traitement d'image sur MXNet. Ici, c'est de la détection d'objets. Avec Rekognition, nous faisions de la détection de scènes, donc nous étions capables de trouver des labels et des personnes. Ici, nous allons un cran plus loin, où nous identifions non seulement des objets ou des personnes à l'intérieur d'une image, mais nous traçons également ce qu'on appelle la bounding box, c'est-à-dire nous donnons les coordonnées à l'intérieur desquelles cet objet ou cette personne se trouve. Il y a plein d'architectures de réseau qui permettent de faire cela, et voici deux projets MXNet sur GitHub. L'un implémente une architecture de réseau qui s'appelle RCNN, une architecture moderne pour ce genre de choses. On appelle également cela un single shot detector, c'est-à-dire détecter tous les objets qui se trouvent dans une seule image. Le projet de droite est plus récent et implémente une architecture qui s'appelle YOLO, ce qui signifie You Only Look Once. Ce sont des projets que vous pouvez récupérer sur GitHub, avec les modèles déjà définis, le code MXNet pour ces modèles, et même les modèles principaux entraînés que vous pouvez télécharger et récupérer. Vous pouvez utiliser ces réseaux à l'état de l'art en 10 minutes.

Un autre cas d'application est ce qu'on appelle la segmentation. C'est un cran plus loin par rapport à la détection. Ici, on cherche la silhouette des objets, on charge vraiment le contour des objets ou des personnes. Un des principaux cas d'utilisation de la segmentation est la conduite autonome, où on veut savoir très précisément où sont les objets sur la route, pas juste de manière approximative, mais très précisément, car 10 cm près, cela peut être la différence entre que cela passe ou non, et cela peut causer un accident. Ce projet implémente une architecture qui s'appelle Mask R-CNN, une évolution supplémentaire au-dessus de R-CNN. Ce projet a été mené par TuSimple, un client AWS qui construit des systèmes de conduite autonome et a fait rouler un poids lourd de manière autonome aux États-Unis sur 300 km. C'est intéressant qu'il ait été open source, et cela peut profiter à tout le monde.

Un autre exemple est la reconnaissance de caractères complexes. Ici, on est sur des formes un peu étranges, des polices très variables et parfois très alambiquées, des images qui peuvent être très floues. Avec ce modèle, on arrive à un niveau de performance assez impressionnant. Quand on voit certaines des images, c'est vraiment étonnant qu'on arrive à extraire le texte correctement. Si vous avez une problématique de reconnaissance de caractères sur des images compliquées, floues, etc., ce modèle basé sur MXNet peut être une bonne solution.

Un autre exemple, que je ne peux pas montrer en animation, est l'estimation de pose. C'est être capable d'identifier en temps réel la position du corps humain, la position des articulations, la position de la tête, etc. On prend un flux vidéo, on le décompose image par image, et on le passe à travers un modèle de deep learning. Ensuite, on marque ces images avec la position des articulations. Ce projet a été initialement fait sur une librairie qui s'appelle Caffe et a été porté sur MXNet. Vous pouvez télécharger et utiliser ce modèle. Si vous êtes dans le monde du jeu vidéo, des médias, ou de la réalité virtuelle, il y a de quoi s'amuser avec ce genre d'application.

Voilà quelques exemples. Il y en a d'autres, mais vous voyez que la reconnaissance d'images n'est pas juste trouver les chats et les chiens, cela peut être des choses plus compliquées. Il y a beaucoup de modèles qui ont été développés par des équipes de recherche et qui sont implémentés dans MXNet. Maintenant que MXNet supporte ONNX, un format d'échange de modèles entre MXNet, CNTK et peut-être d'autres, on pourrait prendre un modèle Caffe, le convertir au format MXNet, et l'utiliser sans avoir à l'implémenter. Cette capacité à aller chercher des modèles sur étagère, à comprendre comment ils fonctionnent, et à les utiliser est assez puissante et est vraiment la façon privilégiée dont les gens travaillent.

Reparlons un peu de l'opération de convolution. Hier, nous avons utilisé un réseau à convolution qui s'appelle LeNet, qui nous a servi à apprendre le dataset MNIST, assez efficace pour classifier les chiffres manuscrits de 0 à 9. Voici l'architecture de ce réseau, que vous voyez ici à droite. On part d'une image, ici un panneau de signalisation, mais c'est le même fonctionnement. On applique une opération de convolution, puis un échantillonnage qui réduit la taille de l'image. On voit ici la taille de l'image qui diminue au fil du temps. On part d'une image 32x32, après la première opération de convolution, on est à 28x28. On la divise par deux dans les deux dimensions avec une opération de pooling, où on garde un pixel parmi n. On passe de 28x28 à 14x14, donc on garde un pixel sur 4. On peut prendre une valeur moyenne sur les 4, ou le pixel qui a la plus grande intensité. L'objectif est de réduire le nombre de pixels. On refait une deuxième opération de convolution, on passe à 10x10, on refait une couche de pooling, on arrive à 5x5. À ce stade, l'image n'a plus rien à voir avec l'image de départ, mais grâce à la convolution et au pooling, on a gardé les éléments clés de cette image, l'ADN de l'image originale. Ensuite, dans les CNN, on a une couche de classification avec une couche fully connected, et la couche de sortie qui a autant de neurones que de catégories qu'on essaie de classifier. C'est une architecture assez traditionnelle.

L'objectif de la convolution est de réduire la taille de l'image, de réduire la dimension du problème, et d'apprendre les filtres qui vont nous aider à détecter les features importants de l'image. Quand on entraîne une couche de convolution, on entraîne des filtres qui apprennent à détecter les features importants de l'image. Ce sont des features géométriques, des angles, des bords, des droites, des diagonales, des ronds, etc. Progressivement, ces features s'assemblent pour nous permettre de reconnaître des images complexes. Par exemple, pour un visage humain, on apprend à détecter l'arrêt du nez, l'arrêt des sourcils, etc. Au fur et à mesure que l'on progresse dans le réseau, les couches de convolution apprennent à détecter des choses de plus en plus compliquées. Elles combinent ces features géométriques pour commencer à faire des features de plus haut niveau. À la fin, on est capable de dire que c'est un visage d'homme, un visage de femme, un visage d'enfant, etc. C'est vraiment fascinant.

Comment marche l'opération de convolution ? Mathématiquement, c'est assez simple. Ici, on a une image bleue, une matrice 4x4, qui est notre image de départ. On applique un filtre 3x3. Dans ce filtre, il y a des valeurs mathématiques. L'opération de convolution consiste à, pour chaque pixel, multiplier le pixel d'origine avec le pixel du filtre qui lui est superposé, puis à additionner toutes ces valeurs. Quand on fait une convolution sur une image 4x4 avec un filtre 3x3, on obtient une image 2x2. Au fur et à mesure de la convolution, on réduit la dimension du problème. En fonction des valeurs dans le filtre, le réseau apprend quelles valeurs mettre pour obtenir un résultat qui nous permette de classifier correctement. Chaque pixel de chaque filtre est un paramètre du réseau. Via la backpropagation, l'optimisation, le réseau apprend pour chaque filtre de chaque couche, quelle est la bonne valeur à mettre pour chaque case de chaque filtre.

Passons aux démos. Nous allons faire quatre démos. Nous allons refaire une petite classification MNIST pour nous échauffer, cette fois avec une librairie que je n'ai pas mentionnée hier, Keras, une alternative à MXNet. Nous verrons que c'est assez semblable. Ensuite, nous utiliserons des réseaux préentraînés, affinerons un réseau préentraîné, et finirons avec une démo de GAN, Generative Adversarial Network, qui apprend à générer de nouveaux échantillons à partir de rien.

Je vais télécharger mon dataset MNIST. Dans Keras, c'est assez facile à faire. Il y a un ensemble d'API qui permettent de télécharger les datasets de référence. Nous sommes sur les mêmes données qu'hier. Nous avons des images de 28 pixels x 28 pixels. Nous téléchargerons le dataset, qui comprend un dataset d'entraînement et un dataset de validation. Nous le normaliserons et afficherons sa forme. Notre dataset d'entraînement a 60 000 échantillons de 28x28, et 10 000 échantillons de tests. Les labels des images sont les chiffres correspondants. Nous avons parlé de One-hot-encoding, comment générer un vecteur avec un bit qui indique le numéro de la catégorie. MXNet le fait automatiquement, mais pas Keras. Donc, je dois convertir ma matrice Y. Voilà, mes catégories initialement sont des numéros, et je fais ce one-hot encoding. Le premier échantillon appartient à la catégorie 5, le deuxième à la catégorie 0, le troisième à la catégorie 4, etc. C'est remplacer des valeurs numériques par un vecteur de bits de la même taille que le nombre de catégories, mais où seul un bit est mis à 1.

Ensuite, nous définirons le réseau. Ici, je vais définir un réseau qui est presque le même que celui d'hier, avec peut-être une couche de convolution en moins et une couche de pooling en moins. On voit les deux couches de convolution, où on entraîne pour chaque couche 32 filtres de 3x3. Le but de l'entraînement est que le modèle apprenne pour chacun de ces 64 filtres. Après, il y a les paramètres de la couche connectée. Nous définissons le réseau, affichons les différentes couches, définissons l'optimiseur et la loss function. Ici, on compare des vecteurs de 10 probabilités, donc la cross-entropie est la bonne méthode. Nous entraînons le modèle pour 10 époques. Les API sont très proches entre Keras et MXNet. On passe de l'un à l'autre assez aisément. Le niveau d'abstraction est vraiment très proche. Keras est une couche d'API qui peut avoir différents back-ends, comme TensorFlow ou Theano. Dans la version 1.x de Keras, on a MXNet, mais pas encore dans la version 2.0.

Nous avons entraîné notre modèle sur Keras. La validation est très bonne, avec 99,25% de précision. Nous pouvons afficher quelques images. Le 7 a été correctement prédit, le 2 a été correctement prédit, etc. Même des valeurs un peu moches sont bien reconnues. Il y a quand même une petite erreur, mais la matrice de confusion montre que la majorité des zéros sont bien reconnus. Il y a très peu d'erreurs marginales. Nous voyons la force des réseaux CNN, où la quasi-totalité des chiffres sont correctement reconnus. Voilà un petit exemple avec Keras, une librairie sympathique que vous pouvez utiliser pour apprendre le deep learning.

Dans le deuxième exemple, nous allons revenir sur MXNet et utiliser des modèles préentraînés. Dans MXNet, comme dans Keras ou Gluon, nous avons un modèle zoo, un ensemble de modèles sur étagère qui ont été non seulement codés mais préentraînés sur des datasets. Nous avons une série de modèles que nous pouvons télécharger et récupérer directement. Je vais récupérer trois modèles de classification d'images : VGG16, Inception V3 et ResNet 152, qui sont les gagnants successifs du Challenge ImageNet. Ce sont des modèles assez efficaces. Je les ai déjà téléchargés, donc nous allons gagner du temps. Quand on télécharge un modèle, on télécharge un fichier params qui contient les poids du modèle, et un fichier JSON qui contient la définition du réseau. Ces fichiers font plusieurs milliers de lignes. Ensuite, nous avons les catégories. Ces datasets ont été entraînés sur ImageNet, avec plus d'un million d'images dans 1000 catégories. Ces catégories sont listées dans un fichier Sinset.txt. Pour charger un modèle, on appelle load_checkpoint, on passe le nom du modèle, et on récupère le symbole, c'est-à-dire le réseau. On l'instancie sur GPU ou CPU, et on associe le modèle à la forme de la donnée. Ces modèles s'attendent à une forme précise d'image, ici une image avec trois canaux de 224x224. On doit donc lui fournir une image de cette forme. On charge le fichier avec les catégories, on charge une image, on la redimensionne en 224x224, et on s'assure qu'elle a bien la forme 1x3x224x224. La prédiction est la même que pour MNIST. On transforme l'image en ndarray de la bonne forme, on la balance dans le réseau, on récupère les valeurs de sortie, on trie les 1000 probabilités, et on affiche les top 5.

Nous allons charger nos trois réseaux sur CPU, puis prendre une image et la classer. Voilà l'image en question. Nous voulons le top 5 des catégories pour chacun des trois réseaux. VGG16 me dit à 96% qu'il y a un violon, ResNet me dit à 96% qu'il y a un violon, et Inception me dit à 82% qu'il y a un violon. Ces modèles entraînés savent reconnaître de manière très précise le violon, car il y a probablement des images de violon dans le dataset d'ImageNet. J'ai simplement téléchargé un modèle, le chargé, et lui passé une image redimensionnée. Nous allons le refaire sur GPU pour voir s'il y a une différence de performance. Nous les rechargeons, mais cette fois, nous utilisons le GPU. Nous reprédisons l'image. Les temps de prédiction sont de 64 microsecondes, 32 microsecondes, et 9 microsecondes. Il semble que j'ai un job bloqué sur mon GPU. Je vais vérifier et arrêter tout ce qui tourne. Et maintenant, si je réexécute, ça marche. C'est intéressant. Le GPU ne peut pas être utilisé par plusieurs jobs en même temps. Si vous avez ce genre de problème, c'est que vous avez un notebook qui est encore en train d'utiliser le GPU. Je suis passé à 0,04 micro, 0,08 micro, 0,12 micro. Donc sur VGG par exemple, on est passé de 0,64 à 0,04. On voit ici que l'accélération fournie par le GPU joue à plein. Et donc c'est ce qu'on disait hier, l'utilisation du GPU lorsqu'on veut prédire à l'échelle et à très gros débit.

Donc voilà comment on utilise un réseau pré-entraîné. Vous voyez, c'est très peu de lignes de code, on fait plus de plomberie Python que de MXNet dans cette histoire. Alors, troisième exemple, je vais repasser juste sur mes slides un instant pour vous expliquer ce qu'on va faire. Hop, on va cacher ce truc-là. Donc ici, j'ai entraîné un réseau sur un dataset qui s'appelle CIFAR-10. Il s'appelle comme ça parce qu'il a 10 classes que vous voyez là. Donc, il y a les avions, les automobiles, les oiseaux, etc. Il y a 60 000 images qui sont réparties équitablement en 10 classes. Et ce sont des images couleur de 32 par 32. J'ai pris un réseau, ResNet, 50 couches, un réseau à convolution. Et je l'ai entraîné, sans trop chercher à optimiser, pendant 200 époques. Ça a pris un jour ou deux, parce que je ne devais pas avoir beaucoup de GPU sous la main. Au bout de ces 200 époques, je suis arrivé à une précision de validation de 82%. Ce n'est pas terrible, mais je n'ai pas cherché à l'optimiser plus que ça. Il y a 10 000 images de validation, je passe les 10 000 images à travers le réseau, j'obtiens 82%. Imaginons que mon problème, ce soit de distinguer les voitures des chevaux. Si je ne passe dans le réseau que des images de chevaux ou de voitures, j'ai 88% de précision. Donc, parmi mes 10 000 images d'échantillon de validation, je ne prends que les 1000 voitures et que les 1000 chevaux, je prédis cela et je regarde la précision, et donc j'ai 88,8. Pour une raison inconnue, finalement dont on se moque un peu, manifestement le réseau marche un peu mieux avec les voitures et les chevaux qu'avec les grenouilles ou les camions. Mon point de départ c'est si je veux reconnaître une voiture d'un camion, ma précision est de 88%.

Ce que je vais faire maintenant... J'ai une instance GPU qui tombe dans AWS. Donc, j'utilise la Deep Learning AMI Ubuntu, dont on a parlé hier. Cette instance, elle a un GPU. C'est une instance P2X large. Donc j'ai un K80. Ici, je n'ai pas utilisé de Volta. J'aurais dû. Ça aurait été plus rapide. Voilà. Ce que je vais faire, voilà mon modèle, voilà le modèle que j'ai pré-entraîné, donc ici c'est sur Keras, mais cette fois avec MXNet comme back-end. J'ai entraîné ce modèle pendant 200 époques, je l'ai sauvé, et donc il arrive à une précision générale de 80%, et pour voiture versus cheval, je vais utiliser une technique qu'on appelle le fine tuning. C'est une technique hyper puissante. D'abord je vais travailler avec deux classes, les numéro 1 et les numéro 7, qui sont les automobiles et les chevaux. Il y a 5000 échantillons par classe pour le training et 1000 échantillons par classe pour la validation. Je vais travailler pendant 10 époques sur un GPU avec une taille de batch 32. Je vais charger le dataset, je vais en extraire les voitures et les chevaux en utilisant les bons labels, etc. Ça, c'est de la plomberie Python. Vous pourrez regarder plus tard si vous voulez. Et puis, je vais charger mon modèle pré-entraîné. Donc ici, j'utilise MXNet comme back-end. C'est le modèle MXNet que je vais charger. Jusque là, on pourrait se dire, c'est un peu comme l'exemple d'avant. On charge un dataset, on le découpe en training et en test. On va extraire uniquement les voitures et les chevaux, parce que c'est ça que j'ai envie de faire. Je charge mon modèle pré-entraîné et maintenant j'ai envie de le spécialiser. Comment est-ce que je pourrais faire pour le spécialiser ? Je veux surtout ne pas le ré-entraîner à partir de zéro. Je veux garder tout l'entraînement préalable. Mais simplement cette fois, j'ai envie de dire, écoute, il n'y a que deux classes qui m'intéressent. Donc, OK, sur ta couche de sortie, tu as dix catégories, mais moi, il n'y en a que deux qui m'intéressent. Donc, on va oublier l'entraînement de la dernière couche. On va garder les poids de toutes les couches antérieures, sauf la dernière. Et on va réentraîner la dernière couche juste sur des voitures et des chevaux. C'est ce que je fais là. C'est très élégant, je trouve. En trois lignes, on dit : pour toutes les couches du modèle, sauf la dernière, elles ne sont pas entraînables. Donc si on remet les choses dans le bon sens, ça veut dire, je vais réentraîner la dernière couche du modèle. Donc j'ai ce flag trainable que je peux mettre dans Keras qui dit « est-ce qu'il faut entraîner la couche ou pas ? » Donc, on ne réentraîne aucune couche sauf la dernière. Et donc ensuite, on réentraîne. Comme d'habitude, on va définir un optimiseur. Et donc, on va faire l'entraînement qui est là. Et dans mon X et mon Y, je vais passer uniquement les images de voitures et de chevaux. Et je vais évaluer la précision. Ok ?

L'idée étant ici, évidemment, de se dire, je veux garder tout l'entraînement, parce que toutes les formes géométriques qui ont été apprises par les couches de convolution, elles restent valables. Mais ce que je veux finalement, c'est affiner mon entraînement juste pour les voitures et les chevaux. Donc vous voyez ici, c'est mon point de départ. Dans le réseau pré-entraîné, je repasse mes 1000 voitures et mes 1000 chevaux et j'ai bien 88,8 de précision. Ensuite, je gèle toutes les couches du réseau sauf la dernière et je ré-entraîne sur les 5000 voitures et les 5000 chevaux et je valide sur 1000 voitures et 1000 chevaux. Et donc vous voyez que c'est assez incroyable finalement, dès la première... Donc au bout de 12 secondes d'entraînement, j'ai déjà 98% de précision de validation. C'est-à-dire qu'il m'a suffi de repasser une fois dans mon réseau 5000 voitures et 5000 chevaux pour passer de 88 à 88,65. Et au bout de 10 époques, j'arrive à quasiment 99%. On va monter encore un tout petit peu. Donc vous voyez la puissance du fine-tuning, finalement avec très peu de données et pendant un temps très court, on va arriver à faire monter la précision de manière spectaculaire. Sans avoir à réentraîner pendant des heures et des heures. Il y a beaucoup de sociétés qui utilisent cette technique. Le fine-tuning c'est une arme de guerre pour le deep learning. Ça permet d'avoir des modèles qu'on affine de manière très pertinente en ayant très peu de données. À condition de partir d'un modèle qui a été entraîné sur des données qui ressemblent un peu aux autres. Ce qui est évidemment le cas ici.

Je n'aurai pas le temps de faire la dernière démo parce qu'on va être trop en retard pour le dernier webinar. Je vais la faire dans le deuxième webinar. Donc on va s'arrêter là. Et on va, je ferai, parce que je veux absolument vous montrer le GAN. Donc je vous montre le GAN dans 5 minutes. C'est ça Hugo ? Alors oui, histoire de se compliquer encore la vie, dans 3 minutes, on voudrait tester un format de diffusion sur YouTube, donc je crois qu'Hugo va vous afficher le lien YouTube dans le chat. Ok, alors le sondage comme d'habitude, vous avez l'habitude maintenant, donc il va vous passer le lien dans le chat. Ce n'est pas du tout obligé de l'utiliser, vous pouvez tout à fait rester sur GoToWebinar et ne rien faire de plus. Si vous voulez nous aider à tester, ouvrez le lien que Kubo vous envoie. Vous allez voir plusieurs écrans en même temps. On est multicam sur YouTube. Vous allez voir en même temps l'écran, en même temps la salle, etc. On voudrait l'expérimenter aujourd'hui. Si ça ne vous dit rien, restez sur GoToWebinar. Si vous voulez tester YouTube, cliquez sur le lien que Kubo envoie. On va continuer pour ce webinaire-là à diffuser sur les deux canaux. Vous pouvez faire comme vous voulez. Et puis vous direz à Hugo dans le chat si ça vous plaît, si ça ne vous plaît pas, si ça marche, si ça ne marche pas. Voilà, voilà. Qu'est-ce que j'avais d'autre à dire en attendant ? Je vous ai mis quelques liens, un peu les mêmes qu'hier. Et puis un bon tuto deep learning qu'on veut suivre sur le blog d'NVIDIA et puis un tuto beaucoup plus approfondi sur la convolution si vous voulez rentrer un peu dans les détails mathématiques et comprendre comment tout ça fonctionne. Voilà, merci beaucoup, désolé pour les problèmes techniques mais j'allais dire des gros mots. Merci Firefox, la leçon est retenue et on vous retrouve donc dans deux ou trois minutes soit sur GoToWebinar soit sur YouTube ou soit sur les deux, pourquoi pas, si vous voulez le son en stéréo. Je ne suis pas sûr que ce soit très agréable. Merci beaucoup et donc à tout de suite.


        </div>
        
        <div class="tags">
            <h2>Tags</h2>
            <span class="tag">Deep Learning</span><span class="tag">Image Processing</span><span class="tag">Convolutional Neural Networks</span><span class="tag">Amazon Rekognition</span><span class="tag">Fine Tuning</span>
        </div>
        
        <div class="links">
            <a href="https://www.julien.org/youtube.html" class="link">Julien.org - Youtube</a>
            <a href="https://youtube.com/@juliensimon.fr" class="link youtube">Julien's YouTube channel</a>
        </div>
    </div>
</body>
</html>
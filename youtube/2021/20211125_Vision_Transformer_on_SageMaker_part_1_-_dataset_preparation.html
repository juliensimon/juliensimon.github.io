<!DOCTYPE html>

<html lang="en">
<head>
<meta content="Vision Transformer on SageMaker part 1   dataset preparation - This video is the first in a series of three, where I focus on training a Vision Transformer model on Amazon SageMaker.

In this  video, I start from the « Dogs..." name="description"/><meta content="Vision Transformer on SageMaker part 1   dataset preparation - Julien Simon" property="og:title"/><meta content="Vision Transformer on SageMaker part 1   dataset preparation - This video is the first in a series of three, where I focus on training a Vision Transformer model on Amazon SageMaker.

In this  video, I start from the « Dogs..." property="og:description"/><meta content="https://www.julien.org/youtube/2021/20211125_Vision_Transformer_on_SageMaker_part_1_-_dataset_preparation.html" property="og:url"/><meta content="video" property="og:type"/><meta content="summary_large_image" name="twitter:card"/><meta content="Vision Transformer on SageMaker part 1   dataset preparation - Julien Simon" name="twitter:title"/><meta content="Vision Transformer on SageMaker part 1   dataset preparation - This video is the first in a series of three, where I focus on training a Vision Transformer model on Amazon SageMaker.

In this  video, I start from the « Dogs..." name="twitter:description"/><link href="https://www.julien.org/youtube/2021/20211125_Vision_Transformer_on_SageMaker_part_1_-_dataset_preparation.html" rel="canonical"/><meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<title>Vision Transformer on SageMaker part 1   dataset preparation - Julien Simon</title>

<!-- Umami Analytics -->
<script defer src="https://cloud.umami.is/script.js" data-website-id="27550dad-d418-4f5d-ad1b-dab573da1020"></script>
<link rel="dns-prefetch" href="//cloud.umami.is">
<style>
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
            line-height: 1.6;
            margin: 0;
            padding: 20px;
            background-color: #f5f5f5;
            color: #333;
        }
        .container {
            max-width: 800px;
            margin: 0 auto;
            background: white;
            padding: 30px;
            border-radius: 10px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
        }
        h1 {
            color: #2c3e50;
            margin-bottom: 10px;
            font-size: 2.2em;
        }
        .date {
            color: #7f8c8d;
            font-size: 1.1em;
            margin-bottom: 30px;
            font-weight: 500;
        }
        .video-container {
            position: relative;
            width: 100%;
            height: 0;
            padding-bottom: 56.25%; /* 16:9 aspect ratio */
            margin-bottom: 30px;
        }
        .video-container iframe {
            position: absolute;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
            border: none;
            border-radius: 8px;
        }
        .description {
            background: #f8f9fa;
            padding: 20px;
            border-radius: 8px;
            margin-bottom: 30px;
            white-space: pre-wrap;
            font-size: 1em;
        }
        .description a {
            color: #3498db;
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s;
        }
        .description a:hover {
            color: #2980b9;
            text-decoration: underline;
        }
        .transcript {
            background: #f8f9fa;
            padding: 20px;
            border-radius: 8px;
            margin-bottom: 30px;
            white-space: pre-wrap;
            font-size: 1em;
        }
        .transcript h2 {
            color: #2c3e50;
            margin-top: 0;
            margin-bottom: 15px;
            font-size: 1.5em;
        }
        .tags {
            margin-bottom: 30px;
        }
        .tags h2 {
            color: #2c3e50;
            margin-bottom: 15px;
            font-size: 1.5em;
        }
        .tag {
            display: inline-block;
            background: #3498db;
            color: white;
            padding: 6px 12px;
            margin: 4px;
            border-radius: 20px;
            font-size: 0.9em;
            font-weight: 500;
        }
        .links {
            display: flex;
            gap: 20px;
            flex-wrap: wrap;
        }
        .link {
            display: inline-block;
            padding: 12px 24px;
            background: #3498db;
            color: white;
            text-decoration: none;
            border-radius: 6px;
            font-weight: 500;
            transition: background-color 0.3s;
        }
        .link:hover {
            background: #2980b9;
        }
        .link.youtube {
            background: #e74c3c;
        }
        .link.youtube:hover {
            background: #c0392b;
        }
        @media (max-width: 600px) {
            .container {
                padding: 20px;
                margin: 10px;
            }
            h1 {
                font-size: 1.8em;
            }
            .links {
                flex-direction: column;
            }
        }
    </style>
</head>
<body>
<div class="container">
<h1>Vision Transformer on SageMaker part 1   dataset preparation</h1>
<div class="date">November 25, 2021</div>
<div class="video-container">
<iframe allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="" src="https://www.youtube.com/embed/jalopOoBL5M">
</iframe>
</div>
<div class="description">This video is the first in a series of three, where I focus on training a Vision Transformer model on Amazon SageMaker.

In this  video, I start from the « Dogs vs Cats » dataset on Kaggle, and I extract a subset of images that I upload to S3. Then, using SageMaker Processing, I run a script that loads the images directly from S3 into memory, extracts their features using the Vision Transformer feature extractor, and stores them in S3 as Hugging Face datasets for image classification.

In the next two videos, I’ll use these datasets to train models using the Trainer API in the Transformers library (<a href="https://youtu.be/iiw9dNG7JcU)," rel="noopener noreferrer" target="_blank">https://youtu.be/iiw9dNG7JcU),</a> and then PyTorch Lightning (<a href="https://youtu.be/rjYV0kKHjBA)." rel="noopener noreferrer" target="_blank">https://youtu.be/rjYV0kKHjBA).</a>

⭐️⭐️⭐️ Don't forget to subscribe to be notified of future videos ⭐️⭐️⭐️

Vision Transformer paper: <a href="https://arxiv.org/abs/2010.11929" rel="noopener noreferrer" target="_blank">https://arxiv.org/abs/2010.11929</a> 
Dataset: <a href="https://www.kaggle.com/c/dogs-vs-cats/" rel="noopener noreferrer" target="_blank">https://www.kaggle.com/c/dogs-vs-cats/</a>
Code: <a href="https://github.com/juliensimon/huggingface-demos/tree/main/vision-transformer" rel="noopener noreferrer" target="_blank">https://github.com/juliensimon/huggingface-demos/tree/main/vision-transformer</a>

New to Transformers? Check out the Hugging Face course at <a href="https://huggingface.co/course" rel="noopener noreferrer" target="_blank">https://huggingface.co/course</a></div>
<div class="transcript">
<h2>Transcript</h2>
            Hi everybody, this is Julien from Hugging Face. This is the first of a series of three videos where I'll focus on training a vision transformer model on Amazon SageMaker. You're certainly familiar with using transformers for natural language processing tasks, but in fact, thanks to the vision transformer, we know that we can also use transformer models for computer vision tasks, such as image classification. And this is exactly what we're going to do here. In the first video, we're going to build a dataset using images stored in S3. In the second video, we'll use this dataset to train a Vision Transformer model using the Hugging Face container on Amazon SageMaker, and we'll use the Trainer API from the Transformers library. In the third video, we'll train again using the Hugging Face container and SageMaker, but this time we'll use PyTorch Lightning, which is a very popular PyTorch edition that makes it pretty easy to train models. So just another way to do it. OK, let's get started.

First of all, we need a dataset. To keep it simple, I'm going to grab the dogs versus cats dataset and we're going to download it and extract it locally. We're going to keep just 10% of the dataset to keep the demo and training time pretty short, but you could certainly use more images. You can see the script here; nothing really difficult. Extract the dataset, move all the dog images to one folder, and all the cat images to another folder, and then basically keep only 1,250 images from each class and sync that to an S3 bucket. I'll include the script in the repo. Again, nothing really complicated here.

So once we've done that, what we have is an S3 bucket where we have at the root one prefix per class. So obviously, the cat prefix includes all the cat images and the dog prefix includes all the dog images. This is the assumption that I make on my data. This is what my dataset script expects. Okay, so pretty typical: one prefix with all the class images under it.

Let's take a look at the high-level process, and then we can look at the individual functions. This is my SageMaker processing script, and here's the entry point, the main function. The script will receive arguments: the bucket, the prefix, the list of classes that are present in the dataset. In this case, it's going to be an array with two classes, dog and cat. I could also pass the name of the model that I want to use, which Vision Transformer model do I want to use? So you receive those arguments, extract them. My first step is going to be to read all those images in S3 directly and figure out what the label is and store the images in a Python dictionary. I'll have two keys in the dictionary: the NumPy array storing the image bytes, and the label, so the class index 0 or 1. We'll look at how this works.

So once I've got this dictionary, then I can use the convenient `from_dict` API in the datasets library, applying the features, so the labels and the image. This is going to be the Hugging Face dataset object from my in-memory dictionary. The next step is, of course, to extract features from the images. I'm using the Vision Transformer Feature Extractor for that. What this really does is add a new feature called Pixel Values to the dataset. Since I'm working with the Hugging Face dataset, I can use the `map` API to apply a preprocessing function that will extract the features using that extractor. Once we've done that, so we have this extra feature in the dataset, we split the dataset into three parts: train, validation, and test, and we save them as Hugging Face datasets to well-known locations inside the SageMaker processing container, and that container will automatically move them to S3. So that's the high-level process: load images into a dictionary, use the feature extractor to add pixel values, and then split and save.

Let's zoom in on the individual functions here. Building the image dictionary starts from an empty dictionary, and we paginate the objects that live in that bucket because you can only list a thousand objects at a time. Since we have more than that, we need to iterate over the pages. For each object in each page, we load it from S3 as a NumPy array, figure out its label, and then add it to the dictionary. How do we load the object? We read it as a byte stream, which we open with the Python image library. We resize the image to 224x224, which is what the Vision Transformer expects. If your images already have the right size, of course, you can ignore that step. I convert the image to a NumPy array and then move the channels to the first position because I'm going to use PyTorch for training, and that's what PyTorch expects. These are color images with three channels: red, green, and blue. We want to make sure that the channels, so the three dimensions (red, green, blue), are the first dimension of the array. Unfortunately, the Python image library does it in the opposite direction, so channels last, which is why I need to move the axis first. That's how you load an image from S3.

Figuring out the label is super simple because, of course, the key for the image contains "dog" or "cat." So we can split that key, find the "dog" or "cat" position, and then, since we have the list of classes, we can find the label, which is just the index of "dog" or "cat" in the class list. So 0 or 1, pretty much. And that's how we do it. So nothing really complicated here.

The preprocessing function is just what you would expect. Starting from a batch of images stored in the dataset, we apply the feature extractor and add that new feature called pixel values. I guess that's the canonical way to work and process a Hugging Face dataset, using that `map` API with a function.

Now let's see how we run this using SageMaker Processing. SageMaker Processing is really super simple. We use a built-in container. In this case, I'll use the sklearn processor. You can use PySpark if you want to distribute computations, but here, I guess I don't need that. So I'll just stick to my good friend sklearn processor, passing my infrastructure requirements. I just need a decent amount of memory on the instance because I'm going to load all those images. So I'm going to go with an M5 for Excel. That should be more than enough. But you can go bigger. You can use larger M5s or very large R5 instances. So if you really need to load tons of data, you can just change the instance type, and you should be fine.

Then I'm just going to run this processor object, passing my image processing script, which we just looked at. It doesn't have an input per se. The input is really defined by the bucket, the prefix, and the list of classes. It has three outputs because, as we saw, we have three splits for the dataset: the training data, the validation data, and the test data. That's it. Super nice. We just run this cell, and we can see a bit of logging here. So we're installing the transformers library and certainly the dataset library. We can see we load images and then build a dataset. We extract features. Here we see we're downloading the feature extractor for the Vision Transformer model we selected. Then we split the dataset, and we see we'll have 2,000 images for training, 250 for validation, and 250 for test. And then we just save them. All of that took nine minutes. And I can visualize the actual location of those outputs. So I see all three are living in that same S3 prefix. If I list that, I obviously see my training dataset, and we see the familiar Arrow file, and the same for the validation dataset and the test dataset.

Okay, so that's the end of this first video. Now we have a dataset in S3, a Hugging Face dataset, and in the next video, I'll show you how to train a model using this dataset and the Trainer API in the Transformers library. Okay, so keep watching.
        </div>
<div class="tags">
<h2>Tags</h2>
<span class="tag">Vision Transformer</span><span class="tag">Amazon SageMaker</span><span class="tag">Dataset Preparation</span><span class="tag">Hugging Face</span><span class="tag">Computer Vision</span>
</div>
<div class="links"><a class="link" href="../../../youtube.html">← Back to YouTube Overview</a></div>
</div>
</body>
</html>
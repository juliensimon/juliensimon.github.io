<!DOCTYPE html>

<html lang="en">
<head>
<meta content="ML Fridays India   Start your AI ML projects right May 2021 - Broadcasted live on 28/05/2021. More information on ML Fridays at https://pages.awscloud.com/ml-fridays.html

⭐️⭐️⭐️ Don't forget to subscribe to be notified of..." name="description"/><meta content="ML Fridays India   Start your AI ML projects right May 2021 - Julien Simon" property="og:title"/><meta content="ML Fridays India   Start your AI ML projects right May 2021 - Broadcasted live on 28/05/2021. More information on ML Fridays at https://pages.awscloud.com/ml-fridays.html

⭐️⭐️⭐️ Don't forget to subscribe to be notified of..." property="og:description"/><meta content="https://www.julien.org/youtube/2021/20210528_ML_Fridays_India_-_Start_your_AI_ML_projects_right_May_2021.html" property="og:url"/><meta content="video" property="og:type"/><meta content="summary_large_image" name="twitter:card"/><meta content="ML Fridays India   Start your AI ML projects right May 2021 - Julien Simon" name="twitter:title"/><meta content="ML Fridays India   Start your AI ML projects right May 2021 - Broadcasted live on 28/05/2021. More information on ML Fridays at https://pages.awscloud.com/ml-fridays.html

⭐️⭐️⭐️ Don't forget to subscribe to be notified of..." name="twitter:description"/><link href="https://www.julien.org/youtube/2021/20210528_ML_Fridays_India_-_Start_your_AI_ML_projects_right_May_2021.html" rel="canonical"/><meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<title>ML Fridays India   Start your AI ML projects right May 2021 - Julien Simon</title>
<style>
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
            line-height: 1.6;
            margin: 0;
            padding: 20px;
            background-color: #f5f5f5;
            color: #333;
        }
        .container {
            max-width: 800px;
            margin: 0 auto;
            background: white;
            padding: 30px;
            border-radius: 10px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
        }
        h1 {
            color: #2c3e50;
            margin-bottom: 10px;
            font-size: 2.2em;
        }
        .date {
            color: #7f8c8d;
            font-size: 1.1em;
            margin-bottom: 30px;
            font-weight: 500;
        }
        .video-container {
            position: relative;
            width: 100%;
            height: 0;
            padding-bottom: 56.25%; /* 16:9 aspect ratio */
            margin-bottom: 30px;
        }
        .video-container iframe {
            position: absolute;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
            border: none;
            border-radius: 8px;
        }
        .description {
            background: #f8f9fa;
            padding: 20px;
            border-radius: 8px;
            margin-bottom: 30px;
            white-space: pre-wrap;
            font-size: 1em;
        }
        .description a {
            color: #3498db;
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s;
        }
        .description a:hover {
            color: #2980b9;
            text-decoration: underline;
        }
        .transcript {
            background: #f8f9fa;
            padding: 20px;
            border-radius: 8px;
            margin-bottom: 30px;
            white-space: pre-wrap;
            font-size: 1em;
        }
        .transcript h2 {
            color: #2c3e50;
            margin-top: 0;
            margin-bottom: 15px;
            font-size: 1.5em;
        }
        .tags {
            margin-bottom: 30px;
        }
        .tags h2 {
            color: #2c3e50;
            margin-bottom: 15px;
            font-size: 1.5em;
        }
        .tag {
            display: inline-block;
            background: #3498db;
            color: white;
            padding: 6px 12px;
            margin: 4px;
            border-radius: 20px;
            font-size: 0.9em;
            font-weight: 500;
        }
        .links {
            display: flex;
            gap: 20px;
            flex-wrap: wrap;
        }
        .link {
            display: inline-block;
            padding: 12px 24px;
            background: #3498db;
            color: white;
            text-decoration: none;
            border-radius: 6px;
            font-weight: 500;
            transition: background-color 0.3s;
        }
        .link:hover {
            background: #2980b9;
        }
        .link.youtube {
            background: #e74c3c;
        }
        .link.youtube:hover {
            background: #c0392b;
        }
        @media (max-width: 600px) {
            .container {
                padding: 20px;
                margin: 10px;
            }
            h1 {
                font-size: 1.8em;
            }
            .links {
                flex-direction: column;
            }
        }
    </style>
</head>
<body>
<div class="container">
<h1>ML Fridays India   Start your AI ML projects right May 2021</h1>
<div class="date">May 28, 2021</div>
<div class="video-container">
<iframe allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="" src="https://www.youtube.com/embed/e7b2Z2rr76g">
</iframe>
</div>
<div class="description">Broadcasted live on 28/05/2021. More information on ML Fridays at <a href="https://pages.awscloud.com/ml-fridays.html" rel="noopener noreferrer" target="_blank">https://pages.awscloud.com/ml-fridays.html</a>

⭐️⭐️⭐️ Don't forget to subscribe to be notified of future videos ⭐️⭐️⭐️

In this session, we’ll see how you can put your AI/ML project on the right track from the get-go. Applying common sense and proven best practices, we’ll discuss skills, tools, methods, and more. We’ll also look at several real-life projects built by AWS customers in different industries and startups.</div>
<div class="transcript">
<h2>Transcript</h2>
            So in this session, we're going to focus on best practices, no-nonsense advice to put your AI ML projects on the right track. There's a lot of confusion and buzzwords, and it's really not easy to figure out how to get started, right? So, by the way, please feel free to ask all your questions. I'm trying to keep an eye on the chat. I'll try to answer questions as we go, and obviously, we will have plenty of time after the session to go through questions. Okay? So, don't be shy. Ask everything you want, and we'll do our best to answer as many as we can. Okay? All right. Let's get started now.

So the first question is: Is AI real? Is it a trend? Is it a buzzword? What's in it for IT practitioners? Does AI have a massive future in the IT industry? Will it grow? Will it keep going? Yes. This is my prediction. Machine learning is about prediction, so this is mine. If you have another question like that, insert another coin, as much as I do on this. And I really don't know about 2030 or 2050 or 2070. I honestly don't think anyone knows anything. But people like to make those long-term predictions. And to be honest, in my opinion, this is not really useful. The better question is how do we, the builders, developers, project managers, architects, data scientists, know how to get there? How do we get from, "my organization wants to implement AI and machine learning," to having projects in production that run well, solve business problems, and make us happy?

And yeah, I guess my answer now would be, hmm, not so sure. IT has actually been around for a long time. I always find it a bit surprising that we tend to reinvent the wheel a lot. We tend to forget that our ancestors have been building IT projects for literally 70 years now. So it's almost a person's life, right? A lifespan. It's a long time. And if we want to know the future, we should look at the past and see how we've collectively done on understanding and implementing disruptive technologies.

So if you want to see how AI adoption will look, how good we will be at adopting it, it's useful to see how we've done in the past. Some of you will remember the 2000s, where we were trying to build web-scale applications, websites, etc. And honestly, the recollection I have from those days is that a lot of projects sunk and died an awful death because we had no idea what we were doing. A few years later, everyone rushed to add e-commerce to their websites. The big rush to e-commerce. And again, a lot of those projects ended underwater because it was so new, with no reference points. We didn't have a lot of best practices. We were just trying to build the best we could. But unfortunately, a lot of the projects went bad.

Fast forward five years, mobile commerce ended up pretty much the same. There was a lot of pain and frustration involved because of new tools, new processes, new technologies, new business models, and new everything. We couldn't get it done right. And my favorite was big data. People rushed to spend tons of money on expensive Hadoop clusters, building teams around that, and piling up data all the way to the moon, not really delivering business value. And again, I've made all those mistakes as well. So I got eaten by the shark again and again. So hopefully, you had a better experience with it.

So the first conclusion is that Jaws is not really a movie about sharks. It's really a movie about managing IT projects. It's the terrifying truth about tech projects. We have confused stakeholders. Remember in the movie, the mayor and the business leaders of the town want to keep the beaches open. They don't understand the danger. They just want the business to move along. But it's not that easy. Business pressure to keep all the tourists coming to the island. Business has got to move on. We can't spend too much time thinking. We can't spend too much time preparing. We need to keep business running. And an unprepared team. Remember the sheriff and his deputies, totally unprepared for shark attacks and how to deal with that. Inadequate tools. If you've seen the movie, you know the way they start hunting that shark is pretty rudimentary and doesn't go well for a few of them. Improvised tactics, literally trying to make things up as they go. And of course, random acts of bravery. Eventually, they get the job done, but it's luck more than anything else.

And I'm making this sound a little sarcastic, but we've all been on those projects where stakeholders don't really know what they want and just want something, putting a lot of pressure on teams. Dealing with new technology is really difficult to figure out how to build. And sometimes we actually get to the end, but it's frustrating, burning out long hours, and quality not what it should be. But of course, people will always tell you that every time new technology comes, they'll say it's different, it's the AI revolution, old rules don't apply. And well, a lot of people still keep saying that, and I don't want to finger-point. You can make up your own opinion on who they are. Not interested in that. But I think this is why we keep failing or delivering very painfully every time we deal with new technology: we forget that we've been there before. Even if you're junior in the IT industry or fresh out of school, you can work with people around you who've been there before and should pay attention to what they're saying. It's not just old people ranting. They've seen this stuff before, even if they've never done AI. And well, there are certain ways of doing things right.

So hopefully, 2020 and beyond is not going to look like that. It's the whole point of this presentation: look at best practices and certain ideas that will hopefully save you from the shark. Because insanity is doing the same thing over and over again and expecting different results. A lot of people say Albert Einstein said this. It looks like it could have been Mark Twain, but either way is fine. Both are geniuses. So whoever said it first is absolutely right. We're all tired of being shark food, right? I'm not sure why I put a question mark on this. I can't see anybody enjoying being shark food. So we want to move away from those negative things on the left. We're tired of that. And AI and machine learning is again a new cycle in the IT industry. It's a chance to get away from that shark bait and do it better.

So instead of having confused stakeholders, we want to set expectations. We want to replace business pressure with clear metrics showing progress, incremental progress. We don't want to be unprepared. Maybe we don't have all the skills, but we need to know where we are. We need to assess the skills and then pick the right tools for the job according to those skills. We want to use best practices. And one of my very strong beliefs here is that traditional best practices are proven best practices for software engineering and also apply to ML projects. And instead of running around fighting randomly, we want a clear methodology, which is, of course, iterate, iterate, iterate. Okay, so let's look at all those points, one by one.

The first one is critical: setting expectations. What is this project about? What are we trying to deliver? And you'd be surprised. No offense to our customers, but I still talk to a lot of people who are not quite sure what they're trying to achieve. They want to invest in machine learning for sure, and they have good reason to do that. There are good questions to be answered in their businesses, but they're not super clear on what the question is. And it's not that easy as it seems, right? Because it should be very crisp. It should be literally one sentence on the whiteboard. Nothing more. It shouldn't be five pages of text. It shouldn't be a 60-slide presentation. It should be simple, a clear sentence that everybody in the company can understand. And of course, it needs to be quantifiable. If your statement is, "We'd like to improve customer retention," well, yes, but how much? 5%, 60%? Clear goals, clear goals.

So one sentence, quantifiable. You could have a business metric that makes sense, whatever your business is. The second question is, well, machine learning is about data, right? It's not about whether we will build a model or not. We may not need to build a model; we may reuse models. But what's the data like? Do we have enough data? Do we have any data at all? And that's obviously an important fact. Everybody has data. You're going to say, "Oh, we have databases, we have backends, we have S3, we have all that good stuff. Tons of data." Right, tons of data. But how good is it? How relevant to the problem is it? It's not about quantity; it's about quality. It's about making it better over time, curating it. And if you don't have enough data or not enough clean data, what's the cost of getting more? Sometimes it's as easy as collecting more weblogs. Sometimes it's as hard as labeling complex pictures for computer vision applications.

Another important point is involving everybody early on. Machine learning is too serious to be left to data scientists, to paraphrase that famous example. It's about solving business problems. So you need to have business stakeholders to educate them on how you're going to build that project and what kind of results they can expect. Setting expectations. You need domain experts to help you understand the problem. Sometimes the problem is straightforward enough that engineers and developers can figure it out. But if you have complex problems in healthcare, chemistry, or finance, you could be a very strong software engineer, but you don't know all the finer points of the domain. So you definitely need help here. Of course, you're going to need IT because they're going to be building, deploying, and monitoring your apps. And of course, data science, if you have a data science team, etc. So everybody around the same table trying to figure out what the problem is, what's the best question we can answer, what would be a good metric, etc.

So here are some examples of terrible whiteboard sentences. This is my favorite one, and honestly, I still hear this one a lot: "We want to see what the technology can do for us." Red flag, alarm, awful, awful. This is guaranteed disaster because you're going to be fooling around with software and servers and whatnot, and you're not going to build anything. Even if you build a POC, what is it good for? It's a toy example. You need to have a good, solid business problem to work on. This is my next favorite: "We have tons of relational data. Surely you can do something with it." Yes, maybe, maybe not. I don't know. It's not about the data; it's about the business question. Data is just here to help you answer the question. But what's the question in the first place? And yes, this one: "I read this cool article about FUBAR ML. We ought to try it." This is terrible too because it's not about tools. It's not about having fun with expensive toys. It's about answering business problems. So if you find yourself getting that as a tool, let's say, an entry point into machine learning, stand your ground, say no, and dive deeper on the problem and figure out what the business question is. Why are we even embarking on this project? I cannot overstate how important this is. This is the number one mistake people make.

Okay, so once you have a rough business question, or maybe you have three or four and you're trying to evaluate which one is the best, you need to define metrics. What is the business metric showing success? This is important, not only to show success to the team making progress but also to show success to the company and to your business stakeholders, saying, "Hey, machine learning is actually quite good at solving this problem. We had a positive impact." Again, technical metrics are nice, but it's about improving business outcomes. It's also very important to understand the baseline. A lot of the time, you're going to be improving or sometimes replacing an existing system, which could be human-based or another IT application. So of course, you want to do better than the existing process. It's important to understand where you are. So what's the baseline you're starting from? What's working well, what's not working well, that kind of stuff.

Once you understand the baseline, you need to understand what's a reasonable but still significant improvement. And this can widely vary. Let's say you want to automatically classify common support tickets, right? Use natural language processing and assign tickets to the right support representative based on the topic. So you could say, "Well, we have a human baseline, and it's a little bit random because people don't understand the domain very well. Maybe we only get 70% or 80% accuracy on assigning to the right team or the right person." So you could say, "Okay, you want to do better than that, right? If your baseline is 80%, 81% is not a reasonable improvement. It's not enough to justify launching a project. 99% is not a reasonable improvement because you're not going to get to 99% short term. So find something impactful for the company, that shows a positive outcome, but that's still reasonable. Because, as I will say again and again, machine learning is an iterative process, and you want to keep iterating and keep improving your project and your model. So you could say, "Well, we want to improve ticket classification by 5% every quarter." Okay, fine. Why not? Start, get some results, and keep improving on that.

And here again, there are some red flags you should be mindful of. Machine learning is just like every other domain; there's a lot of jargon. So it's very easy to define business or machine learning metrics that are super difficult to relate to. For example, coming back to my support ticket classification example, you could have your data science colleagues entering the office and saying, "Oh, yeah, we're super excited. The confusion metrics have significantly improved." If you're into machine learning, you know what that means. If you're a software engineer, not obvious. If you're a business person, you're completely confused and have no idea what they're talking about. Is that good? Is that bad? Confusion metrics? How could something called confusion be good, right? It's weird. And you could say, "Well, sorry, I didn't understand what you said." And that person could go, "Oh, yeah, but you know, P90 time to resolution is now under 24 hours." Like, okay. Is that good? Is that bad? What are you talking about here? And then the person can say, "Oh, well, don't you understand? Misclassified emails have gone down 5.3% using the latest model." And you say, "Ah, okay, now I'm starting to relate to this. We want to classify those support emails correctly. If we do a better job, that's interesting." But at the end of the day, what you want to hear is something like, "Well, the latest support survey shows that very happy customers are up 9.2%." And say, "Ah, now that's really good because now I see business impact." And maybe that's my metric, or maybe that's one of the metrics I'm keeping my eyes on. And well, that's good. It's what I want to see: happy customers, people who get their problems solved quickly so they can enjoy the service or product they bought. We want to make customers happy. So that's good.

So pay attention to that. It's very easy for tech teams to come up with tech metrics that are not necessarily useless. Maybe they're telling the right thing in the sense that, yes, this is working the way it should. But is it telling the business story? Probably not. So make sure to have those metrics as well.

The next one is figuring out what you need and what your skills are when it comes to actually building the project. So it's really about assessing needs, not wants. I'm an engineer. I love to play with technology. I love to get my hands dirty with all sorts of tools. But is this the right thing to do for the project and the company? And am I shooting myself in the foot by using tech that's really too complicated for me or just over-engineered for the problem? So ask yourself, and you have to be super honest about this. So, okay, we understand the business problem. We understand the metrics. Can we build a data set describing the problem? Do we have the data? And like I said, is it costly to get more if we need more? Do we have the big data or just the data platform to quickly and efficiently clean, curate that data in order to build data sets? Is it something we're good at and is this something we need for this problem?

Let me give you an example. Let's say you want to do computer vision on medical images, detecting conditions on medical images. So this is a really specific problem. It's not something that's likely to be available off the shelf. And it's not something, for example, that AWS services can do out of the box. So if you're dealing with that kind of problem, yes, you certainly need to have a data set. You need to collect images from hospitals and medical sources. You need to prepare those images. You need to keep them anonymized, etc. There are so many problems. But yes, you need to do that because it's very specific data. Now, if you want to build, let's say, a fun application for children where they upload animal images, and automatically you recognize what animal that is and give them fun facts and educational facts on animals, you know, why not? You could say, "Well, that's got to be available somewhere, right? That's got to be available. Why should I go and take thousands of pictures of thousands of different animal species, maybe that's not necessary." So you need to understand how specific the problem is, how readily available data could be for that, and what it would look like on your end to work with that data and how much work would be involved.

Once you've kind of got a sense of the data that you need or don't need, you need to ask yourself, can we write and can we tune machine learning algorithms? Is this something we know how to do? Is this something we should be doing for that problem? And again, it's not about, oh, we want to do it because it's fun and it's going to look right on my resume. Please don't do that. What's going to look right on your resume is delivering projects that create business value. You could have a long list of technologies, but if you never had a successful project, I don't think it looks good. It's about showing business impact. So do you have to do it? Again, if you want to recognize everyday life images, if you want to do sentiment analysis on everyday natural language, do you really think it's worth writing your own algo? Those problems are already solved in a way, and there should be limited machine learning work here. Now, if you want to do complex image processing on industrial parts, mechanical parts, or I don't know, semiconductor wafers and that kind of stuff, or satellite images, yes, maybe here you need to actually go and build a custom solution. But again, be critical, do your homework, do your research, see if those problems have been solved before, and try to find the quickest route to success.

And of course, infrastructure, right? Because data, training models, deploying models, all that will require infrastructure. Do you need to do it? Do you want to do it? No, you don't want to do it, right? Trust me, you don't want to do it. You want to focus on the business problem. You want to focus on the machine learning problem. You don't want to manage servers. No company ever wants any customers because they have the best-run Docker cluster or the most amazing EC2 instances. It's not about that. Of course, it needs to run, it needs to scale, it needs to be fast, etc., etc. But it's not the core problem here. The core problem is understanding data to answer the business question and then building a model that does that right. Infrastructure, not so much. So it's a whole spectrum of solutions. So on the left, you could say, "Well, I need a fully managed solution. I think my problem is, let's say, generic enough, and I mean that in a positive way, you know, it's a well-understood problem. It's been solved before. I can probably find off-the-shelf APIs to do that or off-the-shelf models. And definitely, I shouldn't be managing infrastructure." And on the right hand, maybe you have a crazy, innovative, and totally new problem that you're trying to solve. So probably you need your data set. Probably you need algorithms, right? And so you need to find that balance, okay? You need to find that balance. Be super, super honest about what the business needs are and then what the technical needs are and match that with your skills.

So once you know that, then you need to pick the best tool for the job, which is obvious. And well, I'm sure you've seen this before. It used to be called the Iron Triangle of Project Management. And here's my version for machine learning. And cost, time to market, or accuracy, you get to pick two, right? So, for example, if you want a cost-effective and a fast option, it's probably not going to be the most accurate. If you want to go fast, build your POC, show some business value to your stakeholders early on, then you need to do that. And that's okay. It's enough to get started, but maybe it's not good enough for production. But at least you can discover the problem, learn more about the problem. One important thing to know is that improving accuracy will take increasingly more time and money. It's really about diminishing returns. So typically, it's reasonably easy to get 80% accuracy. It's going to be more to get to 90%. It's going to get a lot of work to get to 95%. And anything beyond that, every decimal after that is going to be increasingly, increasingly difficult and expensive generally. So it's really diminishing returns because some problems will not require 99% accuracy, especially when you look at the associated costs of getting there. So it's about understanding when to stop and when it stops making sense to improve the accuracy with respect to the amount of time and resources that need to be invested in there. And it's a gray line. It's not obvious that, oh, 91.9% is where I should stop. You need to figure it out, right? You need to figure it out.

And another problem that I see sometimes is people get super, super excited about state of the art. And, you know, they read the blog post and sometimes they read the research papers and it's like, "Oh, we can use this. It's amazing, and this crazy new model from whoever." And I would give you a word of caution here because state of the art is amazing for sure, but it's hard to work with. It's hard to understand. It can be complex and costly to tune and live with. So I would only focus on what I call the actionable state of the art. What I mean by actionable is stuff that you can really use yourself on a daily basis. So techniques like transfer learning with pre-trained models, AutoML, these are, I would say, reasonable and actionable tools. I would be a little wary of the super crazy, fancy models that you see out there. They're probably overkill and they're probably too complicated to work with unless you really, really have very strong machine learning and data science skills.

And as I mentioned earlier, best practices are critical. So things are not different this time. AI ML is software engineering. And it's not something that lives in an ivory tower or a dungeon where it's a totally different world and none of the best practices ever apply. I think this is, unfortunately, I hear this one a lot, although it's been improving lately, I have to say, probably because more developers get into machine learning and they bring in all the knowledge and the best practices that they've learned over time. So, you know, dev environment, test environments, QA, documentation, agile methods, versioning, etc., etc. So all the good things that help deliver high-quality projects need to apply here. It's not okay, in my opinion, to say, "Oh, well, give me the data and I'll talk to you in six months and maybe I will have a model." And then even if they do, right, you get that black box model and you have no idea if it really works. And, you know, if you're lucky, you're going to get some results and say, "Oh, look, you know, on my test set, this performs very well." It's like, yeah, what about real-life data? So it can't work like that, in my opinion. You need to standardize workflows. Machine learning is still a fairly new field. There are lots of different tools, lots of different ways to build models, deploy models, etc. And standardizing as you go, as your ML practice grows, is really important. Just like with standardization, standardize our development workflows over time. And again, onboard all teams. This is not just about data science and machine learning. It's about embedding the models into your IT applications, deploying them, monitoring them, measuring their business impact, etc., etc. It's really about IT in general and not just data science.

And a very important thing is, again, a lot of machine learning projects tend to be tested in sandboxes or, like I said, with test data sets or A-B testing, etc. And it's all good. You need that. It's an important starting point. But the truth is in production. And coming back to the job, because this is what I'm talking about. Well, this is shark hunting production. And this barrel ID might have looked good on paper or in the harbor. But when you're in the ocean and you actually start testing it, how did that go? Not so well. So the same thing for machine learning models. You need to get them in production early on, as soon as you can, honestly, and as often as needed, because you need to evaluate those models on real-life data. You need to see how they perform on real-life data. And real-life data is always going to look different than the data that you have in your training and test data sets. That's just how it is. Real-life data is never clean, is never exactly right, and there are so many things that can go wrong. And you need to figure them out as early and as often as needed to keep your models operating at high quality. So in order to do that, of course, you need continuous integration, continuous deployment. You need automation as soon as you can start building those. There will be a welcome addition. And generally, all the DevOps practices for machine learning are good. And if people want to call that MLOps, that's OK. But honestly, it's really DevOps all over again. So if you've done that before, good. If you're a DevOps engineer, you have a very, very nice career path into ML. Just learn how to deploy machine learning models and how to monitor them, and you can transition into ML very easily. MLOps is a super, super hot topic right now because I guess it's still very new, and people realize how important it is to get that stuff right.

Can we get to another Aditya, can you bring in the poll now? Folks, we will wait for 20 seconds for you all to respond to this poll, and then Julien will continue post this poll. That's a very good question. Yeah, it's difficult. Yeah, and it is a difficult one. If you ask your boss, you know, if you give that poll to your stakeholders, they will tick all the boxes. We want all of that. And then it's your job saying, well, I'm sorry, my friends, but it's not that easy. I wish we could do that. And over time, you learn how to get better and how to do better on those three things. But early on, you have to pick your... That was precisely the reason why we thought about this particular poll, because what we have learned working with customers across the globe is that from use case to use case, the answer to this question will differ, or rather it should differ. You cannot have an org-level answer for this question. It has to be at the use case level. So that's the thing. Yeah, yeah, exactly. So that's why there are no hard rules here. Because if you were, coming back to my healthcare example, if you're trying to detect, I don't know, let's say you want to do early cancer detection, accuracy is paramount, of course. You don't want to tell people, oh, you have a high chance of having cancer if they're perfectly okay. And I guess it's even worse to tell someone, oh, you're absolutely fine. And then that person is actually ill. So, yeah, when you have life-critical scenarios, I guess, autonomous driving, healthcare. Yeah, in fact, in the current situation, how quick the vaccine can reach the healthcare center, time to market is very important. Yeah, exactly. Or maybe it's startups. We know there's a first mover advantage for startups in new markets. So you could say, well, I want to deploy something next month because I know we have competition and we want to move faster, we want to collect customer feedback faster, and time to market is super important. And accuracy is not so important in the short term. Yeah, so again, this is why those questions, generally, I'm very wary when people come with pre-built answers to everything. That's why I much prefer to give people help them realize what the questions are, what they should be looking at, and then they come up with the answer that's right for them. They know their business, I don't. You have to be very humble here and help people understand what they should be thinking about. A lot of people think accuracy is the most important. I would agree. No one wants to build models that don't predict right. So that makes sense. But again, as you iterate, those things could differ. You could say initially we want time to market, first few iterations. And then we want to work on accuracy a little more. Yeah, so it makes sense.

OK, so the last thing is really how, once you've got all those things figured out, how do you go and deal with the project? And iterate, iterate, iterate, iterate. Also known as Boyd's Law, not new, 1960. Even I wasn't born. And it's about speed of iteration, beating quality of iteration, which is a shocking, shocking statement for software engineers who are heavily invested in code quality, and they should. But yeah, it's important. It's important to move fast and learn fast about the problem. So the best advice I can give you is start small. Don't go for moonshots. It's very tempting to say, oh, we want to build this crazy complicated model because we want to solve this crazy complicated business question. I mean, especially if your organization is new to ML, start with something reasonable. Not toy examples. Not what I said. You can run the toy examples. That's okay. But then find maybe five to ten small interesting projects inside your organization where you could show some improvement and build your machine learning practice, etc. The analogy that I take is your company, your organization is like a huge engine. Imagine a cruise ship engine. It's a huge thing. So you could say, oh, we're going to build the next new and from scratch and it's going to be amazing. And yeah, probably. But I think before you do that, you need to understand how the existing engine works and where you could improve it. So I'm sure you could find five to ten particular spots where a little bit of extra oil, a little bit of screwdriver action or wrench action can improve things. And when you start fixing all those small inefficiencies, when you start plugging all those tiny leaks, then the engine as a whole runs much smoother. And I think this is a good way to look at machine learning. What are in your organizations the small things that drag you down, the manual processes that waste your time, the legacy IT processes that are outdated because they use rigid business rules instead of using prediction, etc. So start with continuous improvement, small things, processes that are well understood, but that you think you can make better with predictive models. And in the same vein, you should try the simple things first. Don't rush to complex deep learning models. I'm thinking about 80% of machine learning models out there are regression models or classification models built with well-understood machine learning algorithms. So be super pragmatic, right? Start with the well-understood algorithms, libraries like scikit-learn, XGBoost, linear regression, etc. The ones that are easy to understand, easy to tune, very inexpensive to train, etc. And if that's not enough, or if your business problem is complex enough, then yeah, you can start fooling around with deep learning models, etc. Of course, for computer vision, natural language processing, it's important to do that. But even there, don't go and start writing your neural network architecture. It's honestly not needed 99.99% of the time. You can start with pre-trained models. And maybe use them as is or maybe fine-tune them on your own data. But again, simple, simple, simple. Keep it simple. Simple usually wins.

As mentioned before, go to production quickly. Once you have some accuracy that's interesting enough, start that on data. Okay, run A-B tests or run tests on, you know, capture real-life data and inject it in your model. See what happens. See if your assumptions were right. See what's not predicted right. Talk to the domain experts. Ask them to look at the data and ask them how they would have predicted it. So is, you know, is everybody wrong about this or would a domain expert get it right? And then why did the model get it wrong? Are you missing features in your data set? Understand the prediction errors. That's super important to do this on a regular basis. Observe prediction errors. And then decide, do you need to fix the data set? So maybe you need more columns. Maybe there's a feature missing here that domain experts can point you to say, oh yeah, this is missing in your data. That particular information is missing and this is probably why the model got it wrong. Do you need more data? It's a fascinating discussion. How much data is needed for machine learning? So depending on the problem, it could be a few thousand samples to a few million samples. So you need to adjust for that. Maybe you need to tweak the algo, right? Maybe you have the right algo, but maybe the parameters that are used for training are not the best. So there's this whole discussion on model tuning that can help. Or maybe you should try another algo. Maybe you want to think outside of the box and say, okay, yeah, we've tried XGBoost and it did reasonably well, but we think maybe we could try deep learning here, simple deep learning, and see how it does on that data. It's in constant flux. That's the thing. New algos come up all the time, new ideas come up all the time. So you need to challenge your assumptions and look at prediction errors and see how to fix them and repeat until accuracy gains become irrelevant—irrelevant here meaning irrelevant to the business problem and with respect to the costs that would be involved in improving the model again. And then you can move to the next project. Job done, and you can start churning those models and then become a well-oiled machine learning team.

And to prove my point that machine learning is an intuitive process, you know, here's the typical machine learning cycle, life cycle, and it kind of shows some of the things we've been talking about. So start from the business problem. That's the starting point. Get that thing right. Otherwise, you'll be running in circles in the desert and you're going to be frustrated. So frame the problem. How do we use data to answer the problem? What do the domain experts tell us? Do we have that data in-house? Do we have enough of it? And then start to collect it, start to centralize it in a central place. Clean it, prepare it, visualize it. Typically what data scientists do. And then engineer features. Transform the data to make it more expressive, to make it easier for algorithms to learn. And then train models and tune parameters to get to high accuracy and measure accuracy. And probably early on, accuracy is not good enough, and business goals are not met. And so you need to go back to square one, so to speak, and figure out what to improve to get to the yes path. And once you get to the yes path, then you move on to deploying models and serving predictions and monitoring all of that and over time retraining to account for new data. Right. So you can see you have multiple cycles in there. So iterate, iterate, iterate, and you also need plenty of different skills, right? From business stakeholders to your domain experts, to your data engineering and data science teams, to machine learning engineers to help you optimize the machine learning performance and the technical performance of your models, and of course, IT to help you deploy and monitor everything. So iterative cross-team effort. If you think like that, you're already on the right path.

So I guess the next question is, does this work? And why am I even talking to you? So Amazon has been doing machine learning for a while. So a while meaning 20 plus years. And here are some fun numbers. So recommendation on Amazon.com and other local websites is important, to say the least. We estimate that about 30% of page views on Amazon.com come from recommendation. And obviously, all the logistics could not run without automation and without machine learning. You're certainly familiar with the Echo devices and Alexa. And of course, those use natural language processing techniques and deep learning techniques. And Prime Air also uses machine learning and computer vision and other techniques for drone delivery. So we've been doing this almost since day one. And over time, we've built a lot of best practices and tools to be successful with machine learning. And this translates into the AWS machine learning stack. And don't worry, we're not going to go through all of this. The key takeaway here is we think of our stack as three layers. The top one, AI services, is the easiest, quickest one to use because most of these services are based on models that we trained on very large data sets and we keep training them and optimizing them. And we expose those capabilities through APIs, just like everything in AWS. So, for example, Rekognition will let you analyze images and videos, object detection, face detection, face comparison, content moderation, etc., etc. Pass the image to the API to get the job done in real time. Super, super simple. You can test it in the AWS console, or you can call the API, and I can guarantee even a junior developer can work with this in 10 minutes. Likewise, we have speech services for speech-to-text, text-to-speech. We have natural language services like Comprehend for sentiment analysis and entity extraction, Translate for translation, Textract for document processing, OCR, etc. I'm not going to go through the list, but you can go and take a look at those. And again, generally, the common theme here is we train the model, we optimize the model. You call an API, get the job done. And a lot of those, again, you can start using literally in minutes. And it's going to be one line of code most of the time. So super, super simple. Zero machine learning skills required. And this is, again, the quickest, simplest way into machine learning. If you want the no-nonsense stuff in your app to solve common machine learning use cases, this is the place to start. And even if you have a bigger project, there are certainly chunks of the project that could be done with AI services. And then you could focus your actual machine learning efforts on smaller chunks. So if you need to do translation or if you need to do text-to-speech within your project, go and do that with Polly and Translator. And then the rest of the problem that is more specific, you can address with SageMaker, which is the next layer down, where you can bring your own machine learning code, whether it's TensorFlow or PyTorch or Scikit-learn or your own custom code. It doesn't even have to be Python code. It could be R. It could be something else. And you have full control over the machine learning lifecycle, all the way from labeling data, preparing data sets, to feature engineering, to training, to optimizing, to deploying, etc. There are a long list of capabilities here. And all are integrated in SageMaker, which is our web-based machine learning IDE. So here, it's more, you know, it's about bringing your code and then using SageMaker to train models and deploy models without ever managing infrastructure. All infrastructure is fully managed. So it's really a one line of code to train, one line of code to deploy, regardless of the scale. Okay. Cool stuff because you can focus on ML and ignore infrastructure. And at the bottom layer, we have foundational building blocks like optimized versions of the machine learning frameworks available as Amazon machine images for virtual machines or containers. And of course, we have the full range of EC2 instances, CPU, GPU, and a few more exotic options for customers who want to build everything themselves. And that's okay. In some cases, it is the better option. Okay, so three layers. And again, a large project could literally cherry-pick from the three layers and pick the right tool for the job. So don't think of those layers as totally separate. They're actually complementary. So we have over 100,000 customers who use machine learning on AWS. And so if your question is, who's the typical customer? It's everybody. Large enterprises, startups, NGOs, universities, everybody, and across all industry segments. Of course, historically, some segments like retail or maybe financial services have adopted machine learning quicker because they had lots of data and they had a very clear business incentive to use data for prediction. But honestly, now it's everybody. Healthcare, industry, education, media and entertainment, telecom. The list goes on. The list goes on. It's literally everybody.

So looking at common machine learning use cases, helping you, because hopefully by now you're a little bit excited about machine learning. And a question that I get sometimes is, where should I start? What should I look for? You're telling me five to ten projects, try to identify five to ten projects. What are people doing typically? And maybe I can start there. So that's a great way to get started. And we can find maybe three big chunks, which are improving the customer experience. So personalizing content is super important. Improving customer relationships, especially in contact centers. That's a good one as well. It's a problem that a lot of companies have. Extracting insights from media, whether it's images or videos or extracting stuff and building high-level insights. So this is a strong theme here. The second theme is, of course, making the business run smoother. Coming back to my cruise ship, put oil and wrench around at the right spots to make the engine run better, faster, leaner, etc., etc. So things like intelligent search, improving search capabilities, whether internally or for your own customers, processing documents, right? All companies have documents. We have mountains of forms and PDF files and whatnot laying around, and we need to extract the data there. Fraud detection is super important. As soon as you do online transactions, you want to do that. And analyzing metrics, forecasting, whether it's supply chain or whether it's inventory or whether it's financial metrics, forecasting is important. And then innovation, building new crazy applications that are powered by machine learning. So I would say machine learning in general, and specifically, there are some interesting developments in DevOps. We have services, for example, that automatically review code and give you a friendly tap on the shoulder saying, you should look at this code because I think something is not right. So lots of different ideas here. And maybe let's look at a couple of examples. So media intelligence, again, is a popular one where our customers want to extract insights from images and videos or add insights to that content. So, for example, you can use Rekognition on images and videos to, like I said, do object detection or face recognition, matching people in the video to names, extracting metadata, and then storing that metadata later on. For example, if you're a news channel, you may want to index all your video files that way so that now you can easily answer questions like, show me a video with the prime minister of India and the French president. And instead of having to do that stuff manually, watching hours and hours and hours of videos and then entering that data somewhere, you can ask Rekognition to do that. That's just an example. You can extract the audio part of your videos and use Amazon Transcribe to transcribe it to text. So that would be speech to text. And if you want to do captions, for example, or if you want just to have a text transcript of the video because you want to run text analytics on that. And then you can use Translate obviously to translate those texts into your own local language, right? Because maybe you want to do captioning in a different language. Okay. And you could run Amazon Comprehend on that to do, again, entity extraction, figuring out which locations are mentioned, which personalities are mentioned, etc., etc. So you can see, you can automate all of that. Now you can ask questions on, show me the list of videos with the prime minister of India, the French president, where they talk about, I don't know, global warming, right? And imagine having to do that manually at scale on thousands of hours of video. It's a daunting task. Now you can automate all of that, store those insights into your database, and query very simply and get results in seconds. Okay, just one idea, but there are so many combinations here.

Another important example is document processing. Like I said, a lot of companies, well, all companies really have lots of documents that they need to process, financial documents, contracts, customer documents, credit applications, mortgage applications. Now, the list is so long. And still, a lot of companies use paper documents. So you can digitize that, you can run Amazon Textract, which is our OCR service to extract the text, to extract tables and forms. It's not just the raw text; it's also the structure of the text, which is really important if you have a form. You don't care so much about the text in there; you want to know that this box was ticked and this field contains this particular string. So it's about the text and the structure too. And then again, you can run Translate, you can run Comprehend, you can combine all those services. They work very easily together because, again, it's really one line of code every time. So you can take the output of one and feed it to the next and build super smart workflows. And you can also add another service called Amazon Augmented AI to the mix. Because, of course, we recognize that a lot of business processes need to be as accurate as possible. And as mentioned before, or there are diminishing returns on the machine learning side. So maybe the machine learning part of this process is, let's say, 92% accurate. But the 8% left are important, right? You don't want your mortgage applications or your credit applications to be botched. So you could say, well, if the machine learning confidence score is lower than, let's say, 90%. So the service tells you, yeah, 89%, sure, this is pretty correct. This is correct. Well, you could say, well, that's not good enough. And I'm going to automatically send that to a human team for review. So hopefully, the bulk of the processing can be automated away. And you can send the rest to a human team to get to hopefully 100% accuracy, even though even humans make mistakes. But you can get very, very close to that. And this is a service called Amazon Augmented AI that lets you automatically build a workflow where I would say lower confidence predictions are automatically dealt with by a human team. So you get the best of both worlds in a way.

And the last thing I want to mention is, of course, once you walk into machine learning per se, where you bring your own data and start to investigate more specific use cases, there is a world of opportunities. So these are the common ones. Predictive maintenance is super important in the manufacturing industry. Trying to anticipate failures so that you fix them before they happen and you minimize downtime. Forecasting. Every single organization has a time series, whether it's sales or again, inventory or financial metrics, and you want to figure out what the future could look like. Fraud detection is again paramount as soon as you're online. Credit risk, credit decisions are again, highly important for financial services and retail. Document processing, document extraction. Sometimes documents are so specific that you can't really use Textract on them. You have to go deeper and train models on very particular vocabularies or very particular documents. Computer vision is huge, right? Healthcare, manufacturing, media and entertainment, like we said a few minutes ago, autonomous driving is increasingly important and it's extremely hard to build. As you can imagine, it needs to be very accurate and very fast. Recommendation, personalization, building personal content. And churn prediction is important across industries, figuring out when customers may stop using your service. So these are common ones, but again, the list is really endless. I just selected two examples to give you a sense of what people build here. So I figured Coinbase, they've been in the news and they use SageMaker, so they build their own models for fraud detection. And the particular use case they're working on is authenticating ID documents because they found out that Krux will actually try to open many different accounts using the same photo, right? The same picture. Forged ID documents. So it's not about recognizing who that person is. It's about recognizing that this face is extremely similar to another face that we've already seen on another document. So not a very easy problem, but of course, very important for them as they want to cut down on fake accounts and forged accounts. And so they use SageMaker to do that. So computer vision problem, pretty sophisticated model. And as SageMaker lets you work with managed infrastructure, you literally are standing on the shoulders of AWS infrastructure all the time, you can scale unlimited. And so if you need very large training clusters, you can get them. And you can get them literally in one line of code. And so they were able to train in only 10 minutes compared to 20 hours before. So this increased agility lets you retrain literally all the time. So you could train multiple times per day, and you could detect fraud very, very quickly. You could say, well, OK, this phase we've seen before and given the volume of new accounts that they have, I'm sure they need to retrain very regularly and they can do this cost-effectively and quickly on SageMaker. And well, this is certainly a company you've heard about, PayU, one of the top payment gateways in India, growing very fast with lots of customers. And here they use SageMaker for credit scoring. So based on customer information, they have a custom model to decide whether they should give credit to customers who do not have credit ratings, card, or bank accounts. So very, very interesting use case. Again, credit. Credit risk, credit decisions are probably one of the top use cases for financial services. And they've been successful here on SageMaker. So just two examples, but we have lots more. I'll point you to this URL, which is easy enough to remember, ml.aa. And this is really where you can find information on all the services in the stack. And I guess more important than services, you can find all the customer references and across all different industry segments. And I'm sure you can find something that you can relate to and that can inspire you to get started. And hopefully with those guidelines, you know now which questions you should be working on and you can get on the right track. So we'll have a little time for questions. I want to thank you very much for tuning in and joining me today. If you want to stay in touch, you can find me easily on LinkedIn and Twitter and Medium. And I have quite a few videos on YouTube as well, if you want to dive deeper into our services and particularly SageMaker. So thank you very much. Thank you to my colleagues for inviting me and organizing this. It's been a pleasure. So let's see if we have questions. Thank you very much. Yeah. So Julien, we do have questions. So while we discuss on the call, question. I'll request Aditya to put in the poll on the screen. So Gillian, the interesting question what I want to discuss with you and I thought instead of writing it and answering in a textual manner, we should be discussing this is about how do we go about doing predictive maintenance for plant equipment on AWS. So if you could share some insights on how people should go about doing predictive maintenance at a plant level.

Okay, yes. I really like predictive maintenance because the outcome is extremely clear. If you do a good job on predictive maintenance, you minimize downtime, you fix problems before they happen, and you avoid very severe breakdowns that are likely to take place at the worst possible time. So it's a very good use case because it, again, it's very clear what the improvement is. And so there are different ways to do it. And again, I should follow my own advice. So we should start simple, okay? And as a matter of fact, we've built specific AWS services in the AI services layer. Okay, remember that top layer. Let me show that again. Yeah, no. Here it is. So you see on the right-hand side, there's a thing called industrial and it says monitor on. And this is a fully managed service that helps you build predictive maintenance applications. And actually, this service includes the sensors that you can put on your equipment. Okay, so let's say you want to monitor, you know, motors or pumps, rotating equipment, that kind of thing. All you have to do is buy some sensors, and they're pretty small. They're smaller than a pack of cigarettes. And you attach them to your equipment. You associate them to the Monitron gateway, which is a small Wi-Fi gateway. The sensors talk to the gateway using Bluetooth, and the gateway talks to the AWS cloud with Wi-Fi. So you associate those things really quickly, literally take minutes, and immediately those sensors start capturing temperature and vibration information that they send to the gateway and that the gateway sends to AWS, where the data is analyzed and models are trained to figure out anomalies and send you alerts on a possible problem developing with this particular equipment. And it's all integrated in a mobile app. So it's very easy to deploy. It's very easy to use in the field because you get all the information there. You don't even need a laptop for that. So that's a super easy way to get started, right? One angle.

Now you could say, well, that's good, but I have my own sensors or I want to build my own sensors. Maybe I want more than temperature and vibration. And that's okay. We have another service, which is called Lookout for Equipment. And here you can push data coming from your own sensors to a cloud-based service that's going to do pretty much the same thing, right? Analyze the data coming from the sensors and building models and alerting on possible problems. Okay, so you could, now that's another way to do it. And it's generally targeted at the same category of equipment. So, you know, motors and pumps and gears and all that stuff. Now you could say, well, that's all good, but I actually do not want to monitor gears and motors. I want to do predictive maintenance for, I don't know, aircraft engines or other pieces of equipment. And so I have my own sensors. I have my own data. And then you can move one level down and move to SageMaker and then start using off-the-shelf anomaly detection algorithms or your own models if you've built that and train again without worrying about infrastructure, without worrying about scale and that train and optimize and deploy those models on SageMaker. So as I mentioned before, whatever your problem is, I think you, you know, I would really recommend that you look at it that way. See if there's an AI service that you can get started with, or if pieces of your projects can be solved very quickly with AI services, and then divide and conquer, so to speak. So split your project into smaller problems. Solve as many as you can with AI because you will save so much time with that and they're really cost-effective too. And then the smaller problem but the harder problem, the more specific problem, you can work on in SageMaker. And this applies to predictive maintenance but you could say the same for computer vision, natural language processing, and other problems.

Perfect. That makes real sense. And what I wanted to also highlight is that predictive maintenance as a problem is something you will have different nuances depending upon the kind of equipment which you have. So within your organization also, you might have to have multiple strategies from a predictive perspective when it comes to actual ML model building. You will have one strategy which talks about predictive maintenance at the org level and then when you are deep dive, you will have to define a strategy at the individual equipment level and that is what kind of data you're getting from the OEMs of those. Plus, if you're not getting the data, then you might also have to look at thinking about putting up an IoT layer before you actually go about running anything from an ML perspective. Because, again, right-to-maintenance is the classical case of garbage in, garbage out. If you don't get the right data, then the right-to-maintenance will not give you the right outcome what you're trying to look at. So, Julien, in interest of time, I want to take one last question. The question what I'm seeing from many folks is that from a data perspective, we have a lot of services which will enable the organization to collect the data from different formats from different equipment and otherwise also. Once there's accumulation of all this data done right, what is the best way leveraging ML to mine that data or to search for the important parameters or correlations within that data. So can you talk about something on that in that lens?

Yeah, that's a very good question. And yeah, it's a popular one. And you're absolutely right. Over time, we've built quite a few data services. The big data portfolio is pretty extensive on AWS. And I think at the end of the day, I would say it comes down to technical preference. So if you love, if your organization, if your team is fond of, let's say, Spark, which is a very, very popular tool to clean and prepare data, then go and use that. It's about maybe what you're already using today. Are you already using maybe EMR clusters for analytics? Or are you already using maybe Glue, which is our ETL service, again for other workflows. You don't have to reinvent the wheel. Like I said, you have to assess the tools that you know, the skills that you have, and build on that. So if your team is good at writing Spark code, then EMR is a good option. You can run your EMR clusters and do data preparation there. If you want a fully managed option, you can pretty much do the same on Glue. You can bring your Spark scripts on Glue and pull data from the data catalog, data catalog from the different backends, and process it on managed infrastructure and then write it back to S3 and start from there. You can use, if you want to use Athena, if you work with data that's hosted in S3, let's say you have lots of logs or you've exported your relational data to S3 as well, Athena is a super good way to do that. And maybe it's a combination. Again, I don't think a single service will do everything that you need. I think it's better to use the right tool for the job and combine tools.

Now, focusing on SageMaker, there's actually a capability in SageMaker called SageMaker Processing, which lets you run batch jobs on managed infrastructure. Again, very simple, a couple of lines of code. And you can bring your own code in Python and PySpark. And if you use PySpark, you can actually do distributed processing automatically. So this is really the simplest option because you don't need to build EMR clusters, etc., etc. And then you can apply your feature engineering code, etc., etc. So again, there are different ways to do this. And you can run Jupyter notebooks connected to an EMR cluster for interactive exploration. So these are probably the three things I would look at. EMR, EMR and Spark, if that's your technical culture, your technical preference. Glue, if you want to have really completely automated end-to-end processing workflows, unattended workflows, and SageMaker processing, if you prefer to do that within the SageMaker environment, because down the line, you want to automate that workflow completely, data preparation, training, deploying on SageMaker. And we have a service called SageMaker Pipelines that lets you build those automated pipelines as well. So these are probably the three things I would look at. But again, all three are a good starting point because, again, use what you know, build on your existing skills, and then figure out if you need to switch to something different or not. Be super pragmatic about this.

Perfect. I agree. One thing which I loved what you said is that be aware of the capabilities that exist within your organization because there are a lot of information which is flowing around in the industry, and you have to be cognizant of the fact that out of that what is most relevant for the ecosystem within your operating and setting up the goals on top of that. That is very, very important instead of getting blown away by what is what you're reading or what you're seeing in the media. Yeah. Don't listen to. Yeah, exactly. I think it's a good conclusion. Come up with your own conclusions. Do your own homework. Run your own tests and factor in your skills and your business environment. And it's not because you read the blog post that says blah, blah, blah, or even you listen to me saying blah, blah, blah. Think for yourselves and build a solution that works in your context, business, technical, skills, cost, etc. You need to experiment and figure out what works best for you. It's a good learning in IT in general, and it certainly applies for machine learning as well. Yeah, absolutely. So with this, we have come to the close of today's ML Fridays session. Julien, thanks a lot for sharing your knowledge with the participants who joined for the day. And I really thank all the participants who took time from their day schedule to invest one and a half hours listening to us and how we look at the overall ecosystem and how you should be building your ML ecosystem. I would request you all to please fill up the feedback form and if you have any further questions or you want to explore how exactly you can go about doing machine learning within your organization, please feel free to reach out back to us on the emails on which you got confirmation for this webinar, and we'll be more than happy to engage with you and help you on your journey of machine learning. So with this, once again, thanks a lot, and I'll close the seminar for the day. Thanks a lot, everybody. Thank you very much. Bye-bye. Have a good day. Thank you. Bye-bye.
        </div>
<div class="tags">
<h2>Tags</h2>
<span class="tag">MachineLearningBestPractices</span><span class="tag">IterativeMLDevelopment</span><span class="tag">BusinessValueInML</span><span class="tag">DataQualityAndPreparation</span><span class="tag">MLProjectManagement</span>
</div>
<div class="links"><a class="link" href="../../index.html">← Back to YouTube Overview</a></div>
</div>
</body>
</html>
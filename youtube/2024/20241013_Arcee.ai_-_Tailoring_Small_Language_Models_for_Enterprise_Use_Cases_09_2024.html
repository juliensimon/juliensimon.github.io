<!DOCTYPE html>

<html lang="en">
<head>
<meta content="Arcee.ai   Tailoring Small Language Models for Enterprise Use Cases 09 2024 - Talk @ AWS Telco hackathon, Dallas, TX - September 2024.

Slides: https://fr.slideshare.net/slideshow/tailoring-small-language-models-for-enterprrise-use-cases/..." name="description"/><meta content="Arcee.ai   Tailoring Small Language Models for Enterprise Use Cases 09 2024 - Julien Simon" property="og:title"/><meta content="Arcee.ai   Tailoring Small Language Models for Enterprise Use Cases 09 2024 - Talk @ AWS Telco hackathon, Dallas, TX - September 2024.

Slides: https://fr.slideshare.net/slideshow/tailoring-small-language-models-for-enterprrise-use-cases/..." property="og:description"/><meta content="https://www.julien.org/youtube/2024/20241013_Arcee.ai_-_Tailoring_Small_Language_Models_for_Enterprise_Use_Cases_09_2024.html" property="og:url"/><meta content="video" property="og:type"/><meta content="summary_large_image" name="twitter:card"/><meta content="Arcee.ai   Tailoring Small Language Models for Enterprise Use Cases 09 2024 - Julien Simon" name="twitter:title"/><meta content="Arcee.ai   Tailoring Small Language Models for Enterprise Use Cases 09 2024 - Talk @ AWS Telco hackathon, Dallas, TX - September 2024.

Slides: https://fr.slideshare.net/slideshow/tailoring-small-language-models-for-enterprrise-use-cases/..." name="twitter:description"/><link href="https://www.julien.org/youtube/2024/20241013_Arcee.ai_-_Tailoring_Small_Language_Models_for_Enterprise_Use_Cases_09_2024.html" rel="canonical"/><meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<title>Arcee.ai   Tailoring Small Language Models for Enterprise Use Cases 09 2024 - Julien Simon</title>
<style>
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
            line-height: 1.6;
            margin: 0;
            padding: 20px;
            background-color: #f5f5f5;
            color: #333;
        }
        .container {
            max-width: 800px;
            margin: 0 auto;
            background: white;
            padding: 30px;
            border-radius: 10px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
        }
        h1 {
            color: #2c3e50;
            margin-bottom: 10px;
            font-size: 2.2em;
        }
        .date {
            color: #7f8c8d;
            font-size: 1.1em;
            margin-bottom: 30px;
            font-weight: 500;
        }
        .video-container {
            position: relative;
            width: 100%;
            height: 0;
            padding-bottom: 56.25%; /* 16:9 aspect ratio */
            margin-bottom: 30px;
        }
        .video-container iframe {
            position: absolute;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
            border: none;
            border-radius: 8px;
        }
        .description {
            background: #f8f9fa;
            padding: 20px;
            border-radius: 8px;
            margin-bottom: 30px;
            white-space: pre-wrap;
            font-size: 1em;
        }
        .description a {
            color: #3498db;
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s;
        }
        .description a:hover {
            color: #2980b9;
            text-decoration: underline;
        }
        .transcript {
            background: #f8f9fa;
            padding: 20px;
            border-radius: 8px;
            margin-bottom: 30px;
            white-space: pre-wrap;
            font-size: 1em;
        }
        .transcript h2 {
            color: #2c3e50;
            margin-top: 0;
            margin-bottom: 15px;
            font-size: 1.5em;
        }
        .tags {
            margin-bottom: 30px;
        }
        .tags h2 {
            color: #2c3e50;
            margin-bottom: 15px;
            font-size: 1.5em;
        }
        .tag {
            display: inline-block;
            background: #3498db;
            color: white;
            padding: 6px 12px;
            margin: 4px;
            border-radius: 20px;
            font-size: 0.9em;
            font-weight: 500;
        }
        .links {
            display: flex;
            gap: 20px;
            flex-wrap: wrap;
        }
        .link {
            display: inline-block;
            padding: 12px 24px;
            background: #3498db;
            color: white;
            text-decoration: none;
            border-radius: 6px;
            font-weight: 500;
            transition: background-color 0.3s;
        }
        .link:hover {
            background: #2980b9;
        }
        .link.youtube {
            background: #e74c3c;
        }
        .link.youtube:hover {
            background: #c0392b;
        }
        @media (max-width: 600px) {
            .container {
                padding: 20px;
                margin: 10px;
            }
            h1 {
                font-size: 1.8em;
            }
            .links {
                flex-direction: column;
            }
        }
    </style>
</head>
<body>
<div class="container">
<h1>Arcee.ai   Tailoring Small Language Models for Enterprise Use Cases 09 2024</h1>
<div class="date">October 13, 2024</div>
<div class="video-container">
<iframe allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="" src="https://www.youtube.com/embed/CVHsH_J65ok">
</iframe>
</div>
<div class="description">Talk @ AWS Telco hackathon, Dallas, TX - September 2024.

Slides: <a href="https://fr.slideshare.net/slideshow/tailoring-small-language-models-for-enterprrise-use-cases/272382540" rel="noopener noreferrer" target="_blank">https://fr.slideshare.net/slideshow/tailoring-small-language-models-for-enterprrise-use-cases/272382540</a>

⭐️⭐️⭐️ Don't forget to subscribe to be notified of future videos. Follow me on Medium at <a href="https://julsimon.medium.com" rel="noopener noreferrer" target="_blank">https://julsimon.medium.com</a> or Substack at <a href="https://julsimon.substack.com." rel="noopener noreferrer" target="_blank">https://julsimon.substack.com.</a> ⭐️⭐️⭐️</div>
<div class="transcript">
<h2>Transcript</h2>
            Good afternoon, everyone. It's a pleasure to be here. My name is Julien. You may remember me from my AWS days or my Hugging Face days. Still the same guy, just a little more tired, I guess. And now I'm working with Arcee. And I actually have colleagues here. Can you guys wave? All right. So if you have questions, don't ask me because I'm jet-lagged. Ask them. You'll get much better answers. But you can still come and say hi and ask simple questions. Okay? So, yes, I am going to talk about small language models. But before I do that, I need to set the scene a little bit.

A couple of years ago, all we had was closed models, and they were cool, useful, and helped educate a ton of people on what AI really was, particularly for businesses and enterprise use cases. The open-source community started running, and fast forward a couple of years, we've come to the point where the quality of open-source models is just indistinguishable from closed models, as you can see on this very nice graph. The latest models from Meta and others are just on par with the largest and best closed models from OpenAI and Anthropic, the most important ones. So that's a given. I'm not even going to argue with this. There is no arguing. I think it's a fact. And because I like to look a little forward, it's very clear to me that open-source models are state-of-the-art, and they're winning this, clearly. The pace of innovation in the open-source community is much higher than in the closed-source community. So there's no doubt in my mind that even now, not to say maybe six months or a year from now, the best models period will be open-source models. So that's a given.

My talk today is not about this. My talk is about how we take those amazing models and make them even better so that they completely outperform the closed models on your enterprise use cases. Before I do that, I just want to call out why customers prefer small language models. And it's not just me saying it. I've met probably 200, 300 of them in the last 18 months, and I've consistently heard this. The number one thing they like is accessibility, thanks to Hugging Face and model builders in the open-source communities. We have great models to pick from. We can download them in seconds. We don't need to pay anyone anything. We just download them and get to work, right? So that's very cool and a good way to speed up innovation.

The models are open source. I prefer to say open weights. What an open-source model really is, another whole talk, but at the very least, you have access to the model architecture and the model weights, so you know what you're working with and can test it, inspect it, and find out when things work and when they don't in many ways, which is pretty much impossible with closed models. Privacy and intellectual property protection go hand in hand. When you work with small language models and open-source models, you deploy them on your own infrastructure. So if you work on AWS, you deploy those models in your VPC. The data you send to the model and the answers the model sends back stay within your VPC. There's no one else but you looking at that data, and that's the way it should be. Plus, if you train those models on your own data, the models become yours, and they stay within your VPC. You don't have to deploy them somewhere else with the odd chance that something goes wrong and there's a knowledge leak or a security breach.

Freedom of choice is very important. With closed models, you can pick from a handful of models. With open-source models, you can pick from almost a million models now. There are almost a million models on Hugging Face. So you're not locked in, and that means you can find the right model for every single project, which is important, and you can probably upgrade to a better model when a new one becomes available. IT flexibility is also very important. Not everyone wants to deploy or train their models the same way. Some folks insist on doing on-prem, maybe because they have to. A lot of folks will use the cloud, etc. So even in the cloud, you could use EC2, EKS, SageMaker, or maybe Bedrock, why not? And again, you have the freedom to pick not only the infrastructure but also the service and technology you want to run your models on.

Cost optimization is a central concern these days. We're not living in the sandbox anymore; projects are moving to production, and ROI and cost performance are everything. Because you control model selection, model size selection, and infrastructure selection, you can find the sweet spot for each particular project—something you cannot do with closed models, which price per token regardless of the use case. And last but not least, model quality. I've seen enough evidence here to confidently say that a small open-source model tailored on quality data will always outperform a generic large model. I used to say almost always, but it's 2023, so now I can say always. I've seen it enough, and we, with our colleagues here, do that every single day.

So what does a typical workflow look like? Typically, you'll start from a pre-trained model. Let's say a LLaMA 3 model that you grab from Hugging Face. Sometimes it's just good to go, and you don't need to do anything else. It will answer your problems in a good way. But most of the time, and particularly in an enterprise setting, you need to tailor the model to the domain. Whether that's telco, energy, financial services, retail, manufacturing, insurance, etc., your domain and company, and maybe your business unit, have specific domain data, vocabulary, and jargon that need to be baked into the model to increase the quality of the generated answers. So you need to go through that workflow. You'll start from a good model, like a LLaMA 3 model, and need to go through different steps. The main three steps are: inject new domain knowledge, teach the model how to answer your questions the way you want, and give feedback on what a good answer is and what a not-so-good answer is. These three steps are continuous pre-training, instruction fine-tuning, and alignment. Typically, customers will go through all these steps, bringing datasets for those different purposes and, hopefully, end up with a model that knows about their domain, knows how to answer questions properly, and is aligned for tone of voice and safety.

Not so easy to do, lots of work, lots of compute, lots of datasets. What Arcee is doing is trying to simplify this, and I'll show you some examples. Let's zoom in on those building blocks. The first one is continuous pre-training. Continuous pre-training means, "Hey, I want to build, let's say, a telco model, so I'm going to take all of 3GPP standards in the last 10 years and train my model on that, or I'm going to take a million pages of Cisco product documentation and train a model on that." So it's a ton of data, billions of tokens, and as you can imagine, it's pretty compute-heavy. The only choice for a while was to do full fine-tuning. So train the full model in original precision, probably FP16 or BF16, on that corpus of data. The problem is, we're talking about billions of parameters, billions of tokens, full fine-tuning, and that's compute-heavy, expensive, and has stopped a lot of folks from actually doing this.

About a year and a half ago, a new way to train models was introduced called parameter-efficient fine-tuning, techniques like LoRA or QLoRA, which enable large memory savings by only training a fraction of the model, not the full model. Introducing maybe even quantization if we use QLoRA. This allows you to train models in a more cost-effective way. Unfortunately, although it works very well for instruction fine-tuning, it doesn't work well for initial training. It's better than nothing, but the degradation, the price you pay for just training a fraction of the model, is pretty high. Recently, Arcee has been contributing to a new technique that, in a nutshell, trains the layers in the model that contribute the most to the prediction, identified through statistical analysis, but in the original precision. This technique is called Spectrum. You can read all about it, and we have the best people here to answer questions on Spectrum. It's an open-source project available on GitHub.

Spectrum typically trains the top 25% to 50% layers. Depending on where you set that threshold, you get very significant speed-ups and memory usage reduction without compromising accuracy. Sometimes it's even a little bit better than full fine-tuning and is almost as efficient and sometimes a little more efficient than QLoRA. So it's a very cool technique. I encourage you to take a look at it.

Fine-tuning, as I've said, has been improved and simplified with LoRA and QLoRA, which are available in Hugging Face libraries and are almost household usage. I won't discuss those too much. They're still very good, pretty cost-effective techniques for fine-tuning. One thing people tend to forget is that it's not just about the training algorithm; it's obviously about the quality of the data you train or fine-tune the model on. People are always obsessed with GPU memory usage and might neglect the quality of the data a little bit. We recently released a new project called EvolKit, which is a toolkit that lets you improve your Q&amp;A dataset. By using a high-quality LLM, we take your existing Q&amp;A dataset and make it better, more diverse, and more complex, which will automatically improve the quality of your fine-tuning. This is another one of our open-source projects. We shared a resulting dataset on Hugging Face as well, so just in case you haven't seen this, it's been out for a few months now.

This is a nice study on taking mostly 7 billion parameter open-source LLMs and fine-tuning them on a bunch of tasks, then comparing the performance to GPT-4. In a nutshell, what we see is that fine-tuning works, and fine-tuned models are much better than the base models. More importantly, all the fine-tuned models outperform the OpenAI models, including GPT-3.5, and the majority also outperform GPT-4. For some reason, Google Gemma doesn't fine-tune very well, so I wouldn't use it, but if you look at Mistral or Zephyr, 7B, it's very easy to fine-tune them and outperform GPT-4. Keep in mind, these models are 7 billion parameter models, and you can run them on very little infrastructure. The smallest GPU instance on AWS today is G5 2XL, which costs $1.2 an hour on-demand, and it runs these models like a dream. From a cost-performance perspective, this is way better than anything you can find out there.

Let's talk about alignment. Arcee has been out for a while, and hopefully, it's not the first time you hear the word. Basically, what Arcee does is use human feedback on generated answers from a model, telling the model, "This answer is, I would rate it, maybe seven out of 10. Here's an answer that I think would be 10." Given a prompt and a generated answer, a human will give a score and a golden answer. Once you build enough of these, you can train the model again using reinforcement learning so that it learns how to improve its own answers based on the feedback. That's the 30-second explanation. It did make ChatGPT impressive, but there's a dark side to the story. RLHF is difficult to scale; we need humans, thousands of people writing answers, scoring prompts, etc. This is not something your typical company can do themselves. Ethics are completely wrong. These companies have been using outsourced workers in different countries, and the Kenya story made the news again in the last few days. This is not the way I think we should be building AI.

There are other problems like bias and quality. One thing I know is that if the people providing the feedback think the same way and all lean the same way, those things happen. Not saying they happen on purpose, just saying they happen. Complexity and costs are problematic because reinforcement learning is a very heavy technique. It takes a lot of data, a lot of compute, and it's not something that your typical company can do. A number of improvements have been suggested, and one of the most popular techniques these days is called DPO. DPO does away with the human workforce and the reinforcement learning element. It can start from an existing dataset, like the one you see here, with a prompt, a chosen answer, and a rejected answer. You can run statistical analysis and automatically learn how to generate answers that are closest to the chosen one and not the rejected one. The cool thing is there are a lot of good DPO datasets on Hugging Face. This is actually one of ours, so chances are, at least initially in your project, you could very well start with these and align your model using those preference datasets, possibly without having to build your own. This is a much faster, much more cost-effective way to align models based on human preference.

We could stop here and say, "Yeah, we did improve a few things. We can do CPT faster and cheaper with Spectrum, have better fine-tuning through LoRA and improved datasets through EvolKit, and we have DPO." But can we just do away with training and fine-tuning completely? Can we get rid of it? I think we can. And that's the point of model merging. Model merging is based on an Arcee library called MergeKit. Building a great model is difficult, and the first difficulty is defining what great means. It means one thing to me in my company and something different to you in a different company in a different industry. But going through that pipeline is not simple. We have all those training and fine-tuning steps, which need time, effort, compute, and slow everyone down. Instead of doing this, because we have a million models on Hugging Face, plus maybe the ones we've already built inside the company, can we find models that have the qualities we need? Let's say we want to build a model that can do code, math, and Cisco log analysis all at once, not three models, just one. We can certainly find a good code model out there, a good math model, and maybe someone in the company has already built the cool Cisco log analysis model we need. Why would we take those datasets and retrain a single model on all of that stuff? That sounds a little bit silly. And that's what merging solves. We literally take the three models and merge them, in the arithmetic sense of the word. Think averaging weights. That's exactly what model merging is. There are different techniques, but at the core, it's about taking the weights from those models and averaging them out. We're combining task-specific models into a single model without any training. It's not an ensembling technique. Ensembling would be predicting with the three and then averaging out. Here, we build one model and work with that.

The good thing is, because there is no training, we don't need any CPU or GPU compute. You can just run this on your laptop on a CPU instance. It's not a very heavy process. So no cost for training, no extra cost for inference, no extra inference latency. If you're merging three LLaMA 3.8 billion models, you get only one, and it will have the same size and speed as an original LLaMA 3.8B model. All you have to do is look at MergeKit, select a merging technique, and write a config file. Here's one using the Thais algorithm. I have a ton of deep dives on my YouTube channel if you want to look at that. But you can pretty much see what we're doing here. We're merging two Mistral 7B models originally built from this base model. One is the Bio-Mistral model, pre-trained on biomedical data, and we're merging it with the vanilla instruction-following Mistral because this one has the domain knowledge we want, and this one has the instruction-following knowledge we want. You merge them into one, and you run this on a CPU instance in 10 to 15 minutes for next to nothing, and you have your merged model.

Now, if we look at our workflow again, adding merging and some of the other things I discussed, we can modernize and accelerate and reduce the cost of this model adaptation workflow while getting to a much better model in the end. Merging can happen at every single step, so we can replace training or fine-tuning or alignment with merging. We could only do merging steps or say, "Well, maybe I have this very specific company data, so I still want to do pre-training because it's unique data I can't find any other model that has already been trained on that, so I'm going to run CPT and maybe I'm going to run it with Spectrum for efficiency." But then for instruction fine-tuning and alignment, I'm just going to do merging because I'm going to inherit whatever those LLaMAs or Mistral or whoever or whatever I've already done. I don't see any added value in doing that myself. Of course, you would do LoRA here and DPO here and use EvolKit to improve your datasets. That's what Arcee brings to the table, helping you go through that pipeline faster, cheaper, and with higher quality in the end.

How do you do this? Well, a lot of what we do is open source. You could absolutely run all of this open source. No worries. Spectrum, EvolKit, MergeKit, plus the Hugging Face libraries for LoRA, DPO, etc. You can do all of it. For some companies, they don't want to reinvent the wheel. So we have built a cloud platform called Arcee Maestro that does all of that in just a few clicks. Literally everything you saw on that previous slide can be done here in a few clicks. Upload your data, go through the steps, and we have a Python SDK as well if you insist on writing code. It's available as a SaaS platform and as a VPC deployment if you want to keep everything super private. But that's not all. To prove our point, we also build models. We have a bunch of models on Hugging Face, open source, and you can go check them out. We're also building commercial models. This one actually came out not two weeks ago, it's called Supernova, a 70 billion parameter model based on the LLaMA 3 architecture. This was built using pretty much all the techniques I've described so far, including merging and model distillation, starting from the bigger LLaMA 3.1, 405 billion and distilling into a smaller model, etc. There's a great blog post. I think Lucas, you helped write it, right? On the training pipeline. It's got a ton of good details. It's the best 70 billion model available today, to the best of our knowledge.

Benchmarks are benchmarks, and we can argue about benchmarks all day and all night, but on the Google IFEVOL benchmark, Supernova not only outperforms LLaMA 3.170B, it outperforms LLaMA 3.1405B and outperforms Claude 3.5 SONET and GPT-4.0. If they truly are trillion parameter models, then this is pretty impressive that we can outperform them with a 70B model. Let's try this model. We have a demo. You'll get the slides. Let's give it a shot. Can you see this? Yes. So it's a demo. You can all try it. It's supernova.arcee.ai. Let's give it a shot. That's the speed you get from a 70B model deployed on quality infrastructure. Faster than the big models, at least as accurate, and the price point, as you would imagine, is much, much lower. I'm not going to prompt this one because, in the interest of time, go try it. We get pretty good feedback on it. If you want to deploy it in your company, it is available on the AWS Marketplace. If you look for Arcee on AWS Marketplace, you'll see Supernova in there, and you can deploy it to SageMaker in a couple of clicks. That's Supernova.

Coming back to my initial claim, when I say these models based on open architectures are state-of-the-art and outperform much larger models, we are there. We did it, and we'll keep doing it. If you take that model and specialize it on your data, it's going to go even higher. We've built a smaller sibling of Supernova, an 8 billion model called Supernova Lite, based on LLaMA 3.1. We were happy to see just a couple of days ago that it's the best model, the best 8 billion model available today. It's number one on the Hugging Face leaderboard. You can go and check that out. You can try this very easily. You can deploy it from Hugging Face if you'd like. Because the model is so small, 8 billion, you can deploy it on very cost-effective infrastructure. In fact, I can run it locally on my machine. Do I have this running here? Yes. Right? So I have a quantized version running on my Mac. Let's just use this prompt. See? You can run this 8B model, the best available today, just like that on your local machine. That's where we are. For production usage, you would probably want to run this on AWS.

Let me show you two things. First, how we can deploy this on Inferentia, which is the AWS accelerator. All those notebooks are on GitHub, so you'll get the link to that. Here, all it takes is to say, "Hey, I want to deploy this on Inf2Excel." This one costs, I think, 99 cents an hour. I'm using the AWS inference container for that and setting some basic settings, and literally calling deploy. Wait for a few minutes, and then I have my SageMaker endpoint ready, and I can start prompting it. Let's do streaming inference. There we go. That's fast, as fast as you need it to be. You can run this inside your AWS account for 99 cents, which is the on-demand price, but as an AWS customer, you would optimize that at least 30% very easily. You get full privacy and full control. You can use the OpenAI prompting format, so if you have OpenAI prompts today, you can just reuse them as is and run them on this model because it supports the OpenAI Messages API, the input, and the output.

But can we get smaller? Of course, we can. And I can run this on this machine. This machine is a Graviton instance, a Graviton 4, a CPU instance with ARM cores, so no GPU whatsoever. Who thinks we can run this fast on a CPU instance, an 8 billion parameter model? Let's take a look. So Supernova Lite quantized to four bits on CPU. How many tokens per second do you think? Place your bets. 57 tokens per second, right? This instance is an R8 instance, which costs something like $2.5 an hour. When we get the C8G instances, they will be even more cost-effective. So a state-of-the-art model running at this speed on a CPU instance is where we are today. We can run the 70B model, but I'm still tweaking, so I'm not going to show that today. You'll have to wait for my YouTube video.

Summing things up, the one takeaway, and honestly, you should all know this by now, but it's worth repeating: there is no model that rules them all. That was the 2023 lie that put a lot of companies in a really bad situation, experimenting and sometimes deploying apps connecting to very large closed models that they were told would do everything, only to realize they couldn't and they cost an arm and a leg. Each project is different, with different requirements, different domain knowledge, and probably a different ROI scenario. You need to study each project separately and find the right model and the right infrastructure for each use case. As we see with our customers every day, small tailored open-source models are the way to go. You take the best open-source models available today, which are already extremely close to state-of-the-art performance, and then through clever techniques, you make them even more amazing on your domain knowledge, which is the only thing you care about. If you're a doctor summarizing patient notes, you don't need a model that writes cooking recipes or poetry. If you want to write poetry, that's okay, but if you're a doctor, that's not what you care about. You just need one thing, but you need it done as well and as cost-effectively as possible.

Training and fine-tuning techniques are moving very fast. All those latest advances, like MergeKit, Spectrum, etc., are hardly a year old. They're really changing the game in terms of speeding up that pipeline, reducing the cost, and increasing the quality, the three things you care about. So that's what we do. Again, you can probably do all of it open source and reinvent the wheel a little bit. Or you can try our platform in the cloud or in your VPC. And of course, you can try our open-source models.

A few resources to close. Go check our blog; there are a lot of good posts in there. Go check out our model and dataset collection on Hugging Face. That GitHub repository is where I put all my AWS notebooks, so if you're interested in deploying models on SageMaker or through the marketplace, etc., that's where you'll find all the notebooks. As mentioned before, I have a rather busy YouTube channel where I keep posting AI stuff, AWS stuff, and so on, and you may like that. This QR code is how to subscribe to our newsletter and stay in touch. We have a bunch of new launches coming, so I don't think you want to miss out. All right, well, that's really what I wanted to tell you. Thanks so much for listening. My colleagues are here for questions, and I guess I can take a few too. Thank you very much.
        </div>
<div class="tags">
<h2>Tags</h2>
<span class="tag">Open-Source Models</span><span class="tag">Model Merging</span><span class="tag">Enterprise AI Solutions</span>
</div>
<div class="links"><a class="link" href="../../index.html">← Back to YouTube Overview</a></div>
</div>
</body>
</html>
<!DOCTYPE html>

<html lang="en">
<head>
<meta content="Open Source AI with Hugging Face   Dallas AI meetup 05 2024 - Join us for the Dallas AI Meetup held in Austin, Texas, on May 29, 2024 (https://dallas-ai.org).

Learn how Hugging Face is changing the ML landscape, get pract..." name="description"/><meta content="Open Source AI with Hugging Face   Dallas AI meetup 05 2024 - Julien Simon" property="og:title"/><meta content="Open Source AI with Hugging Face   Dallas AI meetup 05 2024 - Join us for the Dallas AI Meetup held in Austin, Texas, on May 29, 2024 (https://dallas-ai.org).

Learn how Hugging Face is changing the ML landscape, get pract..." property="og:description"/><meta content="https://www.julien.org/youtube/2024/20240614_Open_Source_AI_with_Hugging_Face_-_Dallas_AI_meetup_05_2024.html" property="og:url"/><meta content="video" property="og:type"/><meta content="summary_large_image" name="twitter:card"/><meta content="Open Source AI with Hugging Face   Dallas AI meetup 05 2024 - Julien Simon" name="twitter:title"/><meta content="Open Source AI with Hugging Face   Dallas AI meetup 05 2024 - Join us for the Dallas AI Meetup held in Austin, Texas, on May 29, 2024 (https://dallas-ai.org).

Learn how Hugging Face is changing the ML landscape, get pract..." name="twitter:description"/><link href="https://www.julien.org/youtube/2024/20240614_Open_Source_AI_with_Hugging_Face_-_Dallas_AI_meetup_05_2024.html" rel="canonical"/><meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<title>Open Source AI with Hugging Face   Dallas AI meetup 05 2024 - Julien Simon</title>
<style>
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
            line-height: 1.6;
            margin: 0;
            padding: 20px;
            background-color: #f5f5f5;
            color: #333;
        }
        .container {
            max-width: 800px;
            margin: 0 auto;
            background: white;
            padding: 30px;
            border-radius: 10px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
        }
        h1 {
            color: #2c3e50;
            margin-bottom: 10px;
            font-size: 2.2em;
        }
        .date {
            color: #7f8c8d;
            font-size: 1.1em;
            margin-bottom: 30px;
            font-weight: 500;
        }
        .video-container {
            position: relative;
            width: 100%;
            height: 0;
            padding-bottom: 56.25%; /* 16:9 aspect ratio */
            margin-bottom: 30px;
        }
        .video-container iframe {
            position: absolute;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
            border: none;
            border-radius: 8px;
        }
        .description {
            background: #f8f9fa;
            padding: 20px;
            border-radius: 8px;
            margin-bottom: 30px;
            white-space: pre-wrap;
            font-size: 1em;
        }
        .description a {
            color: #3498db;
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s;
        }
        .description a:hover {
            color: #2980b9;
            text-decoration: underline;
        }
        .transcript {
            background: #f8f9fa;
            padding: 20px;
            border-radius: 8px;
            margin-bottom: 30px;
            white-space: pre-wrap;
            font-size: 1em;
        }
        .transcript h2 {
            color: #2c3e50;
            margin-top: 0;
            margin-bottom: 15px;
            font-size: 1.5em;
        }
        .tags {
            margin-bottom: 30px;
        }
        .tags h2 {
            color: #2c3e50;
            margin-bottom: 15px;
            font-size: 1.5em;
        }
        .tag {
            display: inline-block;
            background: #3498db;
            color: white;
            padding: 6px 12px;
            margin: 4px;
            border-radius: 20px;
            font-size: 0.9em;
            font-weight: 500;
        }
        .links {
            display: flex;
            gap: 20px;
            flex-wrap: wrap;
        }
        .link {
            display: inline-block;
            padding: 12px 24px;
            background: #3498db;
            color: white;
            text-decoration: none;
            border-radius: 6px;
            font-weight: 500;
            transition: background-color 0.3s;
        }
        .link:hover {
            background: #2980b9;
        }
        .link.youtube {
            background: #e74c3c;
        }
        .link.youtube:hover {
            background: #c0392b;
        }
        @media (max-width: 600px) {
            .container {
                padding: 20px;
                margin: 10px;
            }
            h1 {
                font-size: 1.8em;
            }
            .links {
                flex-direction: column;
            }
        }
    </style>
</head>
<body>
<div class="container">
<h1>Open Source AI with Hugging Face   Dallas AI meetup 05 2024</h1>
<div class="date">June 14, 2024</div>
<div class="video-container">
<iframe allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="" src="https://www.youtube.com/embed/cf8z3Q8PFQQ">
</iframe>
</div>
<div class="description">Join us for the Dallas AI Meetup held in Austin, Texas, on May 29, 2024 (<a href="https://dallas-ai.org)." rel="noopener noreferrer" target="_blank">https://dallas-ai.org).</a>

Learn how Hugging Face is changing the ML landscape, get practical insights on working with large language models, and dive into the details of Retrieval-Augmented Generation (RAG). Watch a demo on building a chatbot for the energy sector, and hear lessons learned from over 200 customer meetings about deploying LLMs in real-world settings. Discover how to choose the best models using Hugging Face leaderboards and see the latest trends in ML engineering.

<a href="https://www.youtube.com/watch?v=cf8z3Q8PFQQ&amp;t=0" rel="noopener noreferrer" target="_blank">00:00</a> Introduction
<a href="https://www.youtube.com/watch?v=cf8z3Q8PFQQ&amp;t=135" rel="noopener noreferrer" target="_blank">02:15</a> Hugging Face
<a href="https://www.youtube.com/watch?v=cf8z3Q8PFQQ&amp;t=610" rel="noopener noreferrer" target="_blank">10:10</a> Working with Large Language Models
<a href="https://www.youtube.com/watch?v=cf8z3Q8PFQQ&amp;t=1530" rel="noopener noreferrer" target="_blank">25:30</a> Double-clicking on Retrieval-Augmented Generation (RAG)
<a href="https://www.youtube.com/watch?v=cf8z3Q8PFQQ&amp;t=1870" rel="noopener noreferrer" target="_blank">31:10</a> RAG demo - building a chatbot for the energy domain
<a href="https://www.youtube.com/watch?v=cf8z3Q8PFQQ&amp;t=2660" rel="noopener noreferrer" target="_blank">44:20</a> LLMs from the trenches: lessons from 200+ customer meetings
<a href="https://www.youtube.com/watch?v=cf8z3Q8PFQQ&amp;t=3795" rel="noopener noreferrer" target="_blank">1:03:15</a> Picking models with the Hugging Face leaderboards
<a href="https://www.youtube.com/watch?v=cf8z3Q8PFQQ&amp;t=4135" rel="noopener noreferrer" target="_blank">1:08:55</a> ML engineering is on fire

#LargeLanguageModels #HuggingFace #MachineLearning #DeepLearning #AI #opensource 

⭐️⭐️⭐️ Don't forget to subscribe to be notified of future videos. Follow me on Medium at <a href="https://julsimon.medium.com" rel="noopener noreferrer" target="_blank">https://julsimon.medium.com</a> or Substack at <a href="https://julsimon.substack.com." rel="noopener noreferrer" target="_blank">https://julsimon.substack.com.</a> ⭐️⭐️⭐️

Notebook: <a href="https://github.com/juliensimon/huggingface-demos/tree/main/langchain" rel="noopener noreferrer" target="_blank">https://github.com/juliensimon/huggingface-demos/tree/main/langchain</a></div>
<div class="transcript">
<h2>Transcript</h2>
            So thank you, everyone, for making it to the office, and hi to everyone on Zoom. Pleasure to be here. My name is Julien. I'm the chief for Arcee. That's what the slide says. What I'm doing is meeting, traveling quite a bit, and meeting with customers, trying to explain to them that open source AI is actually the way to go about this AI business. I'm also working with our partners. We have cloud partners, hardware partners, and that's a fair amount of my time.

In this presentation, which I'd love to keep as interactive as possible, please jump in, ask questions, interrupt me. It's difficult for me to keep track of the questions on Zoom, so if you could kindly relay the questions from our friends, I don't want anyone to feel excluded from asking questions. In the room, you have no excuse. Just wave at me, yell at me, or throw things at me. Okay. Let's have some fun and learn together.

What I'm going to cover today is a quick background on Arcee. We've been doing quite a few things lately. You may not be completely up to speed. Then we'll chat about some of the latest trends we see in LLMs. This is where I go super opinionated, and I'm trying to force you to react to some of that stuff to get the conversation going. So just have at it. We're in Texas. We can. Okay. Don't pull guns. Okay. No guns. But all right. That's fine. I don't have mine. So it's not. And we'll do demos. We'll show you some of the cool things we've been building. And again, answer as many questions as we can.

A quick word on Arcee: Arcee is the home of open source AI. In a few years, we've become the de facto place where the community comes to find models and datasets and also share them. Who has shared a model on Arcee or uploaded a model on Arcee? All of you need to do more. Thank you. We need more. We always need more. Folks have called us, and still call us, the GitHub of machine learning. I think that's fine. I don't mind the analogy, but you will see we're actually building more and trying to be more than just a nice collection of models and datasets.

We have all these nice companies as investors as of last summer. That's good validation for us. We'll try to put that money to good use for the community. More importantly, it's really good validation for open source AI generally, right? Because almost all of them are also building closed models. It's never black or white. If you came here to hear me talking shit about closed models, I'm not going to do that. Closed models have their own use cases. I think open source AI is generally a better idea, but there's probably room for everyone here.

The mile-high view on Arcee: If you looked at Arcee a year ago or have been experimenting casually, you're probably aware of the models and datasets in the bottom right corner. An insane number. I checked this morning and updated the slide. So 680,000, it might be 690,000. Who knows? All of these are open-sourced, though they have different open-source licenses. Please be mindful of that. Not everything is fine for commercial usage, but if you pick stuff with an Apache 2, MIT, or LAMA license, you're good to go. Just be careful.

These models and datasets, we call them Arcee models, but they're really community models. We're the stewards, the model herders. We welcome them, try to take good care of them, but they come from the community, which means everyone from Google, Microsoft, Meta, to universities, startups, and individuals. These datasets and models are the raw material for your AI projects. The next logical step is to work with these models and datasets through our open-source libraries. The most popular one is called Transformers, which made Arcee popular. Over time, we added quite a few more. If you go to our repo on GitHub, you'll find many more, like Diffusers for stable diffusion models, text-to-image, text-to-video, Accelerate for distributed training made simple, and Text Generation Inference (TGI), which I'll come back to later. TGI is our own inference server that you can use to deploy LLMs, and it's what we use in our own services and cloud integrations.

We have many more libraries. Over the course of building Arcee, we've also implemented a few cloud services of our own. Spaces is basically machine learning demos—small web apps you can write and host on our infrastructure to showcase models in a web app, not just a Jupyter notebook. An inference endpoint, as the name implies, is a model deployment service. You can one-click deploy 99.99% of our models on any of the three major clouds. This is a fully managed service, so you deploy it, and we take care of everything else. That's the core of the platform.

We also have the Enterprise Hub, which adds a layer of security, compliance, and features for enterprise users on top of the existing hub, like SSO, auditing, and all those good things. We're not a model-building company, but from time to time, there's an opportunity for Arcee to add new models to the open-source collection. Bloom, in 2022, was the first open-source LLM to compete with GPT-3.5, a very large model. StarCoder, as the name implies, is a code generation model. Edifix is a visual large language model, so you can chat with images, etc. Very cool stuff. There will probably be more at some point.

HuggingChat is our open-source chatbot. If you want a fully privacy-preserving chatbot, this is it. It's based on a curated list of the best open-source models, probably six or seven models. We upgrade them all the time, so you can select the model you want to chat with. From UI to backend to models, everything is open source. You can grab all the bits and pieces and redeploy them in your infrastructure if you want. There's an iOS app, too, so if you have an iPhone, you can chat with the best open-source LLMs on your iPhone.

We have cloud partners, and as you can imagine, we try to integrate our open-source ecosystem into cloud environments, mostly machine learning services. We have hardware partners, and we focus on accelerating training and inference across the board. This work lives under the Optimum umbrella. Optimum is a collection of open-source libraries, like Optimum Intel, Optimum AMD, where we provide a transformers-like API with built-in acceleration, hiding all the hardware complexity. You just import the Optimum library that matches your hardware, and we automatically accelerate and optimize training and inference.

Lastly, we have a consulting/professional services program called Expert Support. We can engage directly with customers to help them build and bring their models to production more quickly.

Doubling down on LLMs a bit: All of you here have tried ChatGPT at some point, right? That's fine. When you work with LLMs, the first step is just using them as is. Ask a question and get an answer. That's easy. But very quickly, you realize the answer might be correct, but not in the tone of voice you wanted, too formal, not formal enough, too long, or you wanted a yes or no answer, or three bullet points. So you need to start providing instructions, which is called prompting. Please do not call it prompt engineering. Any prompt engineers in the room? Just asking. Maybe on Zoom, okay, someone's... Okay, you can always log off and yell at me on LinkedIn or Twitter, or you can hang on and take it.

Prompt engineering is... there is no such thing. Prompting is useful if you limit yourself to tone of voice, safety guidelines, and output control, like brevity and formatting. That works very well. It breaks when you try to teach the model new things through prompting. You think showing five examples is enough to teach the model how to generalize. The so-called many-shot prompts work for very basic things but not for complex tasks. The analogy I use is that I never learned COBOL. I learned a few programming languages but not COBOL, and I don't intend to learn it now. You could show me 20 examples of buggy COBOL snippets, and I could read and, through analogy, pick up a few things. But for more complicated examples, I wouldn't know. Showing me 100 examples wouldn't make me an expert. Yet, the community tries to turn vanilla LLMs into legal experts or engineering experts with five examples. I'm sorry, but no. If you need to bake new domain knowledge into LLMs, there's a procedure for this called fine-tuning.

Fine-tuning means training the model for a particular task. If you want an LLM to answer legal questions in the oil and gas domain, you need a very narrow slice of knowledge. You will never ask for cooking recipes, poetry, or astronomy questions. You will ask specific questions. Fine-tuning lets you do that. The next level up is continuous pre-training. Initial training or pre-training is training completely from scratch. If you have a million pages of legal documents for the oil and gas industry, you can train your model from scratch on that corpus, and it will be pretty good. That's a big, expensive effort. Maybe you have a thousand new pages every month. You could take your vanilla LLM, train it on a thousand pages, and then train it again on another thousand pages incrementally until you get something good enough. Or if it's good at oil and gas legal, but now you need nuclear energy legal, you might want the same model to do both. This is called continuous pre-training.

In parallel, there's retrieval-augmented generation (RAG), which has spread like wildfire. We'll do a RAG demo, and you'll see the benefits. The higher you go, the better the domain adaptation, but it gets more complicated, expensive, and time-consuming. Initial training will cost hundreds of thousands, if not millions. Fine-tuning can be very cheap, as we'll see. Start at the bottom, evaluate in terms of accuracy, and only move up if needed.

The question is, for fine-tuning and continuous pre-training, do I expect companies to build one model per use case per domain or will we see industry models from large players? So far, attempts at industry models haven't been super successful. Some companies have tried healthcare LLMs or legal LLMs. These can be a foundation for your own fine-tuning efforts. If someone trains a model on a million pages of legal corpus, that's great. But if you work at Amazon and want a legal LLM for the retail domain, even that might be too general. You might be dealing with vendors or supply chain partners, which have different nuances. It's difficult to see an external vendor training a model that is so good and relevant to all your specific domain knowledge, vocabulary, product names, and policies that you would use it out of the box.

Healthcare might be different because there's a lot of public knowledge and reports. But if you're working for Pfizer, you still have a lot of confidential knowledge that needs to be injected into the model. How to do that remains to be seen. My dream scenario is seeing open-source LLMs for particular industries and businesses, trained on as much public data as is available. Then companies can fine-tune them on their data. It's never black or white, so some industries might work with an industry model because 99% of the knowledge is out there, like in research activities.

For fine-tuning and continuous pre-training, it could be incremental on the same domain or adding a few different domains. It could work. I would see mostly improving the same domain, not having to re-fine-tune from scratch, which is always boring. Generally, it's a first round, evaluate, and then fix what's not working. Add more examples, train a little more, and keep improving until it's good enough.

I've been hearing about small language models. Are they more for embedded devices? No, we don't need more buzzwords. Large language models are called LLMs, and now we call them small language models. If you compare them to XGBoost models, they're humongous. Small language models are a bullshit definition. I prefer to talk about open source versus closed. Large language models are almost synonymous with closed models like Gemini, OpenAI, and Claude. Anything smaller than 70 billion parameters is a small language model, but it's a bullshit definition. If GPT-4 has a trillion parameters, then 70 billion is small. Most customers are in that orange box, starting with prompting, adding RAG, and eventually doing some fine-tuning for further adaptation.

RAG is adding external data to the mix for generation. Instead of relying on what the LLM knows, which is generally too old, RAG brings freshness and access to company data. You take that data, run it through an embedding model, turn it into high-dimensional vectors, and store those vectors in a backend. When a user asks a question, you convert the question into a high-dimensional vector, run a similarity search, and retrieve the top five or ten vectors that closely match the query. These correspond to documents that hopefully have the information you need. You pull those documents back and inject them into the prompt. This is how you can inject information that happened five minutes ago and is very private and confidential to your company.

This looks like a complicated slide, but I'll show you how to do this in a single notebook. RAG brings freshness and access to company data. It's a good first thing to try. If you want to make it better, you might fine-tune the LLM or tone of voice. If you build an oncology chatbot and the LLM sounds like an excited teenager, that's not appropriate. Prompting can help, but you might want to bake that behavior into the model. You could also fine-tune the embedding model to specialize it for your domain. Generally, you will try RAG first because you need fresh company data. For further accuracy, you might do fine-tuning. You need all these knobs to turn. In the end, you need prompting, RAG, and probably fine-tuning. But more than anything, you need to know the effects of turning these knobs. If you have no sense of what happens when you turn prompting to 11 or fine-tuning to zero, you're more likely to break things.

Let's look at a demo. I don't know why my screen is turning dark. Give me a second. Where is my notebook? Can you read this? Is it large enough? Let's try it a little more. What I'm going to do here is exactly what you saw on that slide. I need two models: an LLM and an embedding model. I'll deploy the LLM on Amazon SageMaker and run the embedding model inline. My ingestion process will be three PDF files, and I'll use an in-memory library for the backend. I'm using Langchain to run everything.

First, I need to deploy my LLM. I'm deploying the latest version of Mistral 7B. This is the full code. There's nothing else. This shows the SageMaker integration for Arcee. You can do this for the majority of models on Arcee. Just define the model, create a Hugging Face model object in the SageMaker SDK, and deploy it. I'm deploying on a small GPU instance. When people tell me deploying LLMs is a project, it's not. If we look at this model on the hub and click on "Deploy SageMaker," we generate the code for you. All you have to do is copy and paste. It's as simple as that. Don't tell me it's complicated to deploy LLMs on AWS.

We wait a few minutes, and Mistral 7B is deployed. If you're telling me you will be great at building an inference container on EC2, I want to cry. There's zero reason to do that. If you want to grab the model from Arcee and reinvent the wheel internally on EC2, EKS, ECS, on-prem, or whatever, be my guest. I'm super happy with copy-paste. Wait seven minutes and have a production-grade LLM in the cloud. That's what we've been building with AWS for three years.

A couple of requests from our online friends: If it's possible to share the link for this. Absolutely. And if you can repeat the questions coming from the room. The question is, "I don't want to use SageMaker because I love reinventing the wheel on EC2. How do I do it?" The answer is, go do it. You don't need me. I'll copy the link and send it to you.

Now we have the LLM as a SageMaker endpoint. We need to plug this into Langchain. It's not too difficult. Langchain needs to know how to serialize and deserialize information coming from the endpoint. This is what it looks like. You need a content handler to turn the input to JSON, transform the output from JSON, and take into account the prompting format for Mistral, which requires those inst tags. Langchain knows how to send data and retrieve data. We can try asking a question directly. As a helpful energy specialist, please answer the question, focusing on numerical data. Don't invent facts if you can't provide a factual answer. Say you don't know. Create a template, create a chain, and ask the question: "What is the trend for solar investments in China in 2023 and beyond?" Run the chain, which sends the query to the model and prints the answer. It's not a bad answer. The model doesn't know, it's telling you it doesn't know, and it's not hallucinating. It's a good model.

Let's add RAG. I'm grabbing three PDF files from the International Energy Agency, copying them to S3, and extracting the text from those PDFs using an AWS service called Textract. I'm chunking them into 256-byte chunks. We ingest 600 pages or so. It took a few minutes, and now I have thousands of text chunks. I need to run them through the embedding model to turn them into vectors. I'm using a small embedding model from Arcee. It takes the text and outputs a high-dimensional vector. I download this, and Langchain has an object for that. I embed all those chunks, run every single chunk through the embedding model, generate the high-dimensional vector, and store it in my in-memory database. I can save it because I don't want to do this repeatedly. Once I've done that, all my chunks have become vectors, and I can retrieve them.

Now, let's configure everything. I'm using my collection of vectors as my source of truth. I'll retrieve 10 chunks every time I query. My template looks like this: the same prompt as before with the user question and the context, which will be the 10 chunks I retrieved from the backend. I put all these bits together with another chain and ask the question again: "What is the trend for solar investments in China in 2023 and beyond?" This time, we embed the question, use the embedded query to find the top 10 documents that closely match, retrieve them, and inject them into the prompt. The answer is much better. It says, "Solar investments in China will continue to be significant in 2023 and beyond. In 2023, approximately 380 billion is expected to be invested in solar globally." The model is telling me based on the provided context. If my retrieval model did a good job, I hit the jackpot. I could ask the model to quote the exact page where it found the information. This is RAG in action. It's simple, not crazy, scalable, but good enough to get started.

In the last eight to nine months, I've met around 250 customers. Retrieval-augmented generation is everywhere. If it's the first time you're hearing about it, you can go home happy because you learned something super useful. RAG doesn't change the model; there's no training involved. The knowledge comes from the external source of truth, not the model. The model is just a writing assistant. Here's my question, here's the context, write me a good story. If you're not using the built-in knowledge of the model, why would you want to work with a very large model? A very large model is large because it has more parameters, which store more knowledge. When I see people doing RAG with 70 billion models, I'm skeptical. Have you tried smaller models? You're probably overspending to run inference on a model you don't fully use. Mistral is 7 billion, but you could go lower. Microsoft's PHI family, particularly PHI-3, is a good candidate. You don't need a ton of built-in knowledge, so use smaller models and save.

Next, content. AI marketing in action, big numbers. Mistral and others typically have 8K context, meaning you can pass 8,000 tokens, roughly 7,000 words. Some people want you to believe you need 100K, 1 million, or even 10 million tokens. This is complete bullshit. A typical novel is 100,000 words, or about 130,000 tokens. If you're passing 100,000 tokens, you're talking about a full novel. What is the use case for this? No one comes up with a reasonable answer. If you need that, go for the large context size, which is expensive and slow. For the other 99.99% of usage, use 8K or 16K context and save a ton of money and latency.

Model customization: If we had met a year ago, and you told me you wanted to fine-tune a 7B model, I would have said it would cost you $10,000 to $20,000. Now, I can say with a straight face that you can do it for $10 to $20. This is due to parameter-efficient fine-tuning (PEFT) techniques like LoRa and QLoRa. Instead of fine-tuning 100% of the model, you fine-tune 0.1% to 1% of the model and get very close to the results of full fine-tuning. It's math, so it's magic. It dropped 1000x in cost. I have demos on my YouTube channel, FineTuningLama2 for $5. It's pretty cool.

Another problem to solve in fine-tuning is reinforcement learning with human feedback (RLHF), which made ChatGPT amazing. But RLHF is difficult because it requires human feedback. You need a task force to look at what the model generates and fix it. Even if you have people to do that, things can go wrong. Alignment is a problem. If we could get rid of human feedback, we would make it simpler, faster, and hopefully eliminate misalignment. Techniques like DPO, PPO, and Oropo (all ending with O for optimization) are promising. If you're curious, read the blockers by Argilla.

The last thing I want to mention is model merging. It's exactly what the name says. You're not training anything; you're taking models and running averaging on them. Model A, 20% of Model A plus 80% of Model B, literally averaging weights. You have a math model, a code model, and a legal model, and you average all of them to have a model that can do math, code, and legal. The results are impressive. The library to do this is called MergeKit. I have a video on the different merging techniques with some intuition on how they work and links to the papers. It's seconds on your laptop because it's literally averaging weights. You can start from hundreds of thousands of models on Arcee and cherry-pick the ones that have the knowledge you like and build a model that has all of them. It's just a simple algorithm, no training involved.

Mixture of experts is different. Mixture of experts is training different copies of the same model in parallel, hoping they learn different things. At inference time, you pick a subset, which saves on latency but still requires loading everything and training everything. Merging is just Model A, Model B, Model C, different weights on those models, and building a single model in the end. It's crazy, but the results are impressive.

How is this different from mixture of experts? Mixture of experts is training different copies of the same model in parallel, hoping they learn different things. At inference time, you predict with only a subset, which saves on latency but still requires loading everything and training everything. Merging is just averaging weights from different models to build a single model. It's crazy, but the results are impressive.

How do you control for hallucinations? You still need to run evaluation and QA on your model. The appeal of merging is that you have lots of models out there. You can cherry-pick the ones that make sense for your use case and build a model with those capabilities. But of course, you still need to test it.

The rest is more straightforward but important. This year is about moving stuff to production. People want results, not just POCs. Cost performance is everything. If you're not evaluating, thinking about latency, throughput, and ROI, you will be disappointed. Inference is the huge cost, not training. Fine-tuning is very cheap now, and model merging, if successful, will eliminate training costs. In the end, what's left is inference. Inference is forever. The model is deployed and stays there until the project dies, which could be years. It needs to scale according to traffic. If you're not thinking about optimizing your inference costs, you're not thinking about cost performance.

In terms of hardware, there are alternatives to NVIDIA GPUs. They're crazy expensive, and there are not enough of them. AMD has a good GPU, the MI300, which is hitting the clouds. We like it a lot and see better cost performance there. AI accelerators from AWS and other clouds are equally interesting. If you use small models, you can use mid-range GPUs, which are plentiful. G5 instances on AWS, about a dollar an hour, are perfect for 7-8 billion parameter models, which are the sweet spot right now. Local CPU inference is quickly becoming a thing. We work with Intel and AMD on CPU inference and model optimization for local inference. The major blocker is finding a way to charge for local inference. Once companies figure out how to charge you, you'll have a local copy.

How do we know which models are the good ones? We have a leaderboard, the LLM leaderboard, which everyone tries to game. This is the version from March 20th. The best one is 63.75. I updated this slide this morning. The same screen, 7 billion and smaller pre-trained models. Microsoft is at the top with 69.86 average. Google GMR is still doing well. Llama 3 popped up. Qwen is still here. Mistral is lower down the list. That's two months. May was busy. April and May were busy times for new models. The pace of innovation is fast. You can't sit for 12 months and hope whatever you have in production today will still be good in 12 months. It will still do the job, but there's probably a much better model that has already come out. You should definitely keep an eye on new models, measure how far ahead the new thing is on your data, and run evaluation on your data. There comes a time when the gap is large enough that you will want to switch.

If you wrote elaborate prompts, do you think they will still work the same? If you switch from JAMA 7B to 5.3 mini 4K instruct, do you think the prompts work the same? Maybe, maybe not. The longer and more domain-specific they are, the more dangerous they are. You will rewrite that stuff. Prompts are technical debt. They are technical debt. If you keep the prompts generic, you'll be in a better place. Even if you pick Gemma 7B, try your prompts on Mistral and Phi 3 and two or three different models, like regression testing. Check the different answers automatically. If you see very different answers, something might be off. If the answers are reasonably similar, you're in a good place.

For performance, tokens per second, memory usage, etc., we have the performance leaderboard, which helps you find the right model size for your infrastructure and latency. We have an embedding leaderboard, which is important for RAG. Embeddings are really cool, and there are lots of new models coming out. We have a leaderboard specifically for embeddings. Sentence Transformers version 3, a major release, came out a couple of days ago. If you're deep into embeddings, read about this new release.

The secret sauce is the data, especially if you do RAG and fine-tuning. The one thing people don't talk about enough is machine learning engineering. Once you have a model you like, how do you squeeze every bit of cost performance out of it? Faster retention layers, model compilation, model quantization, model merging, and hardware acceleration need to be looked at at some point. If you're not factoring in these techniques, you're missing out. Who wants to pass on 2X ROI improvement? No one.

Closed source versus open source: How much domain adaptation do you need, and how much scale will you have? Low scale, low domain adaptation: You need an AI tool for your marketing department to improve product pages or your HR team to summarize resumes. They won't do it 50,000 times a day, and it's not very domain-specific. Use a model API, like Amazon Bedrock, and get it out the door in a few days. Low scale means cost isn't a problem. If this thing costs $500 a month, there's no incentive to make it $50. Engineering time is precious, so let them spend $500 and move on.

High scale, low domain adaptation: You need to translate 10,000 pages of text every day. If you want ROI, you need the smallest possible model. Sure, you can translate with GPT-4 or Claude, but if it costs you $10,000 a month, there might be an incentive to make it $1,000. As soon as you have scale, you need to consider the benefits of a smaller model and less infrastructure.

High scale, high domain adaptation: You need the smallest model possible and fine control over what's going on, which you cannot get with model APIs, even those that offer fine-tuning. High domain adaptation means you need RAG and probably fine-tuning.

High domain adaptation, low scale: This is where humans can still exist. If it's low scale and high domain adaptation, you will probably spend a fair amount of time building a good model to solve hard questions that will only be used 10 times. It's like automation. Would you automate a workflow you do from time to time? Is it worth it? Probably not.

Let's wrap it up here. Thank you, everyone, for joining. I'll send you the slides and the links to the notebooks. You can play with this stuff. Thanks, everyone. Thank you, everyone on Zoom.
        </div>
<div class="tags">
<h2>Tags</h2>
<span class="tag">OpenSourceAI</span><span class="tag">LLMs</span><span class="tag">RAG</span><span class="tag">FineTuning</span><span class="tag">ModelMerging</span>
</div>
<div class="links"><a class="link" href="../../index.html">← Back to YouTube Overview</a></div>
</div>
</body>
</html>